cd ~/.zsh/plugins
curl -L https://raw.githubusercontent.com/sbugzu/gruvbox-zsh/master/gruvbox.zsh-theme > ~/.zsh/themes\

curl -L https://raw.githubusercontent.com/sbugzu/gruvbox-zsh/master/gruvbox.zsh-theme > ~/.zsh/themes/gruvbox.zsh-theme
source ~/.zsh/themes/gruvbox.zsh-theme
brew tap homebrew/cask-fonts\

brew install font-hack-nerd-font\

cd $ZSH/themes
cd ~/.zsh/themes
rm gruvbox.zsh-theme
git clone https://github.com/dracula/zsh.git\

cd zsh
source dracula.zsh-theme
touch test
ls --color=auto
mkdir .config
mkdir ColorScheme
cd ColorScheme
git clone git@github.com:morhetz/gruvbox-contrib.git
cd gruvbox-contrib
cd iterm2
rm ColorScheme
rm -rf ColorScheme
cd AppSupport/
cd DynamicProfiles
cd parsers
cd ~/Library/Preferences/com.googlecode.iterm2.plist
vim ~/Library/Preferences/com.googlecode.iterm2.plist
pbpaste > gruvbox.itermcolors
cd fin
git clone https://github.com/NvChad/NvChad ~/.config/nvim --depth 1 && nvim\
\

brew install nvim
nvim staging
nvim main.tf
nvim ~/.zsh/.zshrc
vim $ZSH/aliases.zsh.plugin
mv aliases.zsh.plugin aliases.plugin.zsh
dev
vim staging
brew install kubectl
vim dev/private
cd dev/private/workstation-setup/setup.sh
mkdir dev/rust
mv forge/cilium rust
rm -rf forge
mv rust learn
git clone https://github.com/rust-lang/rust-by-example
mdbook serve
mdbook serve --help
cd dev/learn
git@github.com:NvChad/nvchad.github.io.git
git clone git@github.com:NvChad/nvchad.github.io.git
mkdir -p $ZSH/aliases
pbpaste > $ZSH/aliases/kubectl.plugin.zsh
vim $ZSH/aliases/kubectl.zsh.plugin
cd $ZSH
mv aliases.plugin.zsh aliases
cd aliases
mv aliases.plugin.zsh customized.plugin.zsh
brew install minikube
brew install docker
minikube start --cni=cilium --memory=4096
brew cask install docker\

docker --version
docker
docker run hello-world
brew install --cask docker\

brew uninstall docker
brew uninstall --cask docker
brew install --cask docker
mkdir forge
cd forge
git clone git@github.com:cilium/cilium.git
git clone /
git clone https://github.com/rust-lang-nursery/rust-cookbook\

cargo install mdbook --vers "0.4.5"
cargo uninstall mdbook
cargo install mdbook
cd rust-cookbook
mdbook serve --open
minikube start --cni=cilium --memory=4096\

kubectl get pods
kubectl get nodes
cd site
brew install yarn
brew install node
yarn
make
git clone git@github.com:localstack/docs.git\

git clone --recurse-submodules --depth 1 git@github.com:localstack/docs.git\

brew install hugo\

cd docs
mv docs localstack-docs
hugo serve\

brew install go
hugo serve
cd rust-by-example
mv rust playground
cd playground
mkdir nvim-stuff
cd nvim-stuff
vim ~/.config/nvim/init.lua
nvim test.py
cd learn
cd Documentation
docker build -t cilium-docs .
cat Dockerfile
docker run cilium-docs
docker rm ad59e087da57
docker rmi cilium-docs
mv ~/Downloads/cilium-stable cilium-docs
rustup doc --book
mkdir rust
cd rust
cargo new hello_cargo --vcs=git
cat Cargo.toml
mkdir src
vi main.rs
nvim main.rs
rustc main.rs
./target/debug/hello_cargo
cd hello_cargo
yarn stdart
yarn start
aliases
vim test.py
python test.py
cd nvchad.github.io
cd localstack-docs
cd cilium
cd cilium-docs
open index.html
brew install --cask stremio
vim ~/.config/nvim
[200~brew install cilium-cli~
brew install cilium-cli
brew install --cask vlc --debug
ping videolan.org
brew install vlc
brew install --cask vlc
brew uninstall stremio
s
cilium --help
cilium install
minikube docker-env
eval $(minikube docker-env)
brew install --cask clipy
vim dev/private/workstation-setup/setup.sh
pbpaste | base64 -D | grep -v access
pbpaste | base64 -D | grep -v access 
cd components/clickhouse
s3-config.xml
vim s3-config.xml
git checkout -b refactor/clickhouse
brew instal awscli
aws --profile apono configure set role_arn arn:aws:iam::277411487094:role/yizhar_apono-apono-oidc-role && aws --profile apono configure set web_identity_token_file ~/.aws/token && echo eyJhbGciOiAiUlMyNTYiLCAidHlwIjogIkpXVCJ9.eyJ1c2VyX25hbWUiOiAieWl6aGFyX2Fwb25vLWFwb25vLW9pZGMtcm9sZSIsICJpc3MiOiAiaHR0cHM6Ly9hcG9uby1vaWRjLnMzLmFtYXpvbmF3cy5jb20vYjU1Njk4MzAtYWU4NC0xMWVjLWFlOGUtMGViNmRiMjNkYTQ1IiwgImlhdCI6IDE2Nzc2NTU3OTksICJleHAiOiAxNjc3NzEzMzk1LCAiYXVkIjogIkFXUyIsICJzdWIiOiAieWl6aGFyX2Fwb25vLWFwb25vLW9pZGMtcm9sZSJ9.GFHTr37WBpq3Jc5ouQqm_e0HKceQSzdiHp5nMtxJ7VW_OdJAf4273JFr73J5VmJHtrHAqTtLhu2cK1d7wQzFymAtm4y0foqf4fvWevpjTIbrsLp-yJ2w-3qdwZ6WFnl5HoKg48Hvfoio-y6YSgk3m74WkF2WfyPMnKe4XBsl5U7tuyamlqYwTMry01xpaxkzPqlUGxQsGRfFS0UTbscVp3m98Z9U2IWL6-gCRIwjVKEHjyM2JDhuMEOa4xOFLad7KXJgTmKvfiTGqkNSzYfiTYLjRay8B2LR4f7-lTtbe_KLt3_cvvf7xMSOnoXEywHAEDLPmBRyzT_Ye1UxtRMNuw > ~/.aws/token
vim ~/.aws/token
aws --profile apono configure set role_arn arn:aws:iam::277411487094:role/yizhar_apono-apono-oidc-role && aws --profile apono configure set web_identity_token_file ~/.aws/token && echo eyJhbGciOiAiUlMyNTYiLCAidHlwIjogIkpXVCJ9.eyJ1c2VyX25hbWUiOiAieWl6aGFyX2Fwb25vLWFwb25vLW9pZGMtcm9sZSIsICJpc3MiOiAiaHR0cHM6Ly9hcG9uby1vaWRjLnMzLmFtYXpvbmF3cy5jb20vYjU1Njk4MzAtYWU4NC0xMWVjLWFlOGUtMGViNmRiMjNkYTQ1IiwgImlhdCI6IDE2Nzc2NTY3NzMsICJleHAiOiAxNjc3NzE0MzY5LCAiYXVkIjogIkFXUyIsICJzdWIiOiAieWl6aGFyX2Fwb25vLWFwb25vLW9pZGMtcm9sZSJ9.EkrexIAy6lIUJH1W_Pd1a9lFf9d-p0gOVsiQZyRnuo4y2_U7pUaGLevfeDsoiYijPHZele5QISaMxz_7dhi87DdO4MMML2pxNjIrsNQCMxYHmDlKltZvOayaxZfiMIu_EbdjdMR1huuE8BxYf-f1cIgQ3Z3IsEW4ZJajZ5n9l-IYMgWKkvpfGihFbZJDhTh1H8twFcqC3VwLPlkdmrfLb8bsxeQQ9iSRgxQ11VBOApozCIcY_vxtluDodH3odCyfOQ9czFSmDQ56kjMxiIK4umVxyXp37LdjlKYrEz2QDWJkx8XhvUo9y6alWhgnCiEOVZqo8fwluDiQ542DaOLPQA > ~/.aws/token\
aws --profile apono configure set role_arn arn:aws:iam::277411487094:role/yizhar_apono-apono-oidc-role && aws --profile apono configure set web_identity_token_file ~/.aws/token && echo eyJhbGciOiAiUlMyNTYiLCAidHlwIjogIkpXVCJ9.eyJ1c2VyX25hbWUiOiAieWl6aGFyX2Fwb25vLWFwb25vLW9pZGMtcm9sZSIsICJpc3MiOiAiaHR0cHM6Ly9hcG9uby1vaWRjLnMzLmFtYXpvbmF3cy5jb20vYjU1Njk4MzAtYWU4NC0xMWVjLWFlOGUtMGViNmRiMjNkYTQ1IiwgImlhdCI6IDE2Nzc2NTY3NzMsICJleHAiOiAxNjc3NzE0MzY5LCAiYXVkIjogIkFXUyIsICJzdWIiOiAieWl6aGFyX2Fwb25vLWFwb25vLW9pZGMtcm9sZSJ9.EkrexIAy6lIUJH1W_Pd1a9lFf9d-p0gOVsiQZyRnuo4y2_U7pUaGLevfeDsoiYijPHZele5QISaMxz_7dhi87DdO4MMML2pxNjIrsNQCMxYHmDlKltZvOayaxZfiMIu_EbdjdMR1huuE8BxYf-f1cIgQ3Z3IsEW4ZJajZ5n9l-IYMgWKkvpfGihFbZJDhTh1H8twFcqC3VwLPlkdmrfLb8bsxeQQ9iSRgxQ11VBOApozCIcY_vxtluDodH3odCyfOQ9czFSmDQ56kjMxiIK4umVxyXp37LdjlKYrEz2QDWJkx8XhvUo9y6alWhgnCiEOVZqo8fwluDiQ542DaOLPQA > ~/.aws/token
brew install terraform
aws eks --region us-east-1 update-kubeconfig --name finout-staging-clickhouse-cluster --role arn:aws:iam::277411487094:role/k8sAdmin
aws configure
aws eks --region us-east-1 update-kubeconfig --name finout-staging-clickhouse-cluster --role arn:aws:iam::277411487094:role/k8sAdmin --profile apono
brew install kubectx
kdelsec s3-aws-credentials-2
kesec s3-aws-credentials
k edit secret s3-aws-credentials
kgsecy s3-aws-credentials
kgsec -o json s3-aws-credentials | jq
brew install jq
kgsec -o json s3-aws-credentials | jq -r '.data.s3\.xml'
kgsec -o json s3-aws-credentials | jq -r '.data.s3.xml'
kgsec -o json s3-aws-credentials | jq -r '.data'
update-ca-certificates
terraform init --debug
terraform init -debug
aws s3 ls s3://finout-data-staging
aws sts get-caller-identity --profile=apono
terraform init -backend=false
AWS_PROFILE=apono terraform help
AWS_PROFILE=apono terraform -help
AWS_PROFILE=apono terraform init -deub
AWS_PROFILE=apono terraform init -debug
TF_LOG=debug AWS_PROFILE=apono terraform init
aws --profile apono configure set role_arn arn:aws:iam::277411487094:role/yizhar_apono-apono-oidc-role && aws --profile apono configure set web_identity_token_file ~/.aws/token && echo eyJhbGciOiAiUlMyNTYiLCAidHlwIjogIkpXVCJ9.eyJ1c2VyX25hbWUiOiAieWl6aGFyX2Fwb25vLWFwb25vLW9pZGMtcm9sZSIsICJpc3MiOiAiaHR0cHM6Ly9hcG9uby1vaWRjLnMzLmFtYXpvbmF3cy5jb20vYjU1Njk4MzAtYWU4NC0xMWVjLWFlOGUtMGViNmRiMjNkYTQ1IiwgImlhdCI6IDE2Nzc2NTY3NzMsICJleHAiOiAxNjc3NzE0MzY5LCAiYXVkIjogIkFXUyIsICJzdWIiOiAieWl6aGFyX2Fwb25vLWFwb25vLW9pZGMtcm9sZSJ9.EkrexIAy6lIUJH1W_Pd1a9lFf9d-p0gOVsiQZyRnuo4y2_U7pUaGLevfeDsoiYijPHZele5QISaMxz_7dhi87DdO4MMML2pxNjIrsNQCMxYHmDlKltZvOayaxZfiMIu_EbdjdMR1huuE8BxYf-f1cIgQ3Z3IsEW4ZJajZ5n9l-IYMgWKkvpfGihFbZJDhTh1H8twFcqC3VwLPlkdmrfLb8bsxeQQ9iSRgxQ11VBOApozCIcY_vxtluDodH3odCyfOQ9czFSmDQ56kjMxiIK4umVxyXp37LdjlKYrEz2QDWJkx8XhvUo9y6alWhgnCiEOVZqo8fwluDiQ542DaOLPQA > ~/.aws/token
brew install --cask perimeter81
brew uninstall perimeter81
ifconfig
cd finout
aws s3 ls s3://finout-data-staging --profile=apono
aws s3 ls s3://finout-terraform-state --profile=apono
ls ..
ls ../..
ls ../../environments
ls ../../environments/staging.tfvars
aws s3 ls s3://finout-terraform-state
aws s3 ls s3://finout-terraform-state/staging/clickhouse/
terraform show
terraform show -no-color
vim provider.tf
terraform plan -var=../../environments/staging.tfvars
kgcm -o yaml aws-auth
AWS_PROFILE=apono terraform plan
cd node-group
brew install helm
AWS_PROFILE=apono terraform show | pbcopy
aws eks --region us-east-1 update-kubeconfig --name finout-staging-test --role arn:aws:iam::277411487094:role/k8sStagingAdmin --profile apono
aws eks --region us-east-1 update-kubeconfig --name finout-staging-test --role arn:aws:iam::277411487094:role/k8sAdmin --profile apono
kubectx arn:aws:eks:us-east-1:277411487094:cluster/finout-staging-clickhouse=finout-clickhouse-staging
kubectx finout-clickhouse-staging=arn:aws:eks:us-east-1:277411487094:cluster/finout-staging-clickhouse
ked clickhouse
kdp clickhouse-deployment-5dcddc7986-n78jf
klf clickhouse-deployment-75587d7674-5jcjl
k edit cm clickhouse-network
kdelp clickhouse-deployment-75587d7674-5jcjl
kgfp
kexecp
keti clickhouse-deployment-dd4f5f94-4dnjn
keti clickhouse-deployment-dd4f5f94-4dnjn bash
kdlep clickhouse-deployment-dd4f5f94-4dnjn
kdelp clickhouse-deployment-dd4f5f94-4dnjn
kdelp clickhouse-deployment-7766454c86-7swjk
kdrs clickhouse-deployment-7766454c86
kdelrs clickhouse-deployment-7766454c86
k delete rs clickhouse-deployment-7766454c86
kdelp clickhouse-deployment-7766454c86-nnf6v
kdp clickhouse-deployment-7766454c86-wg5nr
kgcm -o yaml clickhouse-users
kgcm -o yaml clickhouse-network
keti clickhouse-deployment-dd4f5f94-7tcbn bash
kdelp clickhouse-deployment-dd4f5f94-6dpjg
klf clickhouse-deployment-dd4f5f94-724sc
kdelp clickhouse-deployment-dd4f5f94-724sc
kdelp clickhouse-deployment-5dcddc7986-dgk69
ked 
keti clickhouse-deployment-75587d7674-gnnds
keti clickhouse-deployment-75587d7674-gnnds bash
terraform plan -var-file=../../environments/staging.tfvars
aws eks get-token --cluster-name finout-staging-clickhouse
aws eks get-token --cluster-name finout-staging-clickhouse --profile=apono
AWS_PROFILE=apono terraform plan -var-file=../../environments/staging.tfvars
AWS_PROFILE=apono terraform destroy -var-file=../../environments/staging.tfvars
rm -rf .terraform
aws eks get-token --cluster-name staging-clickhouse-test --profile=apono
kubectl staging-test=.
klf clickhouse-deployment-75587d7674-h7f8w
kdelp clickhouse-deployment-75587d7674-h7f8w
klf clickhouse-deployment-bdc495bc9-qnjpc
kdelp clickhouse-deployment-bdc495bc9-qnjpc
klf clickhouse-deployment-5fc8cf5f6d-f5zcr
klf clickhouse-deployment-dd4f5f94-4dnjn
klf clickhouse-deployment-dd4f5f94-7tcbn
klf clickhouse-deployment-75587d7674-gnnds
klf clickhouse-deployment-579bbd76d5-jdkk6
keti clickhouse-deployment-579bbd76d5-jdkk6
keti clickhouse-deployment-579bbd76d5-jdkk6 bash
kecm clickhouse-network
kdelp clickhouse-deployment-579bbd76d5-jdkk6
keti clickhouse-deployment-579bbd76d5-dxn7m
klf clickhouse-deployment-579bbd76d5-dxn7m
kgp -l app=clickhouse
kdp 
keti app=finout-clickhouse bash
keti -l app=finout-clickhouse bash
keti clickhouse-deployment-579bbd76d5-dxn7m bash
klf clickhouse-deployment-5jjhl
keti clickhouse-deployment-6846d77997-tndwh bash
keti clickhouse-deployment-5jjhl
keti clickhouse-deployment-5jjhl bash
keti clickhouse-deployment-557n2
keti clickhouse-deployment-557n2 bash
vim ch.yaml
kaf ch.yaml
kdelp clickhouse-deployment-6846d77997-pqdcl
klf clickhouse-deployment-b89488f9-mpqf4
kdlep clickhouse-deployment-6846d77997-wj68n
kdelp clickhouse-deployment-6846d77997-wj68n
keti clickhouse-deployment-b89488f9-5jxxt bash
kdelp clickhouse-deployment-b89488f9-5jxxt
kdelp clickhouse-deployment-b89488f9-mpqf4
kdelp clickhouse-deployment-97d746bf6-wdnjk
kdelp clickhouse-deployment-97d746bf6-2n8w9
keti clickhouse-deployment-b89488f9-ckdcm bash
kdelp clickhouse-deployment-b89488f9-ckdcm
keti clickhouse-deployment-97d746bf6-d489x
cd /etc/clickhouse-server/config.d
lsls
keti clickhouse-deployment-97d746bf6-d489x bash
kdelp clickhouse-deployment-b89488f9-vm2rz
kdelp clickhouse-deployment-97d746bf6-lqgmb
kdelp clickhouse-deployment-97d746bf6-fj2vp
kdp clickhouse-deployment-97d746bf6-fj2vp
kgn
keds
keti clickhouse-deployment-68gdp
keti clickhouse-deployment-68gdp bash
cd clickhouse/
keti clickhouse-deployment-5btlm
keti clickhouse-deployment-5btlm bash
kdelp clickhouse-deployment-5btlm
keti clickhouse-deployment-j5xqp bash
kdlep clickhouse-deployment-j5xqp
kdelp clickhouse-deployment-j5xqp
keti clickhouse-deployment-lxpgg bash
AWS_PROFILE=apono terraform apply -var-file=../../environments/staging.tfvars
keti clickhouse-deployment-9m6qb bash
cd. .
git commit -m "clickhouse terraform overhaul"
kgsa aws-load-balancer-webhook-service -o yaml
kgsa aws-load-balancer-webhook-service -n kube-system -o yaml
kgsa aws-load-balancer-webhook-service -n kube-system
kgsa -n kube-system aws-load-balancer-webhook-service
kgsa -n kube-system
k get serviceaccount
k get serviceaccount -n kube-system
k get serviceaccount -n kube-system aws-load-balancer-controller-tf -o yaml
k get secret aws-load-balancer-controller-tf-token-ggjr2 -o json
pbpaste | base64 -D
networksetup -setdnsservers Wi-Fi 8.8.8.8
networksetup -setdnsservers Wi-Fi 1.1.1.1
mv ~/.config/nvim ~/.config/nvim.bak
git clone https://github.com/AstroNvim/AstroNvim ~/.config/nvim\

nvim +PackerSync
rm -rf ~/.config/nvim
mv ~/.local/share/nvim.bak ~/.local/share/nvim
mv ~/.config/nvim.bak ~/.config/nvim
mkdir tfctx
cargo install testcontainers
Sh4m3l3ss
cargo install testcontainers-rs
cargo --help
crate --help
docker rm -f serene_feynman
 /Users/dormunis/.vscode/extensions/vadimcn.vscode-lldb-1.8.1/adapter/codelldb terminal-agent --connect=63308 
 /Users/dormunis/.vscode/extensions/vadimcn.vscode-lldb-1.8.1/adapter/codelldb terminal-agent --connect=63254 
docker rm -f silly
docker rm -f silly_wozniak bold_banach
docker logs vigilant_zhukovsky
curl http://localhost:4566
curl http://localhost:33137
curl http://localhost:33137/
docker rm vigilant_zhukovsky
docker rm -f vigilant_zhukovsky
docker rm -f cranky_benz
docker images
docker pull hello-world
docke rps -a
docker rm funny_hermann quizzical_chaplygin
docker logs jovial_goldstine
docker rm -f jovial_goldstine
 /Users/dormunis/.vscode/extensions/vadimcn.vscode-lldb-1.8.1/adapter/codelldb terminal-agent --connect=63846 
 /Users/dormunis/.vscode/extensions/vadimcn.vscode-lldb-1.8.1/adapter/codelldb terminal-agent --connect=64065 
docke rps
docker rm minikube
docker pull localstack
docker pull localstack/localstack
rust test
carge update
cargo update
docker ps -a | awk '{print $4}'
docker ps -a | awk '{print $5}'
docker ps -a | awk '{print $6}'
docker ps -a | awk '{print $7}'
docker ps -a | awk '{print $8}'
docker ps -a | awk '{print $10}'
docker rm confident_lederberg infallible_liskov crazy_maxwell
docker rm -f confident_lederberg infallible_liskov crazy_maxwell
docker rm eloquent_hofstadter
docker rm -f eloquent_hofstadter
ls -a
cd tests
terraform apply
ls ~/.aws
cat ~/.aws/config
cat ~/.aws/token
rm ~/.aws/token
vim ~/.aws/config
ls ~/.aws/cli
ls ~/.aws/cli/cache
vim ~/.aws/cli/cache/f9c8f0ada73b770f6880635ff9f70d63d804909e.json
rm ~/.aws/cli/cache/f9c8f0ada73b770f6880635ff9f70d63d804909e.json
cargo test
cargo add clap
cargo add clap --features derive
cd target
cd de
cd debug
cd tfctx
tfctx
cd target/debug
./tfctx --help
./target/debug/tfctx -n dor -c 2
env | grep -i tfctx
./target/debug/tfctx --version
./target/debug/tfctx -n dor
echo $TFCTX_TERRAFORM_DIR
./target/debug/tfctx ctx test
./target/debug/tfctx ctx --help
./target/debug/tfctx --help
./target/debug/tfctx install --help
docker rm -f gooft_kepler
docker rm -f goofy_kepler
terraform init -help
terraform apply -help
exi
cd .git
vim config
vim description
cat HEAD
cat description
cat config
cat info/exclude
mv tfctx/ envctl
pipenv --help
./target/debug/envctl --help
./target/debug/envctl help
./target/debug/envctl --version
./target/debug/envctl install
./target/debug/envctl install terraform
cargo build
cargo add reqwest
cargo add futures_util
mv envctl tfctl
cd tfctl
cargo add tempfile
cargo remove error_chain
cargo remove error-chain
cargo add error_chain
ls /var/folders/lt/kkx8hhmn39320fpxwc7m_2700000gn/T/terraformiHGXsO/terraform_1.3.9_darwin_arm64.zip
ls /var/folders/lt/kkx8hhmn39320fpxwc7m_2700000gn/T/terraformiHGXsO
cargo add zip
chmox +x terraform
./terraform -version
chmod +x terraform
./terraform -help
rm terraform_1.3.9_darwin_arm64.zip f
which terraform
ls -l /opt/homebrew/bin/terraform
ls -l /opt/homebrew/Cellar/terraform/1.3.9/bin/terraform
cargo run
cd terraform
cd terraform_1.3.9_darwin_arm64.zip
rm terraform_1.3.9_darwin_arm64.zip
cargo add indicatif
https://github.com/login/device
cd dev/private/tfctx
cd dev/private/tfctl
vim src/main.rs
cargo run -- reverse foo
cargo run -- reverse bar
cargo run -- inspect lorem
cargo run -- inspect FooBar
cargo run -- inspect A1B2C3 --digits
cargo run -- --version
cargo run -- context test --help
cargo run -- context
cargo run -- context test
cargo run -- context --help
kubectl --help
cargo run -- add test
cargo run -- add --help
cargo run -- add context
cargo run -- add context --help
cargo run -- add context test
cargo run -- run --help
cargo run -- install 
cargo run -- install 1.3.7
git remote add origin git@github.com:dormunis/tfctl.git\

git branch -M main
ls ../../profiles
aws --profile apono configure set role_arn arn:aws:iam::277411487094:role/yizhar_apono-apono-oidc-role && aws --profile apono configure set web_identity_token_file ~/.aws/token && echo eyJhbGciOiAiUlMyNTYiLCAidHlwIjogIkpXVCJ9.eyJ1c2VyX25hbWUiOiAieWl6aGFyX2Fwb25vLWFwb25vLW9pZGMtcm9sZSIsICJpc3MiOiAiaHR0cHM6Ly9hcG9uby1vaWRjLnMzLmFtYXpvbmF3cy5jb20vYjU1Njk4MzAtYWU4NC0xMWVjLWFlOGUtMGViNmRiMjNkYTQ1IiwgImlhdCI6IDE2NzgxNzM0NDMsICJleHAiOiAxNjc4MjMxMDMzLCAiYXVkIjogIkFXUyIsICJzdWIiOiAieWl6aGFyX2Fwb25vLWFwb25vLW9pZGMtcm9sZSJ9.GvGIOcHtmhWYcDp5DyMSgq_xSvfPrsNQXB1s7bHMRbZFdMKIrGFiD1m9E3q_zT4E0f2Dy92qz5_UWjZVsZ-6LKlHVU4gx4cv87afAWWBpImKV01Zc49gdxdnQY1AZR-ppp3UCuZOhP_YnjdTPtjhthHJgftKuPK74tPiasHOzUwQ628gAEwnS6lT2P4SVeUgcaDrpANSHYoX1GyhZJiNvvI7ihCp1x-7ppY7FH5FEURSUiaUNkIR04i08X7gJPnHGrZ4rVPVF-a0jnO-RFEGyr5MY4iK_OZN_Ij_alb7zgoOSXmxufZzt_H7XUlRHu4rdJqq5raenZe_vQyEa-BMug > ~/.aws/token
cd dev/finout
cd components
terraform apply -vars-file=../../profiles/clickhouse-staging.tfvars
terraform apply -var-file=../../profiles/clickhouse-staging.tfvars
kubectl describe daemonset aws-node --namespace kube-system\

kgp -n kube-system | grep -i vpc
kgp -n kube-system | grep -i subnet
kubectl describe daemonset aws-node --namespace kube-system | grep -i -e subnet -e vpc
kubectl describe daemonset aws-node --namespace kube-system | grep -i -e subnet
kds aws-load-balancer-webhook-service
kds finout-clickhouse-service-net 
kdd -n kube-system aws-load-balancer-controller
kgp -n kube-system 
cd staging
cd clickhouse-cluster
AWS_PROFILE=apono terraform show
klf aws-load-balancer-controller-765987f468-pkz7p
kgsa -A
aws eks --region us-east-1 update-kubeconfig --name prod-clickhouse-03 --role arn:aws:iam::277411487094:role/k8sAdmin --profile apono
aws eks --region us-east-1 update-kubeconfig --name prod-clickhouse03 --role arn:aws:iam::277411487094:role/k8sAdmin --profile apono
kubectx finout-clickhouse-prod03=arn:aws:eks:us-east-1:277411487094:cluster/finout-prod-clickhouse03
kubectx finout-prod-clickhouse03
kubectx finout-clickhouse03
finout-clickhouse-prod03
kgs -
kgd finout-clickhouse-service-net -n clickhouse
kgs finout-clickhouse-service-net -n clickhouse
kdels -n clickhouse finout-clickhouse-service-net
kdp aws-load-balancer-controller-6c4b979f96-kmq2k
kdp aws-load-balancer-controller-6c4b979f96-kmq2k -n kube-system
klf -l app.kubernetes.io/name=aws-load-balancer-controller
klf -l app.kubernetes.io/name=aws-load-balancer-controller -n kube-system
kgds
kdp aws-load-balancer-controller-6c4b979f96-25w86
AWS_PROFILE=apono terraform apply -var-file=../../environments/clickhouse-staging.tfvars
terraform init -upgrade
eksctl
brew upgrade eksctl && { brew link --overwrite eksctl; } || { brew tap weaveworks/tap; brew install weaveworks/tap/eksctl; }\

eksctl utils associate-iam-oidc-provider --cluster ${var.cluster_name} --approve
AWS_PROFILE=apono eksctl utils associate-iam-oidc-provider --cluster staging-clickhouse-test --approve
AWS_PROFILE=apono eksctl utils associate-iam-oidc-provider --cluster staging-clickhouse-test --region us-east-1 --approve
eksctl create iamserviceaccount --help
eksctl create iamserviceaccount --cluster=staging-clickhouse-test --namespace=kube-system --name=aws-load-balancer-controller-tf --role-name=AmazonEKSLoadBalancerControllerRoleFinoutClickhouse --attach-policy-arn=arn:aws:iam::277411487094:policy/AWSLoadBalancerControllerIAMPolicyV2 --override-existing-serviceaccounts --approve
AWS_PROFILE=apono eksctl create iamserviceaccount --cluster=staging-clickhouse-test --region us-east-1 --namespace=kube-system --name=aws-load-balancer-controller-tf --role-name=AmazonEKSLoadBalancerControllerRoleFinoutClickhouse --attach-policy-arn=arn:aws:iam::277411487094:policy/AWSLoadBalancerControllerIAMPolicyV2 --override-existing-serviceaccounts --approve
AWS_PROFILE=apono eksctl create iamserviceaccount -o json
AWS_PROFILE=apono eksctl create iamserviceaccount --ouput
AWS_PROFILE=apono eksctl create iamserviceaccount --output
AWS_PROFILE=apono eksctl create iamserviceaccount --cluster=staging-clickhouse-test --region=us-east-1 --namespace=kube-system --name=aws-load-balancer-controller-tf --role-name=AmazonEKSLoadBalancerControllerRoleFinoutClickhouse --attach-policy-arn=arn:aws:iam::277411487094:policy/AWSLoadBalancerControllerIAMPolicyV2 --override-existing-serviceaccounts --approve
AWS_PROFILE=apono eksctl create iamserviceaccount --cluster=staging-clickhouse-test --namespace=kube-system --name=aws-load-balancer-controller-tf --role-name=AmazonEKSLoadBalancerControllerRoleFinoutClickhouse --attach-policy-arn=arn:aws:iam::277411487094:policy/AWSLoadBalancerControllerIAMPolicyV2 --override-existing-serviceaccounts --approve
AWS_PROFILE=apono eksctl utils associate-iam-oidc-provider --region=us-east-1 --cluster=staging-clickhouse-test
AWS_PROFILE=apono eksctl utils associate-iam-oidc-provider --region=us-east-1 --cluster=staging-clickhouse-test --approve
AWS_PROFILE=apono eksctl create iamserviceaccount --cluster=staging-clickhouse-test --namespace=kube-system --name=aws-load-balancer-controller-tf --role-name=AmazonEKSLoadBalancerControllerRoleFinoutClickhouse --attach-policy-arn=arn:aws:iam::277411487094:policy/AWSLoadBalancerControllerIAMPolicyV2 --override-existing-serviceaccounts --region us-east-1 --approve
AWS_PROFILE=apono terraform apply -var-file=../../profiles/clickhouse-staging.tfvars
brew install zoom
cd go
cd pkg
rm -rf go
klf aws-load-balancer-controller-6c4b979f96-m82gt
klf aws-load-balancer-controller-6c4b979f96-m82gt -n kube-system
kdelp aws-load-balancer-controller-6c4b979f96-m82gt
×ƒ¼×ƒ‚×¤
k get role
k describe role aws-load-balancer-controller-leader-election-role
k get targetrulebinding
kubectx finout-clickhouse-staigng
k get targetgroupbinding
k get targetgroupbinding -A
k get ingressclassparams -A
k describe ingressclassparams alb
kdelp aws-load-balancer-controller-6c4b979f96-kmq2k
echo "arn:aws:iam::111122223333:oidc-provider/oidc.eks.region-code.amazonaws.com/id/EXAMPLED539D4633E53DE1B71EXAMPLE"\

AWS_PROFILE=apono terraform console
klf aws-load-balancer-controller-7f874bffdd-tqwvb
curl -Lo v2_4_7_full.yaml https://github.com/kubernetes-sigs/aws-load-balancer-controller/releases/download/v2.4.7/v2_4_7_full.yaml\

cat v2_4_7_full.yaml
vim v2_4_7_full.yaml
rm v2_4_7_full.yaml
klf aws-load-balancer-controller-7f874bffdd-tqwvb -n kube-system
kdelp aws-load-balancer-controller-7f874bffdd-tqwvb
kdelp aws-load-balancer-controller-7f874bffdd-tqwvb -n kube-system
klf aws-load-balancer-controller-7f874bffdd-54t5x
kgd finout-clickhouse-service-net
kgs finout-clickhouse-service-net
klf aws-load-balancer-controller-7f874bffdd-54t5x -n kube-system
kdelp aws-load-balancer-controller-7f874bffdd-54t5x -n kube-system
kdelp aws-load-balancer-controller-7f874bffdd-wcv2x
klf aws-load-balancer-controller-7f874bffdd-tbl22
klf aws-load-balancer-controller-7f874bffdd-7t7n4
kdelp aws-load-balancer-controller-7f874bffdd-tbl22 aws-load-balancer-controller-7f874bffdd-7t7n4
kgpo
klf aws-load-balancer-controller-7f874bffdd-bndg4
klf aws-load-balancer-controller-7f874bffdd-69cps
kdelp aws-load-balancer-controller-7f874bffdd-69cps
kdelp aws-load-balancer-controller-7f874bffdd-4xkx9
kdelp aws-load-balancer-controller-7f874bffdd-bndg4   1/1     Running   0             6m20s
kdelp aws-load-balancer-controller-7f874bffdd-bndg4
kdeld aws-load-balancer-controller
k del sa aws-load-balancer-controller
k delete sa aws-load-balancer-controller
k edit sa aws-load-balancer-controller
terraform destroy -target helm_release.eks-alb
AWS_PROFILE=apono terraform destroy -target helm_release.eks-alb
AWS_PROFILE=apono terraform destroy -target helm_release.eks-alb -var-file=profiles/staging-clickhouse.tfvars
ls pro
AWS_PROFILE=apono terraform destroy -target helm_release.eks-alb -var-file=profiles/staging.tfvars
AWS_PROFILE=apono terraform destroy -target kubernetes_service.clickhouse
AWS_PROFILE=apono terraform destroy -target kubernetes_service.clickhouse -target kubernetes_service_account.aws_load_balancer_controller
AWS_PROFILE=apono terraform destroy -target kubernetes_service.clickhouse -target kubernetes_service_account.aws_load_balancer_controller -var-file=profiles/staging.tfvars
k describe sa aws-load-balancer-controller
kgsa
kaf sa.yaml
kdelsa aws-load-balancer-controller
kdsa aws-load-balancer-controller-tf
kdsa aws-load-balancer-controller-tf -n kube-system
kds -n clickhouse finout-clickhouse-service-net
klf aws-load-balancer-controller-7f874bffdd-bghxf
klf aws-load-balancer-controller-7f874bffdd-ljlz8
kgsec -o yaml aws-load-balancer-tl
kgsec -o yaml aws-load-balancer-tl -n kube-system
kgsa -o yaml aws-load-balancer-tl -n kube-system
k get sa -o yaml aws-load-balancer-tl -n kube-system
k get sa -o yaml aws-load-balancer-controller -n kube-system
k get sa 
k get sa -o yaml aws-load-balancer-controller-tf
kubectx finout-clickhouse-staging
kdd aws-load-balancer-controller
kgrs
kdrs aws-load-balancer-controller-7f874bffdd
ked aws-load-balancer-controller
klf aws-load-balancer-controller-7f874bffdd-vzssr
klf aws-load-balancer-controller-7f874bffdd-qdgrs
klf aws-load-balancer-controller-7f874bffdd-z54s6
kgsec -A | grep balancer
kgsec aws-load-balancer-tls -o json
k get sa aws-load-balancer-controller -o yaml
k get sa aws-load-balancer-controller-tf -o yaml
k get sa -n kube-system aws-load-balancer-controller -o yaml
k get sec 
kubectx finout-clickhouse-prod03
kgsec -n kube-system
kgsec -n kube-system -o yaml er-controller.v1   helm.sh/release.v1   1      14m\
âƒ¾ƒ¼ clickhouse (refactor/clickhouse)
cd dev/finout/finout-terraform/components/clickhouse
AWS_PROFILE=apono terraform init -upgrade
AWS_PROFILE=apono eksctl utils associate-iam-oidc-provider --region us-east-1 --cluster staging-clickhouse-test --approve
AWS_PROFILE=apono eksctl create iamserviceaccount --cluster=staging-clickhouse-test --namespace=kube-system --name=aws-load-balancer-controller-tf --role-name=AmazonEKSLoadBalancerControllerRole-staging-clickhouse-test --attach-policy-arn=arn:aws:iam::277411487094:policy/AWSLoadBalancerControllerIAMPolicyV2 --override-existing-serviceaccounts --region us-east-1 --approve
AWS_PROFILE=apono eksctl create iamserviceaccount --cluster=staging-clickhouse-test --namespace=kube-system --name=aws-load-balancer-controller --role-name=AmazonEKSLoadBalancerControllerRole-staging-clickhouse-test --attach-policy-arn=arn:aws:iam::277411487094:policy/AWSLoadBalancerControllerIAMPolicyV2 --override-existing-serviceaccounts --region us-east-1 --approve
AWS_PROFILE=apono eksctl create iamserviceaccount --cluster=staging-clickhouse-test --namespace=kube-system --name=aws-load-balancer-controller --role-name=AmazonEKSLoadBalancerControllerRoleFinoutClickhouse --attach-policy-arn=arn:aws:iam::277411487094:policy/AWSLoadBalancerControllerIAMPolicyV2 --override-existing-serviceaccounts --region us-east-1 --approve
AWS_PROFILE=apono terraform import aws_iam_role.load-balancer-role AmazonEKSLoadBalancerControllerRole-staging-clickhouse-test
AWS_PROFILE=apono terraform import aws_iam_role.load-balancer-role AmazonEKSLoadBalancerControllerRole-staging-clickhouse-test -var-file=profiles/staging.tf
AWS_PROFILE=apono terraform import aws_iam_role.load-balancer-role AmazonEKSLoadBalancerControllerRole-staging-clickhouse-test -var-file=profiles/staging.tfvars
AWS_PROFILE=apono terraform import aws_iam_role.load-balancer-role arn:aws:iam::277411487094:role/AmazonEKSLoadBalancerControllerRole-staging-clickhouse-test
AWS_PROFILE=apono terraform import aws_iam_role.load-balancer-role arn:aws:iam::277411487094:role/AmazonEKSLoadBalancerControllerRole-staging-clickhouse-test -var-file=profiles/staging.tfvars
AWS_PROFILE=apono terraform import aws_iam_role.load-balancer-role arn:aws:iam::277411487094:role/AmazonEKSLoadBalancerControllerRole-staging-clickhouse-test -var-file="profiles/staging.tfvars"
AWS_PROFILE=apono terraform import -var-file=profiles/staging.tfvars aws_iam_role.load-balancer-role arn:aws:iam::277411487094:role/AmazonEKSLoadBalancerControllerRole-staging-clickhouse-test
AWS_PROFILE=apono terraform import -var-file=profiles/staging.tfvars aws_iam_role.load-balancer-role AmazonEKSLoadBalancerControllerRole-staging-clickhouse-test
kdsa
eksctl create iamserviceaccount \\
  --cluster=staging-clickhouse-test \\
  --namespace=kube-system \\
  --name=aws-load-balancer-controller \\
  --role-name AmazonEKSLoadBalancerControllerRole-staging-clickhouse-test \\
  --attach-policy-arn=arn:aws:iam::277411487094:policy/AmazonEKSLoadBalancerControllerRole-staging-clickhouse-test \\
  --approve
AWS_PROFILE=apono eksctl create iamserviceaccount \\
  --cluster=staging-clickhouse-test \\
  --namespace=kube-system \\
  --name=aws-load-balancer-controller \\
  --role-name AmazonEKSLoadBalancerControllerRole-staging-clickhouse-test \\
  --attach-policy-arn=arn:aws:iam::277411487094:policy/AmazonEKSLoadBalancerControllerRole-staging-clickhouse-test \\
  --approve
AWS_PROFILE=apono eksctl create iamserviceaccount \\
  --cluster=staging-clickhouse-test \\
  --namespace=kube-system \\
  --name=aws-load-balancer-controller \\
  --role-name AmazonEKSLoadBalancerControllerRole-staging-clickhouse-test \\
  --attach-policy-arn=arn:aws:iam::277411487094:policy/AmazonEKSLoadBalancerControllerRole-staging-clickhouse-test \\
  --region us-east-1 --approve --override-existing-serviceaccounts
k get sa -n kube-system
k get sa -A | grep controller
k get sa -A | grep balancer
AWS_PROFILE=apono eksctl create iamserviceaccount \\
  --cluster=staging-clickhouse-test \\
  --namespace=kube-system \\
  --name=aws-load-balancer-controller \\
  --role-name AmazonEKSLoadBalancerControllerRole-staging-clickhouse-test \\
  --attach-policy-arn=arn:aws:iam::277411487094:policy/AmazonEKSLoadBalancerControllerRole-staging-clickhouse-test \\
  --region us-east-1 --approve
AWS_PROFILE=apono terraform destroy -target aws_iam_role.load-balancer-role -var-file=../../environments/staging.tfvars
AWS_PROFILE=apono terraform destroy -target aws_iam_role.load-balancer-role -var-file=profiles/staging.tfvars
curl -O https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.4.7/docs/install/iam_policy_us-gov.json\

aws iam create-policy \\
    --policy-name AWSLoadBalancerControllerIAMPolicyTest \\
    --policy-document file://iam_policy.json
aws iam create-policy \\
    --policy-name AWSLoadBalancerControllerIAMPolicyTest \\
    --policy-document file://iam_policy_us-gov.json
aws iam create-policy \\
    --policy-name AWSLoadBalancerControllerIAMPolicyTest \\
    --policy-document file://iam_policy_us-gov.json --profile=apono
vim iam_policy_us-gov.json
aws iam create-policy \\
    --policy-name AWSLoadBalancerControllerIAMPolicyTest \\
    --policy-document file://iamserviceaccount-policy.json --profile=apono
helm repo --help
helm repo list
helm repo add eks https://aws.github.io/eks-charts\

AWS_PROFILE=apono eksctl create iamserviceaccount --help
eksctl update iamserviceaccount
AWS_PROFILE=apono eksctl update iamserviceaccount \\
  --cluster=staging-clickhouse-test \\
  --namespace=kube-system \\
  --name=aws-load-balancer-controller \\
  --role-name AmazonEKSLoadBalancerControllerRole-staging-clickhouse-test \\
  --attach-policy-arn=arn:aws:iam::277411487094:policy/AWSLoadBalancerControllerIAMPolicyTest \\
  --region us-east-1 --approve --override-existing-serviceaccounts
AWS_PROFILE=apono eksctl update iamserviceaccount \\
  --cluster=staging-clickhouse-test \\
  --namespace=kube-system \\
  --name=aws-load-balancer-controller \\
  --attach-policy-arn=arn:aws:iam::277411487094:policy/AWSLoadBalancerControllerIAMPolicyTest \\
  --region us-east-1 --approve --override-existing-serviceaccounts
AWS_PROFILE=apono eksctl update iamserviceaccount \\
  --cluster=staging-clickhouse-test \\
  --namespace=kube-system \\
  --name=aws-load-balancer-controller \\
  --attach-policy-arn=arn:aws:iam::277411487094:policy/AWSLoadBalancerControllerIAMPolicyTest \\
  --region us-east-1 --approve
eksctl --help
AWS_PROFILE=apono eksctl delete iamserviceaccount \\
  --cluster=staging-clickhouse-test \\
  --namespace=kube-system \\
  --name=aws-load-balancer-controller \\
  --role-name AmazonEKSLoadBalancerControllerRole-staging-clickhouse-test \\
  --attach-policy-arn=arn:aws:iam::277411487094:policy/AWSLoadBalancerControllerIAMPolicyTest \\
  --region us-east-1 --approve
AWS_PROFILE=apono eksctl delete iamserviceaccount \\
  --cluster=staging-clickhouse-test \\
  --namespace=kube-system \\
  --name=aws-load-balancer-controller \\
  --attach-policy-arn=arn:aws:iam::277411487094:policy/AWSLoadBalancerControllerIAMPolicyTest \\
  --region us-east-1 --approve
AWS_PROFILE=apono eksctl delete iamserviceaccount \\
  --cluster=staging-clickhouse-test \\
  --namespace=kube-system \\
  --name=aws-load-balancer-controller \\
  --region us-east-1 --approve
AWS_PROFILE=apono eksctl create iamserviceaccount \\
  --cluster=staging-clickhouse-test \\
  --namespace=kube-system \\
  --name=aws-load-balancer-controller \\
  --role-name AmazonEKSLoadBalancerControllerRole-staging-clickhouse-test \\
  --attach-policy-arn=arn:aws:iam::277411487094:policy/AWSLoadBalancerControllerIAMPolicyTest \\
  --region us-east-1 --approve --override-existing-serviceaccounts
helm install --upgrade aws-load-balancer-controller eks/aws-load-balancer-controller \\
  -n kube-system \\
  --set clusterName=staging-clickhouse-test \\
  --set serviceAccount.create=false \\
  --set serviceAccount.name=aws-load-balancer-controller 
helm update aws-load-balancer-controller eks/aws-load-balancer-controller \\
  -n kube-system \\
  --set clusterName=staging-clickhouse-test \\
  --set serviceAccount.create=false \\
  --set serviceAccount.name=aws-load-balancer-controller 
helm upgrade aws-load-balancer-controller eks/aws-load-balancer-controller \\
  -n kube-system \\
  --set clusterName=staging-clickhouse-test \\
  --set serviceAccount.create=false \\
  --set serviceAccount.name=aws-load-balancer-controller 
klf aws-load-balancer-controller-7f874bffdd-nvjt4
AWS_PROFILE=apono eksctl create iamserviceaccount \\
  --cluster=staging-clickhouse-test \\
  --namespace=kube-system \\
  --name=aws-load-balancer-controller \\
  --role-name AmazonEKSLoadBalancerControllerRole-staging-clickhouse-test \\
  --attach-policy-arn=arn:aws:iam::277411487094:policy/AWSLoadBalancerControllerIAMPolicyTest \\
  --region us-east-1 --approve
k get secrets -n kube-system
helm install aws-load-balancer-controller eks/aws-load-balancer-controller \\
  -n kube-system \\
  --set clusterName=staging-clickhouse-test \\
  --set serviceAccount.create=false \\
  --set serviceAccount.name=aws-load-balancer-controller 
k get secrets -A
AWS_PROFILE=apono terraform apply -replace module.eks -var-file=profiles/staging.tfvars
AWS_PROFILE=apono terraform apply -replace=module.eks -var-file=profiles/staging.tfvars
AWS_PROFILE=apono terraform taint module.eks -var-file=profiles/staging.tfvars
kdels finout-clickhouse-service-net -n clickhouse
AWS_PROFILE=apono eksctl delete iamserviceaccount \\
  --cluster=staging-clickhouse-test \\
  --namespace=kube-system \\
  --name=aws-load-balancer-controller \\
  --region us-east-1
helm uninstall aws-load-balancer-controller
AWS_PROFILE=apono terraform destroy -target kubernetes_service_account.aws_load_balancer_controller -var-file=profiles/staging.tfvars
kdsec -n kube-system aws-load-balancer-tls
kdsa aws-load-balancer-controller
kdsa aws-load-balancer-controller -n kube-system
kgnpo
kgsec -n kube-system -o yaml aws-load-balancer-tls
kubectx staging
ked clickhouse-deployment
ked clickhouse-deployment -n kube-system
ked clickhouse-deployment -n clickhouse
kdp clickhouse-deployment-97d746bf6-fdwzm -n clickhouse
rm preprod/developers-airflow/tfstate
git commit -m "started working on airflow"
git config --global --edit
git commit -m "removed irrelevant variables"
aws --profile apono configure set role_arn arn:aws:iam::277411487094:role/yizhar_apono-apono-oidc-role && aws --profile apono configure set web_identity_token_file ~/.aws/token && echo eyJhbGciOiAiUlMyNTYiLCAidHlwIjogIkpXVCJ9.eyJ1c2VyX25hbWUiOiAieWl6aGFyX2Fwb25vLWFwb25vLW9pZGMtcm9sZSIsICJpc3MiOiAiaHR0cHM6Ly9hcG9uby1vaWRjLnMzLmFtYXpvbmF3cy5jb20vYjU1Njk4MzAtYWU4NC0xMWVjLWFlOGUtMGViNmRiMjNkYTQ1IiwgImlhdCI6IDE2NzgyNTU5MjgsICJleHAiOiAxNjc4MzEzNTI0LCAiYXVkIjogIkFXUyIsICJzdWIiOiAieWl6aGFyX2Fwb25vLWFwb25vLW9pZGMtcm9sZSJ9.VVF0ZYxKSt-4i2YcSFasFVsTDDd_LdKkv9-kpqXDOQ6moTy6xgQ5rqc2THyV-w44SiKmgYXMbCz4-ARMbYk4QkdO5s_QXBa_1LUqb-wnyGTrE-rFS4qmcgGYQoYlUMf7oPSdOG-hhURAJDR-zpuTawknFa8_HQGQ0vOaxe8nUQNxqbJ7rdg8O-0ixA2zF5KUfIs1Me3Lwahz9kRx2xqj-I23l3EWyI_0EJ7z_YxQEXU8voaOtsWLSDISyeAjnO4-FXWfuvTD15Zzcn4OpUCbt_auZLo1ujGFOSy0e_X4ATlD1-QAK19RT5e1NzTPz3DZeGfMf_olRvxofAr6S02A > ~/.aws/token
aws --profile apono configure set role_arn arn:aws:iam::277411487094:role/yizhar_apono-apono-oidc-role && aws --profile apono configure set web_identity_token_file ~/.aws/token && echo eyJhbGciOiAiUlMyNTYiLCAidHlwIjogIkpXVCJ9.eyJ1c2VyX25hbWUiOiAieWl6aGFyX2Fwb25vLWFwb25vLW9pZGMtcm9sZSIsICJpc3MiOiAiaHR0cHM6Ly9hcG9uby1vaWRjLnMzLmFtYXpvbmF3cy5jb20vYjU1Njk4MzAtYWU4NC0xMWVjLWFlOGUtMGViNmRiMjNkYTQ1IiwgImlhdCI6IDE2NzgyNTY5NTcsICJleHAiOiAxNjc4MzEzNTQxLCAiYXVkIjogIkFXUyIsICJzdWIiOiAieWl6aGFyX2Fwb25vLWFwb25vLW9pZGMtcm9sZSJ9.B3yS6Fq88qQdjY8k4ySX-3XiQdq1pK4d9ngrKGm_WJj8zh5tef_VrivCygP3fkBTwNsHQciypd_j8NQtiwrzwGUkkInZf2WtkdwI_w2ti5g56RCglXOlpFfgNxvRBr-KBRl6Isx2ZoShmmDn1xpWT7-q27aVp_IAvqkgT86TyIG6oZRRaBiiMB3kY5dKXTs4Wz7Qge2dt7z_91CKpDJW_10qhoQTQhVuh9GgYHZxwbc_i_AJkQ8-hsRV9_AaK3L8R-3nUnQ37KGk7LfSMMr79_YSJMB2K_dXfAl4saAdCCVGYBOw3y4G_wl2fd0BoTCjc5NcYwIU4oXUGrrwxWCJ2A > ~/.aws/token
kubectx --help
kubectx -d test
kubectx test=.
kubectx -d finout-clickhouse-prod03
kubectx -d finout-clickhouse-pstaging
kubectx -d finout-clickhouse-staging
aws eks --region us-east-1 update-kubeconfig --name finout-clickhouse-staging --role arn:aws:iam::277411487094:role/k8sAdmin --profile apono
kubectx arn:aws:eks:us-east-1:277411487094:cluster/finout-staging-clickhouse=staging
kubectx arn:aws:eks:us-east-1:277411487094:cluster/finout-staging-clickhouse
kubectx staging=.
kgsec -n kube-system | grep balancer
kdsec -n kube-system aws-load-balancer-tls 
kgsec -n kube-system aws-load-balancer-tls -o yaml
k get secrets -n kube-system 
k describe secrets -n kube-system aws-load-balancer-tls 
k get sa -n kube-system | grep balancer
kdsa -n kube-system aws-load-balancer-controller
AWS_PROFILE=apono terraform destroy -target kubernetes_service.clickhouse -target kubernetes_service_account.aws_load_balancer_controller -target helm_chart.eks-alb -var-file=profiles/staging.tfvars
TF_LOG=debug AWS_PROFILE=apono terraform apply -var-file=profiles/staging.tfvars
AWS_PROFILE=apono terraform apply -var-file=profiles/staging.tfvars
kubectx test
kepi clickhouse-deployment-qzpg5 -n clickhouse bash
vim $ZSH/aliases/kubectl.plugin.zsh
keds clickhouse-deployment -n clickhouse
keti clickhouse-deployment-qzpg5 -n clickhouse bash
git commit -m "finished clickhouse"
git push -u origin refactor/clickhouse
git commit -m "removed old clickhouse-cluster from staging/"
git push origin refactor/clickhouse
git checkout -b refactor/airflow
kubectx -d staging
kubectx clickhousetest=test
kubectx preprod=.
kubectx clickhousetest
cd preprod
cd developers-airflow
kgsec finout-ssh-git-secret -o json
kgsec finout-ssh-git-secret -o json | jq -r '.data.gitSshKey'
kgsec finout-ssh-git-secret -o json | jq -r '.data.gitSshKey' | base64 -D | pbcopy
kgsec finout-ssh-git-secret -o json | jq -r '.data.gitSshKey' | base64 -D 
wha
kubectx -D
kubectx lol=.
kubectx -d lol
kubectx prod=.
helm values --help
helm --help
aws s3 cp s3://finout-terraform-state/dd.yaml .
aws s3 cp s3://finout-terraform-state/pms.yaml  .
aws s3 rm s3://finout-terraform-state/pms.yaml
aws s3 rm s3://finout-terraform-state/dd.yaml
aws eks --region us-east-1 update-kubeconfig --name finout-staging-clickhouse --role arn:aws:iam::277411487094:role/k8sAdmin --profile apono
kgp -n clickhouse
jkgp
keti clickhouse-deployment-97d746bf6-n6g7m bash
kds finout-clickhouse-service-net -n clickhouse
kes finout-clickhouse-service-net
kds
kgs finout-clickhouse-service-net -o yaml > svc.yaml
kdels finout-clickhouse-service-net
kaf svc.yaml
kgs -n clickhouse
vim svc.yaml
cat svc.yaml > svc2.yaml
cat svc2.yaml
vim svc2.yaml
kaf svc2.yaml
mv svc2.yaml svc.yaml
kds finout-clickhouse-service-net
brew uninstall slack
cd dev/finout/finout-terraform/preprod/developers-airflow
cd prom-export-cron
kubectx prod
aws s3 cp s3://finout-terraform-state/airflow.yaml
aws s3 cp s3://finout-terraform-state/airflow.yaml .
aws s3 cp s3://finout-terraform-state/airflow.yaml . --profile=apono
aws s3 rm s3://finout-terraform-state/airflow.yaml . --profile=apono
aws s3 rm s3://finout-terraform-state/airflow.yaml --profile=apono
aws s3 cp s3://finout-terraform-state/key.txt . --profile=apono
aws s3 rm s3://finout-terraform-state/key.txt --profile=apono
kubectx preprod
helm ls -n airflow-preprod-avishai
helm get values -n airflow-preprod-avishai airflow > test.yaml
mv preprod/developers-airflow/prom-export-cron/test.yaml .
cd prod/airflow-cluster
cd scheduler-killer-cron
AWS_PROFILE=apono terraform show -no-color
cd task-clear-cron
AWS_PROFILE=apono terraform show -no-color | pbcopy
gi status
git commit -m "migrated everything, testing phase"
AWS_PROFILE=apono terraform init
git commit -m "fixed some stuff, onwards
git commit -m "fixed some stuff, onwards"
xcode-select â€ƒ³install
xcode-select â€ƒ³-install
xcode-select --install\

cargo add dirs
rm ~/.tfctl/bin/1.3.9
~/.tfctl/bin/1.3.9 --version
mv tfctl envctl
git remote remove origin
git remove add origin git@github.com:dormunis/envctl.git
git remote add origin git@github.com:dormunis/envctl.git
cargo add async-trait
cargo uninstall serde_json
cargo uninstall serde-json
cargo install serde_json
cargo add serde_json
cargo add serde_json --all-features
cargo add serde_json --features=*
cargo add serde_json --features *
 /Users/dormunis/.vscode/extensions/vadimcn.vscode-lldb-1.8.1/adapter/codelldb terminal-agent --connect=60892 
 /Users/dormunis/.vscode/extensions/vadimcn.vscode-lldb-1.8.1/adapter/codelldb terminal-agent --connect=60905 
 /Users/dormunis/.vscode/extensions/vadimcn.vscode-lldb-1.8.1/adapter/codelldb terminal-agent --connect=60919 
 /Users/dormunis/.vscode/extensions/vadimcn.vscode-lldb-1.8.1/adapter/codelldb terminal-agent --connect=60928 
 /Users/dormunis/.vscode/extensions/vadimcn.vscode-lldb-1.8.1/adapter/codelldb terminal-agent --connect=60939 
 /Users/dormunis/.vscode/extensions/vadimcn.vscode-lldb-1.8.1/adapter/codelldb terminal-agent --connect=60948 
 /Users/dormunis/.vscode/extensions/vadimcn.vscode-lldb-1.8.1/adapter/codelldb terminal-agent --connect=60958 
cargo add semver
cargo add semver --feature serde
cargo add semver --features serde
rm terraform_1.4.0_darwin_arm64.zip
rm -f terraform_1.4.0_darwin_arm64.zip
cd "/var/folders/lt/kkx8hhmn39320fpxwc7m_2700000gn/T/terraform"
cd /var/folders/lt/kkx8hhmn39320fpxwc7m_2700000gn/T/terraform
cd /var/folders/lt/kkx8hhmn39320fpxwc7m_2700000gn/T/
ls -l terraform
ls -lH terraform
ls -lh terraform
cd ~/.envctl/terraform/bin
./terraform -v
rm terraform
 /Users/dormunis/.vscode/extensions/vadimcn.vscode-lldb-1.8.1/adapter/codelldb terminal-agent --connect=61346 
 /Users/dormunis/.vscode/extensions/vadimcn.vscode-lldb-1.8.1/adapter/codelldb terminal-agent --connect=61359 
~/.envctl/terraform/bin/terraform -v
rm -f ~/.envctl/terraform/bin/terraform
rm -f ~/.envctl/terraform/bin/terraform_1.4.0_darwin_arm64.zip
~/.envctl/terraform/bin/terraform-1.4.0 -vers
~/.envctl/terraform/bin/terraform-1.4.0 -v
rm -f ~/.envctl/terraform/bin/*
git commit -m "finished install for now"
rm -f /Users/dormunis/.envctl/terraform/bin/terraform-1.4.0
cargo run -- uninstall 1.3.9
cargo run -- install 1.4.0
 /Users/dormunis/.vscode/extensions/vadimcn.vscode-lldb-1.8.1/adapter/codelldb terminal-agent --connect=61505 
 /Users/dormunis/.vscode/extensions/vadimcn.vscode-lldb-1.8.1/adapter/codelldb terminal-agent --connect=61518 
 /Users/dormunis/.vscode/extensions/vadimcn.vscode-lldb-1.8.1/adapter/codelldb terminal-agent --connect=61538 
 /Users/dormunis/.vscode/extensions/vadimcn.vscode-lldb-1.8.1/adapter/codelldb terminal-agent --connect=61545 
cargo run -- uninstall 
cargo run -- uninstall 1.4.0
ls ~/.envctl/terraform/bin
cargo run -- uninstall 1.4.0-rc1
curl -s https://api.github.com/repos/kubernetes/kubernetes/releases | jq -r '.[] | select(.prerelease == false) | select(.draft == false) | .tag_name + " " + .published_at' | head -n1\

curl -s https://api.github.com/repos/kubernetes/kubernetes/releases | jq -r '.[] | select(.prerelease == false) | select(.draft == false) | .tag_name + " " + .published_at'
cargo run -- install --versions
cargo run -- install 1.3.9
cargo run -- install 1.3.9 --rm
cargo run -- install 1.4.0 --rm
cargo run -- install kubernetes
cargo run -- install terraform 1.4.0
git commit --amend
cargo run -- install terraform
cargo run -- install terraform 1.4.0 --rm
cargo run -- install terraform 1.4.0 
cargo run -- install terraform --rm 1.4.0
cargo run -- install terraform --ls
git commit -m "cleaned up a bit"
cargo run -- install
cargo add structops
cargo add structopt
 /Users/dormunis/.vscode/extensions/vadimcn.vscode-lldb-1.8.1/adapter/codelldb terminal-agent --connect=51580 
 /Users/dormunis/.vscode/extensions/vadimcn.vscode-lldb-1.8.1/adapter/codelldb terminal-agent --connect=51597 
 /Users/dormunis/.vscode/extensions/vadimcn.vscode-lldb-1.8.1/adapter/codelldb terminal-agent --connect=51615 
~/.envctl/kubectl/bin/kubectl-v1.26.2 --version
~/.envctl/kubectl/bin/kubectl-v1.26.2 --help
~/.envctl/kubectl/bin/kubectl-v1.26.2 version
cargo run -- install kubectl ls
cargo run -- ls kubectl
cargo run -- list kubectl
cargo run -- list --help
cargo run -- install --help
cargo run -- install --ls
cargo run -- install --ls kubectl
ls -lH ~/.envctl/kubectl/bin
rm ~/.envctl/kubectl/bin/kubectl-
rm ~/.envctl/kubectl/bin/kubectl-ls
cargo run -- uninstall kubectl v1.26.2
cargo run -- install kubectl --rm
cargo run -- install kubectl
cargo run -- install kubectl --ls
cargo run -- install kubectl v1.26.2 --rm
cargo run -- install kubectl v1.26.2
git commit -m "added kubectl support"
curl -H "Accept: application/vnd.github.v3+json" https://api.github.com/repos/helm/helm/releases\

curl -H "Accept: application/vnd.github.v3+json" https://api.github.com/repos/helm/helm/releases | jq
curl -H "Accept: application/vnd.github.v3+json" https://api.github.com/repos/helm/helm/releases | jq -r > helm.json
cargo check
 /Users/dormunis/.vscode/extensions/vadimcn.vscode-lldb-1.8.1/adapter/codelldb terminal-agent --connect=51639 
 /Users/dormunis/.vscode/extensions/vadimcn.vscode-lldb-1.8.1/adapter/codelldb terminal-agent --connect=51659 
 /Users/dormunis/.vscode/extensions/vadimcn.vscode-lldb-1.8.1/adapter/codelldb terminal-agent --connect=51694 
 /Users/dormunis/.vscode/extensions/vadimcn.vscode-lldb-1.8.1/adapter/codelldb terminal-agent --connect=51916 
curl https://github.com/kubernetes/kubernetes/releases  /Users/dormunis/.vscode/extensions/vadimcn.vscode-lldb-1.8.1/adapter/codelldb terminal-agent --connect=51935 
 /Users/dormunis/.vscode/extensions/vadimcn.vscode-lldb-1.8.1/adapter/codelldb terminal-agent --connect=51938 
 /Users/dormunis/.vscode/extensions/vadimcn.vscode-lldb-1.8.1/adapter/codelldb terminal-agent --connect=51946 
wget -v https://dl.k8s.io/release/v1.26.2/bin/darwin/arm64/kubectl
 /Users/dormunis/.vscode/extensions/vadimcn.vscode-lldb-1.8.1/adapter/codelldb terminal-agent --connect=52041 
 /Users/dormunis/.vscode/extensions/vadimcn.vscode-lldb-1.8.1/adapter/codelldb terminal-agent --connect=52056 
 /Users/dormunis/.vscode/extensions/vadimcn.vscode-lldb-1.8.1/adapter/codelldb terminal-agent --connect=52065 
 /Users/dormunis/.vscode/extensions/vadimcn.vscode-lldb-1.8.1/adapter/codelldb terminal-agent --connect=52074 
 /Users/dormunis/.vscode/extensions/vadimcn.vscode-lldb-1.8.1/adapter/codelldb terminal-agent --connect=52081 
 /Users/dormunis/.vscode/extensions/vadimcn.vscode-lldb-1.8.1/adapter/codelldb terminal-agent --connect=52106 
curl -v https://dl.k8s.io/release/v1.26.2/bin/darwin/arm64/kubectl
export PROFILE=preprod
TF_LOG=debug terraform apply -var-file=profiles/preprod.tfvars
aws eks --region us-east-1 update-kubeconfig --name airflow-preprod-test --profile apono
helm ls -n airflow-main
kgp -n monitoirng
kdp prometheus-stack-alertmanager-0 -n monitoring
helm get values prometheus
helm ls --help
kgd prometheus-stack-server-86859d455d-vphrv
kgp prometheus-stack-server-86859d455d-vphrv
kdelp prometheus-stack-alertmanager-0
kdno ip-10-1-30-105.ec2.internal
kdp prometheus-stack-server-86859d455d-vphrv
kdp prometheus-stack-alertmanager-0
k top nodes
kds -n airflow-main airflow-service 
kgp datadog-agent-8sv9x
kgp prometheus-stack-server-86859d455d-d5cfh
kdd prometheus-stack-server
klf prometheus-stack-server-86859d455d-d5cfh
klf prometheus-stack-server-86859d455d-d5cfh prometheus-server-configmap-reload
helm ls
kdp prometheus-stack-server-86859d455d-d5cfh
AWS_PROFILE=apono
terraform import aws_db_instance.postgres[airflow-main] airflow-preprod-test-airflow-main-postgres\

terraform import aws_db_instance.postgres["airflow-main"] airflow-preprod-test-airflow-main-postgres\

kgp -A --al
kgp -A --all
helm install airflow -n airflow-main airflow --help
helm install airflow-main -n airflow-main airflow -f ../airflow.yaml
helm install airflow -n airflow-main airflow -f ../airflow.yaml
helm install airflow airflow -n airflow-main -f ../airflow.yaml
helm install airflow airflow/airflow -n airflow-main -f ../airflow.yaml
klf airflow-worker-0 init
klf airflow-worker-0 airflow-worker
kdp airflow-worker-0
klf airflow-worker-0
klf airflow-worker-0 dag-git-sync
klf airflow-worker-0 log-cleanup
klf airflow-worker-0 check-db
klf airflow-pgbouncer-5c7bbc68-kqxv5
helm install --upgrade airflow airflow/airflow -n airflow-main -f ../airflow.yaml
helm upgrade airflow airflow/airflow -n airflow-main -f ../airflow.yaml
klf airflow-pgbouncer-57778f4cb9-nn6wk
klf airflow-db-migrations-6fbdf4fbf8-4gcl7
helm uninstall airflow -n airflow-main
aws --profile apono configure set role_arn arn:aws:iam::277411487094:role/yizhar_apono-apono-oidc-role && aws --profile apono configure set web_identity_token_file ~/.aws/token && echo eyJhbGciOiAiUlMyNTYiLCAidHlwIjogIkpXVCJ9.eyJ1c2VyX25hbWUiOiAieWl6aGFyX2Fwb25vLWFwb25vLW9pZGMtcm9sZSIsICJpc3MiOiAiaHR0cHM6Ly9hcG9uby1vaWRjLnMzLmFtYXpvbmF3cy5jb20vYjU1Njk4MzAtYWU4NC0xMWVjLWFlOGUtMGViNmRiMjNkYTQ1IiwgImlhdCI6IDE2Nzg2ODgzMDksICJleHAiOiAxNjc4NzQ1OTA1LCAiYXVkIjogIkFXUyIsICJzdWIiOiAieWl6aGFyX2Fwb25vLWFwb25vLW9pZGMtcm9sZSJ9.OsxciO3pT0xe34ehJmxveRq6qFuOTTyyhg4fEgcFdqEFFsAbPetDJ0l-cyndB6BcGOKCxzZTdVqn8dJIZ6wG7_9Jx86xuUNpJm4FSaeCXdcDQrNV-QOq1Sy09DGe9sVSvP6j8ghvgrLhomLeANkZUym5VvbNA0MZjaycAipPVGp8UNPfZAjBo306l3R1KHILAvDMUaT79tLubODm7gSkF2spkmxTaOu231s30-t5cpTkn3Tabekca-q802xK535Eyb57lw32OAHjJgVgTgyKv-gTKpH-V4YJOl2xvy5b99m-cGBX285li_rsdKC4SF0X26ivP0UIfR7qq3j9C2Av9w > ~/.aws/token
brew install slack
cat svc.yaml
rm svc.yaml
cd dev/private
cd ../finout
git checkout refactor/clic
git checkout refactor/clickhouse
kubectx -d preprod
kubectx -d prod
kubectx -d clickhousetest
kubectx -d arn:aws:eks:us-east-1:277411487094:cluster/finout-staging-clickhouse
AWS_PROFILE=apono terraform destroy -var-file=profiles/staging.tfvars
AWS_PROFILE=terraform init -backend-config=profiles/staging.tfvars
AWS_PROFILE=apono terraform init -backend-config=profiles/staging.tfvars
aws --profile apono configure set role_arn arn:aws:iam::277411487094:role/yizhar_apono-apono-oidc-role && aws --profile apono configure set web_identity_token_file ~/.aws/token && echo eyJhbGciOiAiUlMyNTYiLCAidHlwIjogIkpXVCJ9.eyJ1c2VyX25hbWUiOiAieWl6aGFyX2Fwb25vLWFwb25vLW9pZGMtcm9sZSIsICJpc3MiOiAiaHR0cHM6Ly9hcG9uby1vaWRjLnMzLmFtYXpvbmF3cy5jb20vYjU1Njk4MzAtYWU4NC0xMWVjLWFlOGUtMGViNmRiMjNkYTQ1IiwgImlhdCI6IDE2Nzg2OTI0NTksICJleHAiOiAxNjc4NzUwMDU1LCAiYXVkIjogIkFXUyIsICJzdWIiOiAieWl6aGFyX2Fwb25vLWFwb25vLW9pZGMtcm9sZSJ9.Jxugj9WjJ_40H_opEFMV1gOLKgmS6rTzpaZO3J4j8hiGz1nJHAwgAkgf26JDii2PgkfF_IJ1ozVWIAxHWLI9jm-IQQh6FI1sGnRNEIG_fV4AzVmO8dwRG7HG8usJr_WyuOoyIuwxQmzy9HhM9AbENMAS2ULULITkDzhAH6Hpp2OWRIf3knpcW996Gf2Uax6INlMwJozWkgtlQhfTPh1AfcsMkqUtAw_s471b3E7s7Ag5msiGaqEssBF1ilYCcZXxo1mdb9EnbcmG4IwqX4QPUOdqm9Bn_uRgWsy3FWgi6oI8jgbSJ7jmHTSHfQ7nhQVvFnF-pNPQvVWOudl0CUj87Q > ~/.aws/token
AWS_PROFILE=apono terraform init -backend-config=profiles/backend-staging.tfvars
TF_LOG=debug AWS_PROFILE=apono terraform init -backend-config=profiles/backend-staging.tfvars
aws s3 ls s3://finout-terraform-state/staging/clickhouse/ --profile=apono
AWS_PROFILE=apono terraform plan -var-file=profiles/staging.tfvars
AWS_PROFILE=apono terraform plan -var-file=profiles/staging.tfvars -var-file=profiles/backend-staging.tfvars
TF_LOG=debug AWS_PROFILE=apono terraform init -backend-config=profiles/staging.tfvars
AWS_PROFILE=apono terraform init -backend-config=profiles/staging.tfvars --help
AWS_PROFILE=apono terraform --help
AWS_PROFILE=apono terraform init --help
AWS_PROFILE=apono terraform init -backend-config=profiles/staging.tfvars -input=false
AWS_PROFILE=apono terraform init -backend-config=profiles/backend-staging.tfvars 
AWS_PROFILE=apono terraform init -backend-config=profiles/staging.backend.tfvars 
AWS_PROFILE=apono terraform init -backend-config=profiles/staging.backend.tfvars -reconfigure
AWS_PROFILE=apono terraform init -backend-config=profiles/staging.s3.tfbackend -reconfigure
export PROFILE=staging
AWS_PROFILE=apono terraform apply -var-file=profiles/$PROFILE.tfvars -var-file=profiles/$PROFILE.s3.tfbackend
AWS_PROFILE=apono terraform apply -var-file=profiles/$PROFILE.tfvars -backend-config=profiles/$PROFILE.s3.tfbackend
AWS_PROFILE=apono terraform apply -var-file=profiles/$PROFILE.tfvars
terraform workspace list -backend-config=profiles/staging.s3.tfbackend
terraform workspace show default
terraform workspace new
terraform workspace --help
terraform workspace delete staging
git commit -m "dynamic backend; todo: move to workspaces"
terraform init -backend-config=profiles/staging.s3.tfbackend
terraform destroy -var-file profiles/staging.tfvars
git commit -m "changed readme"
git pull origin refactor/clickhouse --rebase
terraform init -backend-config=profiles/preprod.s3.tfbackend -var-file=profiles/preprod.tfvars
terraform apply -var-file=../../profiles/preprod.tfvars
ls profiles
helm repo add datadog https://helm.datadoghq.com\

helm repo add airflow https://airflow-helm.github.io/charts
terraform import aws_db_instance.postgres["airflow-main"] airflow-preprod-test-airflow-main-postgres
terraform import "aws_db_instance.postgres[airflow-main]" airflow-preprod-test-airflow-main-postgres
terraform import "aws_db_instance.postgres[\"airflow-main\"]" airflow-preprod-test-airflow-main-postgres -var-file=profiles/preprod.tfvars
terraform import "aws_db_instance.postgres[\"airflow-main\"]" airflow-preprod-test-airflow-main-postgres
terraform -var-file profiles/preprod.tfvars import "aws_db_instance.postgres[\"airflow-main\"]" airflow-preprod-test-airflow-main-postgres
terraform import -var-file profiles/preprod.tfvars "aws_db_instance.postgres[\"airflow-main\"]" airflow-preprod-test-airflow-main-postgres
terraform import -var-file profiles/preprod.tfvars "aws_db_instance.postgres[\"airflow-historical\"]" airflow-preprod-test-airflow-historical-postgres
terraform init -backend-config=profiles/preprod.s3.tfbackend
kdno ip-10-1-4-219.ec2.internal
kdno ip-10-1-0-19.ec2.internal
terraform import -var-file profiles/preprod.tfvars kubernetes_service.airflow["airflow-main"] airflow-main/airflow-service
terraform import -var-file profiles/preprod.tfvars "kubernetes_service.airflow[\"airflow-main\"]" airflow-main/airflow-service
helm ls -A --al
helm ls -A --all;
kgsec -A
kgns monitoring
kdp prometheus-stack-server-86859d455d-2nh4g
kdno ip-10-1-32-35.ec2.internal
kdno ip-10-1-59-228.ec2.internal
kdno ip-10-1-49-52.ec2.internal
kdno ip-10-1-11-246.ec2.internal
klf prometheus-stack-server-79cc6d48cb-8l75b
kdp prometheus-stack-server-79cc6d48cb-8l75b
kgp -A | grep cni
klf aws-load-balancer-controller-84f84555c-t4rm6 -n kube-system
ked airflow-service
kdp airflow-airflow-main-web-f9c46776d-76nkk -n airflow-main
klf aws-load-balancer-controller-84f84555c-52nnv
klf aws-load-balancer-controller-84f84555c-52nnv -n kube-system
kes airflow-service
kes -n airflow-historical airflow-service
kds -n airflow-historical airflow-service
kgs -A | grep LoadBalancer
aws --profile apono configure set role_arn arn:aws:iam::277411487094:role/yizhar_apono-apono-oidc-role && aws --profile apono configure set web_identity_token_file ~/.aws/token && echo eyJhbGciOiAiUlMyNTYiLCAidHlwIjogIkpXVCJ9.eyJ1c2VyX25hbWUiOiAieWl6aGFyX2Fwb25vLWFwb25vLW9pZGMtcm9sZSIsICJpc3MiOiAiaHR0cHM6Ly9hcG9uby1vaWRjLnMzLmFtYXpvbmF3cy5jb20vYjU1Njk4MzAtYWU4NC0xMWVjLWFlOGUtMGViNmRiMjNkYTQ1IiwgImlhdCI6IDE2Nzg4NjcwNDMsICJleHAiOiAxNjc4OTI0NjM4LCAiYXVkIjogIkFXUyIsICJzdWIiOiAieWl6aGFyX2Fwb25vLWFwb25vLW9pZGMtcm9sZSJ9.KGsGnQecSLE2xBuQYUoCiSY7cVFGniiM591_RJe1A4WTsy03YZjoST8dRa8erMQAw0bW8TUPiSZ9kmIJC9gwWsv_0slXfrdea9v5uYyqtnkvmvDMxDZIbKYHF9dRhQuoJCWE14h9wnOzKW19zL32xeCey8I9jGSOYi_4XJc1m2kTYvHSUzCBWHhj7C0X5Wbrwvb852iZm9VSVTBZcS2IopOSnlk88ig0094JpMjTGJAznxKmfa33RYbwyEquPCc_B-IuVsnCJqyu6eQZbNWrp6puskh5n6vpbS_1ywppLJD51mdFX1dR8pPkG6L-U9lbf0jWh2cJOF55YbNN1dbZSg > ~/.aws/token
aws eks --region us-east-1 update-kubeconfig --name preprod-airflow-test --role arn:aws:iam::277411487094:role/k8sAdmin --profile apono
kgc -n airflow-main
kes -n airflow-main airflow-service
kds airflow-hisotrical -n airflow-main
kds airflow-historical -n airflow-main
kdels airflow-service -n airflow-main
kdels airflow-service -n airflow-historical
kaf ../airflow.yaml
kds airflow-service -n airflow-historical
kds airflow-service -n airflow-main
kgs -n airflow-main
kgs -n airflow-historical
kgs -A | grep "internal-a1efbc518a8f2465293eed5c17798c65-1391063913.us-east-1.elb.amazonaws.com\
"
kgs -A | grep "internal-a1efbc518a8f2465293eed5c17798c65-1391063913.us-east-1.elb.amazonaws.com"
klf prometheus-stack-server-86859d455d-5tzm6
kdp prometheus-stack-server-86859d455d-5tzm6 -n monitoring
kdp prometheus-stack-server-86859d455d-5tzm6
kdp airflow-airflow-historical-scheduler-797c5c9fc7-z527d -n airflow-historical
git commit -m "finished airflow"
git push -u origin refactor/airflow
git push origin refactor/clickhouse --force
git pull origin master
git pull origin master --rebase
git pull origin main --rebase --reapply-cherry-picks
git pull origin refactor/airflow --rebase --reapply-cherry-picks
git pull origin refactor/airflow --reapply-cherry-picks
git pull --reapply-cherry-picks origin refactor/airflow
git commit -m "hosted zone fix"
git checkout -b bugfix/clickhouse-loadbalancer
git commit -m "bugfix for owned load balancers"
git commit -m "removed old airflow folders"
mv prometheus.yaml ../prometheus-airflow-prod.yaml
git commit -m "fixed finout exporter for prometheus"
git checkout bugfix/clickhouse-loadbalancer
git brach -M feature/clickhouse-ingress
git branch -M feature/clickhouse-ingress
aws eks --region us-east-1 update-kubeconfig --name finout-prod-clickhouse --role arn:aws:iam::277411487094:role/k8sAdmin --profile apono
kgi clickhouse-ingress -n clickhouse -o yaml > clickhouse-ingress.yaml
kdp prometheus-stack-server-68547d9cfc-qh28j
klf prometheus-stack-server-68547d9cfc-qh28j
klf prometheus-stack-server-68547d9cfc-qh28j 
klf prometheus-stack-server-68547d9cfc-qh28j prometheus-server-configmap-reload
kecm prometheus-stack-server
klf prometheus-stack-server-68547d9cfc-qh28j prometheus-server
helm get values prometheus-stack
helm uninstall -n monitoring prometheus-stack
helm repo --ls
helm repo ls
helm repo add prometheus https://prometheus-community.github.io/helm-charts
helm repo update
helm repo install prometheus-stack prometheus/prometheus -f ../prometheus.yaml -n monitoring
helm repo install --help
helm install prometheus-stack prometheus/prometheus -f ../prometheus.yaml -n monitoring
kdp prometheus-stack-server-77c675b697-7wdnz
kdp prometheus-stack-server-77c675b697-7wdnz -n monitoring
klf prometheus-stack-server-77c675b697-7wdnz
klf prometheus-stack-server-77c675b697-7wdnz -n monitoring
klf prometheus-stack-server-77c675b697-7wdnz -n monitoring prometheus-server
kdp prometheus-stack-server-68547d9cfc-p7r4z
k get cronjob -A
k describe cronjob finout-prometheus-exporter-job -n monitoring
kubectx arn:aws:eks:us-east-1:277411487094:cluster/finout-prod-clickhouse
kgs -o yaml clickhouse -n clickhouse
kgs -o yaml finout-clickhouse-service-net -n clickhouse
kubectx minikub
kubectx minikube
kdp airflow-sync-users-684f65f5b5-fv5sl -n airflow
kdp airflow-worker-3 -n airflow
cd zsh-completions
vim zsh-completions.plugin.zsh
cd src
ls | grep git
aws --profile apono configure set role_arn arn:aws:iam::277411487094:role/yizhar_apono-apono-oidc-role && aws --profile apono configure set web_identity_token_file ~/.aws/token && echo eyJhbGciOiAiUlMyNTYiLCAidHlwIjogIkpXVCJ9.eyJ1c2VyX25hbWUiOiAieWl6aGFyX2Fwb25vLWFwb25vLW9pZGMtcm9sZSIsICJpc3MiOiAiaHR0cHM6Ly9hcG9uby1vaWRjLnMzLmFtYXpvbmF3cy5jb20vYjU1Njk4MzAtYWU4NC0xMWVjLWFlOGUtMGViNmRiMjNkYTQ1IiwgImlhdCI6IDE2Nzg4NjcxMzksICJleHAiOiAxNjc4OTI0NzM0LCAiYXVkIjogIkFXUyIsICJzdWIiOiAieWl6aGFyX2Fwb25vLWFwb25vLW9pZGMtcm9sZSJ9.JUVDhfr9QpnbHpVhulkiVTkS2G5Aybohy1ZaeY7IcJORKOZuihZRiN6viQqNDrki5cvlNA4i2x8S1LDGEQjZ2Tw1MGB2ZWpdq9oQfHXzhAIAJeEBU5ngtcZFl8KMlPMoF20WT1sAKbi_JLNP_hzLdeKw-YCoDruJtoEgc3k4qYvSBuvTH8gOJ5stB1rIFSTlRakvopyGGysTC7wNsTHmGi2ZrewqO0q0OqkOw_AcUHC7qkb6kXYpa5dSBgwcHhKF6ZP4QHDZfchHY90FtKhN0PnbmxE28KUDUH2RheV79tvXASxtraRMXISG8m9KgKOkH5Kl-9-DQuYIJsm5RwCegw > ~/.aws/token
cd ls
rm airflow.yaml
git diff ../clickhouse/loadbalancer.tf
docker login
cat ~/.docker/config.json\

rm ~/.docker/config.json
terraform init -backend-config=profiles/preprod.s3.tfbackend -upgrade
aws eks describe-cluster --name airflow-preprod-test --query "cluster.identity.oidc.issuer" --output text
aws eks describe-cluster --name airflow-preprod-test --query "cluster.identity.oidc.issuer" --output text --region us-east-1
aws iam list-open-id-connect-providers | grep //oidc.eks.us-east-1.amazonaws.com/id/DEDB32687CBE155BE9BBC714FDC530C0
aws iam list-open-id-connect-providers | grep DEDB32687CBE155BE9BBC714FDC530C0
eksctl utils associate-iam-oidc-provider --cluster airflow-preprod-test --approve
klf prometheus-stack-server-68547d9cfc-p7r4z prometheus-server
klf prometheus-stack-server-68547d9cfc-p7r4z
kdelp prometheus-stack-server-68547d9cfc-p7r4z
klf prometheus-stack-server-68547d9cfc-hk5j8 prometheus-server
eksctl utils associate-iam-oidc-provider --cluster airflow-preprod-test --approve --region us-east-1 --help
kubect
kgp -A | grep -iv Running
kdp airflow-db-migrations-5fc7bf85f6-mg9g8
klf airflow-db-migrations-5fc7bf85f6-mg9g8
klf airflow-db-migrations-5fc7bf85f6-mg9g8 check-db
kdp airflow-pgbouncer-557869df97-s44gj
kdelp airflow-pgbouncer-557869df97-s44gj
ked airflow-pgbouncer
kdp airflow-pgbouncer-7d67bb9854-bfckl
kdp airflow-pgbouncer-557869df97-zz2m6
kgsec -o jsn airflow-admin-credentials
kgsec -o json airflow-admin-credentials
kgsec -o json airflow-admin-credentials | jq -r '.data.username' | base64 -D
kgsec -o json airflow-admin-credentials | jq -r '.data.password' | base64 -D
kgsec -o json airflow-admin-credentials | jq -r '.data.password' | base64 -D | pbcopy
lgns
kdp dag-run-generator.e474fdfcd40b4a75802a15e00e97011c
kgp dag-run-generator.e474fdfcd40b4a75802a15e00e97011c -o yaml
kdp airflow-task-clearer-27981605-wrdzk
git ccheckout feature/clickhouse-ingress
terraform state show
terraform state 
terraform state rm data.kubernetes_namespace.kube-system
mv .terraform .bak.terrraform
mv .bak.terrraform .bak.terraform
terraform destroy -var-file=profiles/staging.tfvars
terraform import aws_iam_policy.load-balancer-policy AWSLoadBalancerControllerIAMPolicy-staging-clickhouse-test -var-file=profiles/staging.tfvars
terraform import -var-file=profiles/staging.tfvars aws_iam_policy.load-balancer-policy AWSLoadBalancerControllerIAMPolicy-staging-clickhouse-test
terraform import -var-file=profiles/staging.tfvars aws_iam_policy.load-balancer-policy arn:aws:iam::277411487094:policy/AWSLoadBalancerControllerIAMPolicy-staging-clickhouse-test\

terraform outputs
terraform -help
kdp airflow-pgbouncer-557869df97-zfkhb | grep -i image
docker pull ghcr.io/airflow-helm/pgbouncer:1.17.0-patch.0
git commit -m "middle of work"
kgs -A
kgs -A | grep airflow
kubens airflow-main
kpf airflow-airflow-main-web 8080:8080
kpf svc/airflow-airflow-main-web 8080:8080 -n airflow-historical
kubectl get secret -o json airflow-admin-credentials | jq -r '.data.password' | base64 -D | pbcopy
kpf svc/airflow-airflow-main-web 8080:8080
kpf svc/airflow-airflow-main-web -n airflow-main 8080:8080
kds airflow-service
helm ls -n airflow-historical
helm ls -n airflow-historical --all
kdp wiz-costcenter-snow-fetch-1678896657-exec-3
kpf airflow-airflow-historical-web 8080:8080
kpf svc.airflow-airflow-historical-web 8080:8080
kpf svc/airflow-airflow-historical-web 8080:8080
kdelp wiz-costcenter-snow-fetch-1678896612-exec-1
kdelp wiz-costcenter-snow-fetch-1678896612-exec-2
kdelp wiz-costcenter-snow-fetch-1678896612-exec-3
kdelp wiz-costcenter-snow-fetch-1678896612-exec-4
kdelp wiz-costcenter-snow-fetch-1678896612-exec-5
kdelp wiz-costcenter-snow-fetch-1678896612-exec-6
git idff
eksctl utils associate-iam-oidc-provider --cluster airflow-preprod-test --approve --region us-east-1
crsutil status
brew install koekeishiya/formulae/yabai\

brew services start yabai
mkdir -r ~/.config/yabai
mkdir ~/.config/yabai
touch ~/.config/yabai/yabairc
nvim ~/.config/yabai/yabairc
vim ~/.config/yabai
csrutil enable
rm -rf /private/etc/sudoers.d/yabai
sudo rm -rf /private/etc/sudoers.d/yabai
  brew install koekeishiya/formulae/skhd\

brew --help
brew doctor
brew upgrade
brew config\

brew doctor\

curl -LI https://formulae.brew.sh/api/formula.json\

brew update
brew service restart yabai
mkdir ~/.config/skhd
cd ../skhd
yabai -m window --toggle exit-fullscreen
nvim skhdrc
whoami
shasum -a 256 $(which yabai)
yabai -m mouse
yabai -m query --spaces | jq '.[] | select(."has-focus" == true and .index == 1)' >/dev/null || (yabai -m space --create && yabai -m space move --next); yabai -m window --space prev && yabai -m space --focus prev
yabai -m query --spaces | jq '.[] | select(."has-focus" == true and .index == 1)' 
yabai -m query --spaces | jq '.[] | select(."has-focus" == true).index' 
yabai -m query --spaces | jq '.[] | select(."has-focus" == true).index' == 1
yabai -m query --spaces | jq '.[] | select(."has-focus" == true).index' == "1"
is_first=$(yabai -m query --spaces | jq '.[] | select(."has-focus" == true).index' == "1")
is_first=$(yabai -m query --spaces | jq '.[] | select(."has-focus" == true).index' = "1")
is_first=$(yabai -m query --spaces | jq '.[] | select(."has-focus" == true).index' -eq "1")
YABAI_SPACE_ID, $YABAI_RECENT_SPACE_ID
chmod +x ../yabai/clear-empty-space.zsh
ls -a ~
cat ~/.yabanirecentspaceid
cat ~/.yabanispaceid
yabai -m query --spaces | jq 'select(.has_focus == true)'
yabai -m query --spaces | jq 'select(.has-focus == true)'
yabai -m query --spaces | jq 'select(.[]"has-focus" == true)'
yabai -m query --spaces | jq 'select([]."has-focus" == true)'
yabai -m query --spaces | jq 'select(."has-focus" == true)'
yabai -m query --spaces | jq '.[] | select(."has-focus" == true).index'
vim ~/.config/yabai/clear-empty-space.zsh
rm ~/.yabanirecentspaceid ~/.yabanispaceid
cat ~/currentspaceid
yabai -m query --spaces | jq '.windows | length == 0'
yabai -m query --spaces | jq '. | select(.windows | length == 0)'
yabai -m query --spaces | jq '. | select(.windows) | length == 0)'
yabai -m query --spaces | jq '.[] | select(.windows | length == 0 and .index != 1).index
yabai -m query --spaces | jq '.[] | select(.windows | length == 0 and .index != 1).index'
yabai -m query --spaces | jq '.[] | select(.windows | length == 0).index'
yabai -m query --spaces | jq '.[] | select(.windows | length == 0)'
yabai -m query --spaces | jq '.[] | select(.windows | length == 0 and .index != 1)'
yabai -m query --spaces | jq '.[] | select(has("windows") and .windows | length == 0 and .index != 1).index'\

yabai -m query --spaces | jq '.[] | select(has("windows") and .windows | length == 0 and has("index") and .index != 1).index'
yabai -m query --spaces | jq '.[] | select(has("windows") and (.windows | length == 0) and (.index != 1)).index'\

yabai -m query --spaces | jq '.[] | select(has("windows") and (.windows | length == 0) and (.index != 1)).index'
yabai -m query --spaces | jq '.[] | select(has("windows") and (.windows | length == 0) and (.index != $CURRENT_SP     ACE_ID)).index' --arg CURRENT_SPACE_ID "1" | tac
brew install coreutils
yabai -m query --spaces | jq '.[] | select(has("windows") and (.windows | length == 0) and (.index != $CURRENT_SPACE_ID)).index' --arg CURRENT_SPACE_ID "1" | tac
rm ~/currentspaceid
cat ~/spaces
yabai -m query displays
yabai -m query display
yabai -m winodw --stack north
yabai -m window --stack north
yabai -m window --stack west
yabai -m window --stack east
sudo nvram boot-args=-arm64e_preview_abi
rm ~/Downloads/batman.webp
mv ~/Downloads/batman.jpeg ~/Pictures/
yabai -m space --focus next || yabai -m space --create
csrutil status
sudo visudo -f /private/etc/sudoers.d/yabai\

yabai -m space --foucs nex
yabai -m space --foucs next
yabai -m space --foucs 2
yabai -m space --foucs 1
yabai -m space --move previous
yabai -m space --move prev --help
yabai --help
yabai -m space --focus prev
yabai -m query space
yabai -m query --space
yabai -m query --spaces 2
yabai -m query --spaces 1
yabai -m query --space 1
yabai -m query --spaces --space 2 | jq '.windows'
yabai -m query --spaces --space 2 | jq 'select(.windows length == 0)'
yabai -m query --spaces --space 2 | jq '.windows | length == 0'
yabai -m query --spaces --space 2 | jq '.windows | length == 0' | grep -q 'true'
brew uninstall rectangle
tail $HOMEBREW_PREFIX/var/log/yabai/yabai.err.log
man ya
brew install --cask karabiner-elements
cd ../yabai
code yabairc
yabai -m query --windows | jq -r '.'
yabai -m query --windows | jq -r 'select(. | length > 3)'
yabai -m query --windows | jq -r 'select(. | length > 0)'
vim data.json
cat data.json
cat data.json | jq
cat data.json | jq 'length == 0 | if . then "lol" else empty end'
cat data.json | jq 'length != 0 | if . then "lol" else empty end'
yabai -m query --spaces | select .index == 1
yabai -m query --spaces | jq 'select .index == 1'
yabai -m query --spaces | jq 'select(.index == 1)'
yabai -m query --spaces | jq '.[] | select(.index == 1)'
yabai -m query --spaces | jq '.[] | select(.index == 2)'
yabai -m query --spaces | jq '.[] | select(.has-focus == true)'
yabai -m query --spaces | jq '.[] | select(."has-focus" == true)'
yabai -m query --spaces | jq '.[] | select(."has-focus" == true && .index > 0)'
yabai -m query --spaces | jq '.[] | select(."has-focus" == true & .index > 0)'
yabai -m query --spaces | jq '.[] | select(."has-focus" == true and .index > 3)'
yabai -m query --spaces | jq '.[] | select(."has-focus" == true and .index > 0)'
which yabai
ls $(which yabai)
ls $(which yabai)/
ls $(which yabai)/var
ls -lh $(which yabai)
cd ../Cellar/yabai/5.0.2/bin/yabai
rm data.json
tail -f /usr/local/var/log/yabai/yabai.err.log\

echo $HOMEBREW_PREFIX
echo $HOMEBREW_PREFIX/var/log/yabai/yabai.err.log
tail -f $HOMEBREW_PREFIX/var/log/yabai/yabai.out.log
tail -f $HOMEBREW_PREFIX/var/log/yabai/yabai.err.log
vim ../yabai/yabairc
 yabai -m window --grid 3:2:3:1:1:3
 yabai -m window --grid 3:2:3:1w:1:1
 yabai -m window --grid 3:2:3:1:1:1
 yabai -m window --grid 2:3:3:1:1:2
 yabai -m window --grid 2:3:3:1:1:5
 yabai -m window --grid 2:3:3:1:2:1
 yabai -m window --grid 2:3:1:3:1:1
 yabai -m window --grid 2:3:1:1:1:1
yabai -m window --grid 2:2:3:1:1:3
yabai -m window --grid 2:3:3:1:1:3
yabai -m window --grid 2:3:3:1:0:2
yabai -m window --grid 2:3:3:1
yabai -m window --grid 2:3:3:1:10
yabai -m window --grid 2:3:3:1:10:10
yabai -m window --grid 2:3:3:1:1:1
yabai -m window --grid 1:3:1:0:1:1
yabai -m window --grid 1:3:0:1:2:1
yabai -m window --grid 1:3:1:0:2:1
yabai -m window --grid 1:3:2:0:2:1
yabai -m window --grid 1:2:0:1:2:1
yabai -m window --grid 1:2:0:0:2:1
yabai -m window --grid 1:2:0:0:1:0
yabai -m window --grid 1:2:0:0:0:1
yabai -m window --grid 1:1:0:0:1:1
yabai -m window --grid 1:0:0:0:1:1
yabai -m window --grid 1:2:0:0:1:1
yabai -m window --grid 2:2:0:0:1:1
yabai -m window --grid 2:2:1:0:1:1
yabai -m window --grid 2:2:0:1:1:1
yabai -m window --grid 2:2:1:1:1:1
yabai -m window --grid 2:3:0:0:0:1
yabai -m window --grid 2:3:1:0:0:0
yabai -m window --grid 2:3:2:0:0:0
yabai -m window --grid 2:3:0:1:0:0
yabai -m window --grid 2:3:1:1:0:0
yabai -m window --grid 2:3:0:0:0:0
exit
man skhd
yabai -m window --grid 2:3:2:1:0:0
yabai -m window --grid 1:2:0:0:0:0
yabai -m window --grid 1:2:1:0:0:0
yabai -m window --grid 2:1:0:0:0:0
yabai -m window --grid 2:1:0:1:0:0
rm spaces
cd ~/.config/yabai
pbpaste > threecolumns.zsh
chmod +x threecolumns.zsh
yabai -m query --spaces space 1
yabai -m query --spaces 
yabai -m query --spaces --space | jq '.index'
yabai -m window --toggle zoom-parent
vim ~/.config/yabai/yabairc
yabai -m window --toggle layout
yabai -m window --layout float
yabai -m space --move prev
vim move_to_previous_space.zsh
chmod +x move_to_previous_space.zsh
cp move_to_previous_space.zsh move_to_next_space.zsh
yabai -m space --move next
vim move_to_next_space.zsh
yabai -m window --space south
cp move_to_next_space.zsh move_window_to_space.zsh
cat move_to_previous_space.zsh| pbcopy
rm move_to_next_space.zsh
rm move_to_previous_space.zsh
./move_window_to_space.zsh prev
./move_window_to_space.zsh 2
./move_window_to_space.zsh next
vim threecolumns.zsh
mv threecolumns.zsh rebalance-columns.zsh
yabai -m window --move left
vim yabai/yabairc
vim skhd/skhdrc
vim skhd
cp move_window_to_space.zsh move_window_in_grid.zsh
pbpaste > move_window_in_grid.zsh
vim move_window_in_grid.zsh
vim ~/.zsh
vim move_window_to_space.zsh
cat skhdrc
skhd
skhd --help
yabai -m window --resize right:20:20
yabai -m window --resize left:20:20
yabai -m window --resize left:-20:20
yabai -m window --resize left:-20:-20
yabai -m window --resize right:-20:-20
yabai -m window --resize top:-20:-20 ; \\
    yabai -m window --resize bottom:-20:-20 ; \\
    yabai -m window --resize right:-20:-20 ; \\
    yabai -m window --resize left:-20:-20
yabai -m query --displays
vim warp_windows.zsh
touch warp_windows.zsh
code warp_windows.zsh
chmod +x warp_windows.zsh
touch resize_window.zsh
chmod +x resize_window.zsh
yabai -m query --windows --window | jq '.split_type'
yabai -m query --windows --window | jq '."split-type"'
yabai -m window --resize top:-20:0
yabai -m window --resize top:-20:-20
yabai -m window --resize bottom:-20:-20
yabai -m window --resize bottom:20:20
yabai -m window --resize bottom:20:0
yabai -m window --resize bottom:0:20
yabai -m window --resize bottom:0:-20
yabai -m window --resize right:0:-20
yabai -m window --resize right:0:20
yabai -m window --resize left:0:20
yabai -m window --resize left:20:0
yabai -m window --resize left:10:0
yabai -m window --resize left:0:10
yabai -m window --resize right:0:10
./resize_window.zsh 10
./resize_window.zsh -10 2>/dev/null
./resize_window.zsh -10
brew services restart skhd
yabai -m space --toggle focus
yabai -m window --resize right:10:10
yabai -m window --resize bottom:10:10
cat resize_window
code move_window_to_space.zsh
rm warp_windows.zsh
yabai -m window --split vertically
yabai -m query --spaces space
yabai -m query --spaces --space | jq -r '.windows | length
yabai -m query --spaces --space | jq -r '.windows | length'
cat ~/windows
yabai -m query --spaces --space 
rm ~/resize_window
rm ~/windows
cat ~/current_space.txt
rm ~/current_space.txt
code ../yabai/yabairc
yabai -m query --windows -window
yabai -m window --move prev:800:0
yabai -m window --move prev
yabai -m space --toggle show-desktop
cat move_window_in_grid.zsh
code skhdrc
launchctl unload -F /System/Library/LaunchAgents/com.apple.WindowManager.plist > /dev/null 2>&1 &\

cp skhdrc bak.skhdrc
yabai -m window grid 4:4:1:1:2:2
yabai -m window --grid 4:4:1:1:2:2
yabai -m window --resize top:20:20
yabai -m window --resize top:20:0
yabai -m window --resize top:0:20
yabai -m window --resize top:0:-20
yabai -m window --resize bottom:0:10
yabai -m window --resize right:-10:10
yabai -m window --resize right:10:0
yabai -m window --resize top:0:-10; \\
                            yabai -m window --resize bottom:0:10; \\
                            yabai -m window --resize right:10:0; \\
                            yabai -m window --resize left:-10:0
yabai -m window --resize abs:10:10
yabai -m window --resize bottom_left:10:10
yabai -m window --resize bottom_right:10:10
yabai -m window --resize top_left:10:10
yabai -m window --resize top_left:-10:-10
yabai -m window --move rel:10:10
yabai -m window --move rel:10:0
yabai -m window --move abs:500:500
yabai -m query --windows --window | jq -r '.frame.w'
yabai -m query --displays --display | jq -r '.frame.w'
yabai -m query --displays --display | jq -r '.frame.h'
touch center_window.zsh
code center_window.zsh
chmod +x center_window.zsh
cat ~/a
ls ~
rm ~/avgs
rm test
yabai -m window --move abs:840 525
yabai -m window --move abs:840:525
yabai -m window --move abs:420:262
X_WIN_POS=600
X_DIS_POS=1200
echo $((($X_WIN_POS + $X_DIS_POS) / 2))
echo $((($X_DIS_POS) / 2)) - ((($X_WIN_POS) / 2))
echo $((($X_DIS_POS) / 2)) - ($X_WIN_POS) / 2)
echo $((($X_DIS_POS) / 2)) - ($X_WIN_POS) / 2
echo $((($X_DIS_POS) / 2) - ($X_WIN_POS) / 2)
echo $((($X_DIS_POS) / 2)) - (($X_WIN_POS) / 2))
echo $((($X_DIS_POS) / 2)) - (($X_WIN_POS) / 2)))
echo $((($X_DIS_POS) / 2)) - (($X_WIN_POS) / 2)
echo (($X_DIS_POS) / 2) - (($X_WIN_POS) / 2)
echo ((($X_DIS_POS) / 2) - (($X_WIN_POS) / 2))
echo (((($X_DIS_POS) / 2)) - ((($X_WIN_POS) / 2))0
echo (((($X_DIS_POS) / 2)) - ((($X_WIN_POS) / 2)))
$((($X_DIS_POS / 2) - ($X_WIN_POS / 2)))
echo $((($X_DIS_POS / 2) - ($X_WIN_POS / 2)))
./center_window.zsh
yabai -m window  west --stack $(yabai -m query --windows --window\
./center_window.zsh
yabai -m config layout stack
yabai -m window  west --stack
yabai -m window  west --stack $(yabai -m query --windows --window)
yabai -m window  west --stack 
yabai -m window east --stack 
yabai -m window east --stack $(yabai -m query --windows --window)
yabai -m window east --stack 2785
yabai -m window west --stack 2785
yabai -m query --windows --window "${wid}" | jq -re '.split == "horizontal"' \\
    && yabai -m window "${wid}" --toggle split
yabai -m query --windows --window "${wid}" | jq -re '.split == "horizontal"'
yabai -m query --windows --window "${wid}" 
yabai -m query --windows --window "${wid}" | jq -re '."split-type" == "horizontal"'
yabai -m window --insert "$(jq -nr \\
	--arg window_placement "$(yabai -m config window_placement)" \\
	--argjson display "$(yabai -m query --displays --display)" \\
	'[["west", "east"], ["north", "south"]][if $display | .frame | .w >= .h then 0 else 1 end][if $window_placement == "first_child" then 0 else 1 end]')"
jq -nr \\
	--arg window_placement "$(yabai -m config window_placement)" \\
	--argjson display "$(yabai -m query --displays --display)" \\
	'[["west", "east"], ["north", "south"]][if $display | .frame | .w >= .h then 0 else 1 end][if $window_placement == "first_child" then 0 else 1 end]'
yabai -m config window_placement
yabai -m query --displays --display | jq display
code resize_window.zsh
yabai -m window west --stack $(yabai -m query --windows --window | jq -r '.id')
yabai -m window east --stack $(yabai -m query --windows --window | jq -r '.id')
yabai -m window --insert north
yabai -m window --insert stack
../yabai/rebalance-columns.zsh
code move_window_in_grid.zsh
yabai -m query --windows --window | jq -e '.floating == 1'
yabai -m query --windows --window | jq '.floating'
yabai -m query --windows --window | jq -e '."is-floating" == true'
yabai -m query --windows --window | jq -e '."is-floating" == false'
touch ../yabai/threecolumns.zsh
chmod +x ../yabai/threecolumns.zsh
open ../yabai/threecolumns.zsh
code ../yabai/threecolumns.zsh
rm window
cat columns2
rm columns*
code bak.skhdrc
yabai -m query --windows | jq -r '[.[] | select(.["split-type"] == "vertical")] | length
yabai -m query --windows | jq -r '[.[] | select(.["split-type"] == "vertical")] | length'
cd ~/.config/skhd
rm rebalance-columns.zsh
code clear-empty-space.zsh
rm current_space_id
code ~/.config/skhd/move_window_to_space.zsh
yabai -m query --spaces --space 630
yabai -m space --destroy 2
yabai -m query --spaces --space 2
yabai -m query --spaces --space 1
rm columns
yabai -m query --spaces
rm space
ls current_space_id
YABAI_RECENT_SPACE_ID=630
yabai -m query --spaces | jq --arg space "$YABAI_RECENT_SPACE_ID" '.[] | select(.id == $space).index'
yabai -m query --spaces | jq '.[] | select(.id == $YABAI_RECENT_SPACE_ID).index'
yabai -m query --spaces | jq '.[] | select(.id == $space).index' --arg space "$YABAI_RECENT_SPACE_ID"
yabai -m query --spaces | jq '.[] | select(.id == 630).index'
yabai -m query --spaces | jq --arg v "$YABAI_RECENT_SPACE_ID" '.[] | select(.id == $v).index'
TEST=630 yabai -m query --spaces | jq --arg v "$TEST" '.[] | select(.id == $v).index'
export TEST=630
yabai -m query --spaces | jq --arg v "$TEST" '.[] | select(.id == $v).index'
yabai -m query --spaces | jq --arg v $TEST '.[] | select(.id == $v).index'
yabai -m query --spaces | jq --arg v $TEST '.[] | select(.id == $[v|tonumber]).index'
yabai -m query --spaces | jq --arg v $TEST '.[] | select(.id == [$v|tonumber]).index'
yabai -m query --spaces | jq --arg v $TEST '.[] | select(.id == $v|tonumber).index'
yabai -m query --spaces | jq --arg v "$TEST" '.[] | select(.id == $v|tonumber).index'
yabai -m query --spaces | jq --argjson v "$TEST" '.[] | select(.id == $v|tonumber).index'
yabai -m query --spaces | jq --argjson v "$TEST" '.[] | select(.id == $v).index'
cat current_space_id
cd .config/
code
cat window
cat columns
cat space
rm columns window space
cat split.txt
cat windows.txt
rm split.txt
WINDOWS=$(yabai -m query --spaces --space | jq -re '.windows | length')
echo $WINDOWS
rm test*
rm windows.txt
yabai -m window --warp south
yabai -m window --warp south || yabai -m window --toggle split
yabai -m window --warp south || yabai -m window --toggle split && yabai -m window --warp south
git commit -m "middle of work2"
GIT PULL
klf prometheus-stack-server-75d86776f6-tklcq
klf prometheus-stack-server-75d86776f6-tklcq prometheus-server-configmap-reload
klf prometheus-stack-server-75d86776f6-tklcq prometheus-server
klf prometheus-stack-prometheus-node-exporter-pvw7c
keti prometheus-stack-server-75d86776f6-tklcq bash
keti prometheus-stack-alertmanager bash
keti prometheus-stack-alertmanager sh
keti prometheus-stack-alertmanager-0 bash
keti prometheus-stack-kube-state-metrics-585858696f-dzvvm bash
keti prometheus-stack-kube-state-metrics-585858696f-dzvvm sh
keti prometheus-stack-server-75d86776f6-tklcq sh
keti finout-prometheus-exporter-job-27986880-7tk8w
keti finout-prometheus-exporter-job-27986880-7tk8w bash
keti prometheus-stack-alertmanager-0 sh
pbpaste > lol.yaml
kdelp busybox -n default
aws --profile apono configure set role_arn arn:aws:iam::277411487094:role/yizhar_apono-apono-oidc-role && aws --profile apono configure set web_identity_token_file ~/.aws/token && echo eyJhbGciOiAiUlMyNTYiLCAidHlwIjogIkpXVCJ9.eyJ1c2VyX25hbWUiOiAieWl6aGFyX2Fwb25vLWFwb25vLW9pZGMtcm9sZSIsICJpc3MiOiAiaHR0cHM6Ly9hcG9uby1vaWRjLnMzLmFtYXpvbmF3cy5jb20vYjU1Njk4MzAtYWU4NC0xMWVjLWFlOGUtMGViNmRiMjNkYTQ1IiwgImlhdCI6IDE2NzkyMTA5ODYsICJleHAiOiAxNjc5MjY4NTgyLCAiYXVkIjogIkFXUyIsICJzdWIiOiAieWl6aGFyX2Fwb25vLWFwb25vLW9pZGMtcm9sZSJ9.PmAp2VUHXEpbD7mcYEBPOJ6Z7CqR3Fr0ab32QfSNozYVUYMBwr7wD9_Yt-IZRj9QIauPISOlSsdVtDh3yAwhnY3hOZKhmnbXQ7ntaWFSSPOMSe4159JdxovPjRjRfzgkoJKy1ms20jeGT5GU_RWbT1KTuZ7SfmUMbDSSqWELcPTZM8B4KxX1xBNut-kxNwqNjXBTVjt8F7M7kRe9JzD123KZJRxybDEzOaKhSkgGbQRxLJLDJdVEUkaXnmDAuLoqguSOWwUeiWvidc0aMb89pnxQPf7TYpaJ00_2W2vDwFm9PriDMBdeqMbl_FsClyIm6Z65kBFmTZ6-0CmmtB5M1w > ~/.aws/token\

kgd -n monitoring
kubectx -d arn:aws:eks:us-east-1:277411487094:cluster/finout-prod-airflow-spark
kubectx -d arn:aws:eks:us-east-1:277411487094:cluster/finout-prod-clickhouse
ked opencost
aws eks --region us-east-1 update-kubeconfig --name finout-preprod-airflow --role arn:aws:iam::277411487094:role/k8sAdmin --profile apono
rm airflow/lol.yaml
git commit -m "added opencost"
aws eks --region us-east-1 update-kubeconfig --name finout-prod-clickhouse03 --role arn:aws:iam::277411487094:role/k8sAdmin --profile apono
ked busybox
kep bu
kep busybox
kdp busybox
kdno ip-10-1-10-35.ec2.internal/10.1.10.35
kdno ip-10-1-10-35.ec2.internal
keti busybox bash
kdelp busybox
git pull origin feature/clickhouse-ingress
eksctl utils associate-iam-oidc-provider --cluster staging-clickhouse-test --approve --region us-east-1
kgi -o yaml clickhouse-ingress -n clickhouse
kei clickhouse-ingress -n clickhouse
kubens clickhouse
k get all
git checkout .
kdp airflow-web-58fcb8ccf6-74274 -n airflow
kgi -n clickhouise
kgi -n airflow finout-airflow-ingress -o yaml > pbcopy
kgi -n airflow finout-airflow-ingress -o yaml
git commit -m "created ingress instead of service"
git commit -m "moved some files around and changed finout exporter to use variables again"
kgi -n airflow-main airflow -o yaml
git commit -m "fixed airflow"
git pull origin refactor/airflow
git pull origin refactor/airflow --rebase
git rebase --continue
kei clickhouse -n clickhouse
kei clickhouse
gity diff
git commit -m "removed eks oidc provider"
git commit -m "minor"
k get sa -n airflow
k get sa -n default
k get sa -A
k describe sa airflow-airflow-main -n airflow-main
k get sa airflow-airflow-main -n airflow-main -o yaml
kgcrb
k get crb
cat lol
cat lol.
git commit -m "changed data bucket"
clickhouse-ingress
kei clickhouse-ingress
kdi clickhouse
kgd -n kube-systewm
kgd -n kube-system
kdd aws-load-balancer-controller -n kube-system | grep -i "Service account"
k describe sa aws-load-balancer-controller -n kube-system
aws eks describe-cluster --name my-cluster --query "cluster.identity.oidc.issuer" --output text | cut -d '/' -f 5
aws eks describe-cluster --name staging-clickhouse-test --region us-east-1 --query "cluster.identity.oidc.issuer" --output text | cut -d '/' -f 5
kgi -n airflow
kgi -o yaml finout-airflow-ingress -n airflow
kgs -n airflow
kgs -n airflow airflow-custom-service -o yaml
keti airflow-worker-4 -u root bash
kubectl exec airflow-worker-4 -u root bash
k exec --help
keti airflow-worker-2 bash
kdp airflow-worker-2
keti airflow-worker-4
keti airflow-worker-4 bash
k get endpoints
kubectl run mycurlpod --image=curlimages/curl -i --tty -- sh\

kubectl exec -i --tty mycurlpod -- curl http://airflow-web.airflow.svc.cluster.local
kubectl exec -i --tty mycurlpod -- sh
kdelp mycurlpod
kubectx arn:aws:eks:us-east-1:277411487094:cluster/finout-preprod-airflow
kgcj -n monitoring
kecj finout-prometheus-exporter-job -n monitoring
k trigger cj finout-prometheus-exporter-job -n monitoring
k create job --from cronjob/finout-prometheus-exporter-job -n monitoringfinout-prometheus-exporter-job-boris
k create job --from cronjob/finout-prometheus-exporter-job -n monitoring
k create job -n monitoring --from cronjob/finout-prometheus-exporter-job boris01
kdi clickhouse-ingress -n clickhouse
kgi clickhouse-ingress -n clickhouse
vim lol.yaml
kaf lol.yaml
kdi clickhouse-ingress
terraform console -var-file=profiles/staging.tfvars
kgp -A | grep autoscaler
klf cluster-autoscaler-aws-cluster-autoscaler-5bdbdf7bd6-kjcdl -n cluster-autoscaler
kdp forter-costcenter-snow-virtual-1679232039-exec-50 -n airflow-historical
kgnp
klf cluster-autoscaler-aws-cluster-autoscaler-5bdbdf7bd6-kjcdl -n cluster-autoscaler -f --tail=50
klf cluster-autoscaler-aws-cluster-autoscaler-5bdbdf7bd6-kjcdl -n cluster-autoscaler -f --tail=50 | grep -i -e 'fail' -e 'error'
klf cluster-autoscaler-aws-cluster-autoscaler-5bdbdf7bd6-kjcdl -n cluster-autoscaler -f --tail=50 | grep -i -e 'fail'
klf cluster-autoscaler-aws-cluster-autoscaler-5bdbdf7bd6-kjcdl -n cluster-autoscaler -f --tail=50 | grep -v 'precicate checking error'
klf cluster-autoscaler-aws-cluster-autoscaler-5bdbdf7bd6-kjcdl -n cluster-autoscaler -f --tail=50 | grep -i 'scale up'
klf cluster-autoscaler-aws-cluster-autoscaler-5bdbdf7bd6-kjcdl -n cluster-autoscaler -f | grep -i 'scale up'
klf cluster-autoscaler-aws-cluster-autoscaler-5bdbdf7bd6-kjcdl -n cluster-autoscaler -f --tail=50 | grep -v 'predicate checking error'
klf cluster-autoscaler-aws-cluster-autoscaler-5bdbdf7bd6-kjcdl -n cluster-autoscaler -f | grep -v 'spark
klf cluster-autoscaler-aws-cluster-autoscaler-5bdbdf7bd6-kjcdl -n cluster-autoscaler -f | grep -v 'spark'
klf cluster-autoscaler-aws-cluster-autoscaler-5bdbdf7bd6-kjcdl -n cluster-autoscaler -f | grep 'spark'
klf cluster-autoscaler-aws-cluster-autoscaler-5bdbdf7bd6-kjcdl -n cluster-autoscaler -f | grep 'spark' | grep -v 'predicate'
git commit -m "modified autoscaler"
git stash
kdp forter-costcenter-cust-virtual-1679232040-exec-49
kdp forter-costcenter-snow-virtual-1679232039-exec-47
kdp
kdno ip-10-1-29-61.ec2.internal
kdp forter-costcenter-snow-virtual-1679233666-exec-67
klf cluster-autoscaler-aws-cluster-autoscaler-5bdbdf7bd6-kjcdl -n cluster-autoscaler -f | grep scaleup
klf cluster-autoscaler-aws-cluster-autoscaler-5bdbdf7bd6-kjcdl -n cluster-autoscaler -f | grep up
kngo
chmod +x clickhouse/thumbprint.sh
./thumbprint.sh us-east-1
kgp | grep forter | awk "${print $1}"
kdp forter-costcenter-amaz-virtual-1679241287-exec-16
kdp airflow-airflow-historical-worker-2
klf airflow-airflow-historical-worker-1
klf airflow-airflow-historical-worker-1 airflow-worker
klf airflow-airflow-historical-worker-1 dags-git-sync
cd dev/finout/finout-terraform/clickhouse
kubectx arn:aws:eks:us-east-1:277411487094:cluster/staging-clickhouse-test
terraform init -backend-config=profiles/staging.s3.tfbackend -reconfigure
terraform init -backend-config=profiles/staging.s3.tfbackend -upgrade
git commit -m "almost there"
cd ../airflow
terraform show -var-file=profiles/preprod.tfvars -no-color > tfstate
terraform show -var-file=profiles/preprod.tfvars -no-colors > tfstate
terraform show -no-colors > tfstate
kdp forter-costcenter-amaz-virtual-1679240441-exec-102
kdelp -l account_name=Forter
kdp airflow-airflow-main-worker-0 -n airflow-main
kgp -A | grep -v Running
kgp -n airflow-historical | grep -v Running | awk '{print $1}'
kgp -n airflow-historical | grep -v Running | awk '{print $1}' | xargs kubectl -n airflow-historical delete pods --force --grace-period=0
kgp -n airflow-main --no-headers | grep -v Running | awk '{print $1}' | xargs kubectl -n airflow-main delete pods --force --grace-period=0
rm tfstate
git commit -m "fixed launch templates"
rm test2.txt
kgp -n airflow-historical 
kubectx -d arn:aws:eks:us-east-1:277411487094:cluster/finout-preprod-airflow arn:aws:eks:us-east-1:277411487094:cluster/finout-prod-airflow-spark arn:aws:eks:us-east-1:277411487094:cluster/finout-prod-clickhouse03 arn:aws:eks:us-east-1:277411487094:cluster/staging-clickhouse-test
kubectx -d arn:aws:eks:us-east-1:277411487094:cluster/finout-preprod-airflow
kubectx -d arn:aws:eks:us-east-1:277411487094:cluster/staging-clickhouse-test
kubectx -d arn:aws:eks:us-east-1:277411487094:cluster/finout-prod-airflow-spark\
\

kubectx -d arn:aws:eks:us-east-1:277411487094:cluster/finout-prod-clickhouse03
klf airflow-airflow-historical-worker-2
cd dev/finout/finout-terraform/
aws --profile apono configure set role_arn arn:aws:iam::277411487094:role/yizhar_apono-apono-oidc-role && aws --profile apono configure set web_identity_token_file ~/.aws/token && echo eyJhbGciOiAiUlMyNTYiLCAidHlwIjogIkpXVCJ9.eyJ1c2VyX25hbWUiOiAieWl6aGFyX2Fwb25vLWFwb25vLW9pZGMtcm9sZSIsICJpc3MiOiAiaHR0cHM6Ly9hcG9uby1vaWRjLnMzLmFtYXpvbmF3cy5jb20vYjU1Njk4MzAtYWU4NC0xMWVjLWFlOGUtMGViNmRiMjNkYTQ1IiwgImlhdCI6IDE2NzkyMTA5ODYsICJleHAiOiAxNjc5MjY4NTgyLCAiYXVkIjogIkFXUyIsICJzdWIiOiAieWl6aGFyX2Fwb25vLWFwb25vLW9pZGMtcm9sZSJ9.PmAp2VUHXEpbD7mcYEBPOJ6Z7CqR3Fr0ab32QfSNozYVUYMBwr7wD9_Yt-IZRj9QIauPISOlSsdVtDh3yAwhnY3hOZKhmnbXQ7ntaWFSSPOMSe4159JdxovPjRjRfzgkoJKy1ms20jeGT5GU_RWbT1KTuZ7SfmUMbDSSqWELcPTZM8B4KxX1xBNut-kxNwqNjXBTVjt8F7M7kRe9JzD123KZJRxybDEzOaKhSkgGbQRxLJLDJdVEUkaXnmDAuLoqguSOWwUeiWvidc0aMb89pnxQPf7TYpaJ00_2W2vDwFm9PriDMBdeqMbl_FsClyIm6Z65kBFmTZ6-0CmmtB5M1w > ~/.aws/token
helm values airflow-airflow-historical
helm get values airflow-airflow-historical
helm get values airflow-airflow-historical > historical.yaml
helm upgrade --help
helm upgrade airflow-airflow-historical airflow/airflow -f historical.yaml
kgp | grep -v Running | awk '{print $1}' | xargs kubectl delete pods
klf airflow-airflow-historical-db-migrations-677d88d9bc-n7c55
klf airflow-airflow-historical-db-migrations-677d88d9bc-n7c55 check-db
klf airflow-airflow-historical-db-migrations-677d88d9bc-n7c55 dags-git-clone
yabai -m query --displays --display | jq .display
cd .config/skhd
vim skhdrc
vim yabairc
rm /Library/Preferences/com.apple.keyboardtype.plist
sudo rm /Library/Preferences/com.apple.keyboardtype.plist
ls go
ls go/pkg/mod/github.com/nametake
git clone git@github.com:koekeishiya/yabai.git
cd yabai
yabai -m query --windows --window | jq -re '."split-type" = horizontal'
yabai -m query --windows --window | jq -r '."split-type"'
yabai -m query --windows --window | jq -re '."split-type" == horizontal'
yabai -m window --warp insert west
yabai -m query --windows | jq '.[] | select(.has_focus == false)'
yabai -m query --windows | jq '.[]
yabai -m query --windows | jq '.[] | select(.space == 2)' 
yabai -m query --windows | jq '.[] | select(.space == "1")' 
yabai -m query --windows | jq '.[] | (select(.space == "1"))' 
yabai -m query --windows | jq '.[].space' 
yabai -m query --windows | jq '.[] | select(.space == 1)' 
yabai -m query --windows | jq '.[] | select(.space == 1)) | min_by(.frame.x)'
yabai -m query --windows | jq '.[] | map(select(.space == 1)) | min_by(.frame.x)'
yabai -m query --windows | jq '.[] | map(select(.space == 1))' 
yabai -m query --windows | jq '.[]'
yabai -m query --windows | jq '.[] | select(.space == 1)''
yabai -m query --windows 
yabai -m query --windows | jq '.[] | select(.space == 1) | min_by(.frame.x)'
yabai -m query --windows | jq '.[] | map(select(.space == 1))'
yabai -m query --windows | jq 'map(.[] | select(.space == 1))'
yabai -m query --windows | jq '.[] | select(.space == 1) | map()'
yabai -m query --windows | jq '.[] | select(.space == 1)'
yabai -m query --windows | jq '.[] | select(.space == 1) | map(.frame)'
yabai -m query --windows | jq '(.[] | select(.space == 1)) | min_by(.frame.x)'
yabai -m query --windows | jq '(.[] | select(.space == 1)) | .[] | min_by(.frame.x)'
yabai -m query --windows | jq '(.[] | select(.space == 1)) | .[]'
yabai -m query --windows | jq '(.[] | select(.space == 1))'
yabai -m query --windows | jq '(.[] | select(.space == 1)).frame'
yabai -m query --windows | jq '(.[] | select(.space == 1)) | min_by(.x)'
yabai -m query --displays --display | jq -re '.uuid == FC67F57F-AFD4-9C2E-0857-D6964E3302DB'
yabai -m query --displays --display | jq -re '.uuid == "FC67F57F-AFD4-9C2E-0857-D6964E3302DB"'
HOME_DISPLAY_UUID="FC67F57F-AFD4-9C2E-0857-D6964E3302DB"
yabai -m query --displays --display | jq -re --argjson var "$HOME_DISPLAY_UUID" '.uuid == "$var"'
yabai -m query --displays --display | jq -re --arg var "$HOME_DISPLAY_UUID" '.uuid == "$var"'
yabai -m query --displays --display | jq -r '.uuid'
yabai -m query --displays --display | jq -re --argjson var "$HOME_DISPLAY_UUID" '.uuid == $var'
yabai -m query --displays --display | jq -re --arg var "$HOME_DISPLAY_UUID" '.uuid == $var'
yabai -m query --windows --window --space
yabai -m query --windows --space | jq 'length'
A=1
B=2
echo A
echo $A
A=3
echo $((A * B))
yabai -m spaces --focus next
yabai -m --spaces --focus next
yabai -m space --focus next
date
echo $date $SHELL
echo "$date $SHELL"
echo "$(date) $SHELL"
echo "$(date)\t$SHELL"
echo "$(date)\t\t$SHELL"
yabai -m window --insert 57 west
yabai -m window --insert west 57
yabai -m window --stack 57
yabai -m window --insert --window west 57
yabai -m window swap 57
yabai -m window --swap 57
yabai -m window --move west
yabai -m window --toggle managed
yabai -m window --move rel:0:0
yabai -m window warp west
yabai -m window --warp top
yabai -m query --windows --window last
yabai -m query --windows --window first
yabai -m window --insert --window east 4289
yabai -m window --insert --window 4289 east
yabai -m window --insert window 4289 east
yabai -m window 4289 --insert east
yabai -m window --stack 4289
yabai -m window --stack 203
yabai -m window --swap first
yabai -m window --swap last
yabai -m window 203 --insert east
yabai -m window --warp next
yabai -m window --warp prev
yabai -m window --warp mouse
yabai -m space --create
brew uninstall toodist
brew uninstall todoist
brew uninstall --help
brew uninstall --zap todoist
brew install todoist
brew services stop skhd
brew services start skhd
yabai -m window --warp first
yabai -m window --stack 442
yabai -m window 442 --insert west
yabai -m window --warp last
yabai -m query --space --windows
yabai -m query --spaces --windows
yabai -m query --spaces --window
yabai -m query --spaces --space
yabai -m query --windows --window  | jq -re '."split-type" == "horizontal"'
yabai -m winodw --warp north
yabai -m window 1880 --insert west
yabai -m window --move abs:0:0
yabai -m window --move abs:0:500
yabai -m window --move abs:0:1000
rm todoist
cat lol.log
cat
touch todoist.log
touch todoist-launched.log
cat todoist
rm todoist-launched.log
yabai -m window toggle split
cat todoist-launched.log
yabai -m window --swap east
yabai -m window --swap west
yabai -m window prev --insert west
yabai -m window --insert west
yabai -m query --windows next
yabai -m --windows --window | jq -r '.frame.h'
yabai -m query --windows --space | jq -r '.[2]'
yabai -m query --windows --space | jq -r '.[0]'
yabai -m query --windows --space | jq -r '.[1]'
yabai -m query --windows --window | jq -r '.id'
yabai -m query --windows --window | jq -r '.index'
yabai -m query --windows --window 
yabai -m query --windows | pbcopy
FOCUSED_X=$(yabai -m query --windows --space | jq '.[] | select(.["has-focus"]) | .frame.x')\

echo $FOCUSED_X
LARGEST_WINDOW_ID=$(yabai -m query --windows --space | jq -r --argjson FOCUSED_X "$FOCUSED_X" '[.[] | select(.frame.x == $FOCUSED_X) | {id, area: .frame.w * .frame.h}] | sort_by(-.area) | .[0].id')\

MATCHING_WINDOWS=$(yabai -m query --windows --space | jq --argjson FOCUSED_X "$FOCUSED_X" '.[] | select(.frame.x == $FOCUSED_X) | {id, area: .frame.w * .frame.h} | sort_by(-.area)')\

LARGEST_WINDOW_ID=$(yabai -m query --windows --space | jq --argjson FOCUSED_X "$FOCUSED_X" '[.[] | select(.frame.x == $FOCUSED_X) | {id, area: .frame.w * .frame.h}] | sort_by(-.area) | .[0].id')\

LARGEST_WINDOW_ID=$(yabai -m query --windows --space | jq --argjson FOCUSED_X "$FOCUSED_X" '[.[] | select(.frame.x == 2) | {id, area: .frame.w * .frame.h}] | sort_by(-.area) | .[0].id')\

LARGEST_WINDOW_ID=$(yabai -m query --windows --space | jq '[.[] | select(.frame.x == 2) | {id, area: .frame.w * .frame.h}] | sort_by(-.area) | .[0].id')\

LARGEST_WINDOW_ID=$(yabai -m query --windows --space | jq '[.[] | select(.frame.x == 2) | {id, area: .frame.w \* .frame.h}] | sort_by(-.area) | .[0].id')\

LARGEST_WINDOW_ID=yabai -m query --windows --space | jq '[.[] | select(.frame.x == 2)\
\
asd\
)
LARGEST_WINDOW_ID=yabai -m query --windows --space | jq '.[] | select(.frame.x == 2)'
yabai -m query --windows --space | jq '.[] | select(.frame.x == 2)'
yabai -m query --windows --space | jq '.[] | select(.frame.x == 2 && .has_focus == false)'
yabai -m query --windows --space | jq '.[] | select(.frame.x == 2 && ."has-focus" == false)'
yabai -m query --windows --space | jq '.[] | select(.frame.x == 2 and ."has-focus" == false)'
yabai -m query --windows --space | jq --argjson FOCUSED_X $FOCUSED_X '.[] | select(.frame.x == $FOCUSED_X and ."has-focus" == false)'
yabai -m query --windows --space | jq --argjson FOCUSED_X $FOCUSED_X '.[] | select(.frame.x == $FOCUSED_X and ."has-focus" == false) | sort_by(-.frame.w) | .[0]'
yabai -m query --windows --space | jq --argjson FOCUSED_X "$FOCUSED_X" '.[] | select(.frame.x == $FOCUSED_X and ."has-focus" == false) | sort_by(-.frame.w) | .[0]'
yabai -m query --windows --space | jq --argjson FOCUSED_X "$FOCUSED_X" '(.[] | select(.frame.x == $FOCUSED_X and ."has-focus" == false)) | sort_by(-.frame.w) | .[0]'\

yabai -m query --windows --space | jq --argjson FOCUSED_X "$FOCUSED_X" '(.[] | select(.frame.x == $FOCUSED_X and ."has-focus" == false)) | sort_by(-.frame.w) | .[0]'
yabai -m query --windows --space | jq --argjson FOCUSED_X "$FOCUSED_X" '.[] | select(.frame.x == $FOCUSED_X and ."has-focus" == false) | max_by(.frame.w) | .[0]'
yabai -m query --windows --space | jq --argjson FOCUSED_X "$FOCUSED_X" '.[] | select(.frame.x == $FOCUSED_X and ."has-focus" == false) | max_by(.frame.w)'
yabai -m query --windows --space | jq --argjson FOCUSED_X "$FOCUSED_X" '(.[] | select(.frame.x == $FOCUSED_X and ."has-focus" == false)) | max_by(.frame.w)'
yabai -m query --windows --space | jq --argjson FOCUSED_X "$FOCUSED_X" '.[] | select(.frame and .frame.x == $FOCUSED_X and ."has-focus" == false) | sort_by(-.frame.w) | .[0]'\

yabai -m query --windows --space | jq --argjson FOCUSED_X "$FOCUSED_X" '.[] | select(.frame and .frame.x == $FOCUSED_X and ."has-focus" == false) | sort_by(-.frame.w) | .[0]'
yabai -m query --windows --space | jq --argjson FOCUSED_X "$FOCUSED_X" '(.[] | select(.frame and .frame.x == $FOCUSED_X and ."has-focus" == false)) | sort_by(-.frame.w) | .[0]'
yabai -m query --windows --space | jq --argjson FOCUSED_X "$FOCUSED_X" '(.[] | select(.frame and .frame.x == $FOCUSED_X and ."has-focus" == false)) | sort_by(-.frame.w)'
yabai -m query --windows --space | jq --argjson FOCUSED_X "$FOCUSED_X" '(.[] | select(.frame and .frame.x == $FOCUSED_X and ."has-focus" == false))'
yabai -m query --windows --space | jq --argjson FOCUSED_X "$FOCUSED_X" '(.[] | select(.frame and .frame.x == $FOCUSED_X and ."has-focus" == false)).frame.w'
yabai -m query --windows --space | jq --argjson FOCUSED_X "$FOCUSED_X" '.[] | select(.frame and .frame.x == $FOCUSED_X and ."has-focus" == false) | if .frame.w then . else empty end | sort_by(-.frame.w) | .[0]'\

yabai -m query --windows --space | jq --argjson FOCUSED_X "$FOCUSED_X" '[.[] | select(.frame and .frame.x == $FOCUSED_X and ."has-focus" == false) | sort_by(-.frame.w) | .[0]] | if length == 1 then .[0] else . end'\

yabai -m query --windows --space | jq --argjson FOCUSED_X "$FOCUSED_X" '[.[] | select(.frame and .frame.x == $FOCUSED_X and ."has-focus" == false)] | sort_by(-.frame.w) | .[0]'\

yabai -m query --windows --space | jq --argjson FOCUSED_X "$FOCUSED_X" '[.[] | select(.frame and .frame.x == $FOCUSED_X and ."has-focus" == false)] | max_by(-.frame.w)'\

chmod +x focus.zsh
rm todoist*
cat todoist.log
rm todoist.log
cd .config/yabai
chmod +x todoist.zsh
yabai -m window first --insert west
yabai -m space --balance
FOCUSED_X=$(yabai -m query --windows --window | jq -re '.frame.x')
INSERT_TO_WINDOW_ID=yabai -m query --windows --space | jq --argjson FOCUSED_X "$FOCUSED_X" '[.[] | select(.frame and .frame.x == $FOCUSED_X and ."has-focus" == false)] | max_by(-.frame.w).id'
INSERT_TO_WINDOW_ID=$(yabai -m query --windows --space | jq --argjson FOCUSED_X "$FOCUSED_X" '[.[] | select(.frame and .frame.x == $FOCUSED_X and ."has-focus" == false)] | max_by(-.frame.w).id')
echo $INSERT_TO_WINDOW_ID
yabai -m window $INSERT_TO_WINDOW_ID --toggle float
yabai -m query --windows --space | jq '.[] | select(.id == 2440)'
yabai -m query --windows --space | jq '.[] | select(.id == 2449)'
yabai -m query --windows --space | jq --argjson FOCUSED_X "$FOCUSED_X" '[.[] | select(.frame and .frame.x == $FOCUSED_X and ."has-focus" == false)] | max_by(-.frame.w).id
yabai -m query --windows --space | jq --argjson FOCUSED_X "$FOCUSED_X" '[.[] | select(.frame and .frame.x == $FOCUSED_X and ."has-focus" == false)] | max_by(-.frame.w).id'
yabai -m query --windows --space | jq '.[] | select(.id == 84)'
yabai -m window 84 --toggle split
yabai -m query --windows --window | jq -re '.frame.x
yabai -m query --windows --window | jq -re '.frame.x'
yabai -m query --windows --window | jq '.frame'
yabai -m query --windows --space | jq --argjson FOCUSED_X "3838" '[.[] | select(.frame and (.frame.x + .frame.w) == $FOCUSED_X and ."has-focus" == false)] | max_by(.frame.w).id
yabai -m query --windows --space | jq --argjson FOCUSED_X "3838" '[.[] | select(.frame and (.frame.x + .frame.w) == $FOCUSED_X and ."has-focus" == false)] | max_by(.frame.w).id'
yabai -m --windows --window
yabai -m query --windows --space | jq '.[] | select(.id == 2713)'
yabai -m window --toggle float
yabai -m query --windows --window | jq -re '."split-type" == "vertical"'
rm lol.log
yabai -m query --windows --space | jq '.[] | min_by(.frame.x)'
yabai -m query --windows --space | jq '.[] | min_by(-.frame.x)'
cd .config
cd skhd
chmod +x edge.zsh
./edge.zsh
yabai -m query --windows --space | jq '.[] | max_by(.frame.x + .frame.w)'
yabai -m query --windows --space | jq '[.[] | {id, "sum": .frame.x + .frame.w}]'
jq '[.[] | {id, "sum": .frame.x + .frame.w}]'\

yabai -m query --windows --space  |jq '[.[] | {id, "sum": .frame.x + .frame.w}]'\

yabai -m query --windows --space  | jq '[.[] | {id, "sum": .frame.x + .frame.w}]'\

yabai -m query --windows --space  | jq '[.[] | {id, sum: .frame.x + .frame.w}] | sort_by(-.sum)'
yabai -m query --windows --space | jq '[.[] | {id, sum: .frame.x + .frame.w}] | sort_by(-.sum)'\

yabai -m query --windows --window | jq -r '.frame.x + .frame.w'
yabai -m query --windows --space | jq -re '.[] | select(.frame.x + .frame.w == 3838)'
yabai -m query --windows --space | jq -re '.[] | select(.frame.x + .frame.w == 3838 and ."has-focus" != true)'
echo #?
yabai -m query --windows --space | jq -re '.[] | select(.frame.x + .frame.w == 3838 and ."has-focus" == false)'
yabai -m query --windows --space | jq -re '.[] | select(.frame.x + .frame.w >= 3838 and ."has-focus" == false)'
yabai -m query --windows --space | jq -re '.[] | select(.frame.x + .frame.w >= 200 and ."has-focus" == false)'
focused_window_right_x=$(yabai -m query --windows --window | jq -r '.frame.x + .frame.w')
focused_window_right_x=200
more_to_the_right=$(yabai -m query --windows --space | jq --arg width "${focused_window_right_x}" -re '.[] | select(.frame.x + .frame.w >= $width and ."has-focus" == false)')
more_to_the_right=$(yabai -m query --windows --space | jq --argjson width "${focused_window_right_x}" -re '.[] | select(.frame.x + .frame.w >= $width and ."has-focus" == false)')
echo $more_to_the_right
yabai -m window --swap east || yabai -m window --warp east
yabai -m window --warp west
yabai -m window --warp east
yabai -m window --toggle split
skhd --observe
yabai -m query --windows --window | jq -r '.frame.y'
yabai -m query --windows --window | jq -r '.frame.h'
yabai -m query --windows --space | jq --argjson width "26" -re '.[] | select(.frame.y <= $width and ."has-focus" == false)'
yabai -m query --windows --space | jq --argjson width "26" -re '.[] | select(.frame.y < $width and ."has-focus" == false)'
yabai -m window --warp north
if yabai -m query --windows --window | jq -re '."split-type" == "vertical"'; then echo hi; fi
yabai -m query --displays --display | jq '.frame.w / 4'
yabai -m query --windows --space | jq -r 'length - 1'
yabai -m space --toggle float
yabai -m space --toggle managed
yabai -m window --grid 2:1:0:0:2:1
yabai -m window --grid 1:1:0:0:2:1
yabai -m window --grid 1:4:0:0:2:1
yabai -m window --grid 1:4:1:0:0:1
yabai -m window --grid 1:4:1:0:2:1
yabai -m window --grid 1:4:1:0:2:2
yabai -m query --windows --space | jq -r '(.[] | select(."has-focus" != true)) | min_by(.frame.x)'
yabai -m query --windows --space | jq -r '.[] | min_by(.frame.x)'
yabai -m query --windows --space | jq -r 'min_by(.frame.y)'
yabai -m query --windows --space | jq -r 'min_by(.frame.x)'
yabai -m query --windows --space | jq -r 'max_by(.frame.x)'
yabai -m query --windows --space | jq -r '[.[] | select(."has-focus" != true)] | min_by(.frame.x).id'
yabai -m window 3891 --grid 1:4:0:0:1:0
yabai -m window 3891 --grid 1:4:3:0:1:0
yabai -m window 509 --grid 1:4:0:0:1:0
yabai -m query --windows --space | jq -r '[.[] | select(."has-focus" != true)] | min_by(.frame.x) | length'
yabai -m query --windows --space | jq -r '[.[] | select(."has-focus" != true)] | min_by(.frame.x)'
yabai -m query --windows --space | jq -r '[.[] | select(."has-focus" != true)] | max_by(.frame.x)'
tail focus.log
yabai -m window 6001 --resize right:636:0
yabai -m window 6001 --resize left:636:0
yabai -m window --grid 2:1:0:0:2:0
yabai -m window --grid 1:1:0:0:2:0
yabai -m window --grid 1:1:2:0:2:0
yabai -m window --grid 1:2::0:2:0
yabai -m window --grid 1:2:0:0:2:0
yabai -m window --grid 2:2:0:2:2:0
yabai -m window --grid 1:2:0:2:2:0
yabai -m window --grid 1:1:0:4:2:0
yabai -m window --grid 1:1:0:4:4:0
yabai -m window --grid 2:1:0:0:4:0
yabai -m window --grid 1:1:0:0:4:0
yabai -m window --grid 1:2:0:0:4:0
yabai -m window --grid 1:2:0:0:4:2
yabai -m window --grid 3:3:0:0:2:2
yabai -m window --grid 3:3:0:0:2:1
yabai -m config split_ratio 0.5
yabai -m config split_ratio 0.33
yabai -m window --grid 3:1:0:0:2:1
yabai -m window --grid 3:3:0:0:2:3
yabai -m window --grid 1:3:0:0:2:3
yabai -m window --grid 1:3:0:2:2:0
yabai -m window --grid 1:3:0:2:0:1
yabai -m window --grid 1:3:0:0:2:1
yabai -m window --grid 1:3:0:0:1:1
yabai -m window --grid 1:3:2:0:1:1
yabai -m query --windows
yabai -m window 3891 --grid 1:3:2:0:1:1
current_window=$(yabai -m query --windows --window | jq -r '.id')
echo $current_window
echo "$state" | jq -r --argjson current_window "$current_window" '.[] | select(.id != "$current_window")'
echo "$state" | jq -r --argjson current_window "$current_window" '.[] | select(.id != $current_window)'
yabai -m window --grid 1:3:0:0:2:0
echo "$state" | jq -r 'length'
echo "$state" | jq -r --argjson '.[] | select(."has-focus" == false) | .id'
echo "$state" | jq -re --argjson '.[] | select(."has-focus" == false) | .id'
echo "$state" | jq -re '.[] | select(."has-focus" == false) | .id'
yabai -m config window_placement first_child
yabai -m config window_placement second_child
yabai -m window --grid 1:4:1:0:2:0
other_window=$(echo "$state" | jq -re '.[] | select(."has-focus" != true) | .id')
echo $other_window
yabai -m config split_ratio .33
yabai -m window --insert east
yabai -m window 3891 --toggle float
yabai -m config split_ratio .5
yabai -m config split_ratio .66
yabai -m config layout float
yabai -m window --ratio .3
yabai -m window --ratio 3
yabai -m window --ratio left:.3
yabai -m window --ratio left:.2
yabai -m window --ratio left:.1
yabai -m window --ratio left:.5
yabai -m window --ratio right:20
yabai -m window --ratio right:20:5
yabai -m query --displays --display | jq '.frame'
yabai -m query --windows --window | jq -r '.frame'
window_count=$(echo "$state" | jq -r 'length')
FOCUS_RATIO=1.6
echo $display_size
yabai -m query --windows --space | jq -re '.[] | select(.frame.x == $FOCUSED_STATE_X and .frame.y == $FOCUSED_STATE_Y and .frame.w == $FOCUSED_STATE_W and .frame.h == $FOCUSED_STATE_H) | .id'
FOCUSED_STATE_X=726\
FOCUSED_STATE_Y=26\
FOCUSED_STATE_W=2388\
FOCUSED_STATE_H=2132
yabai -m query --windows --space | jq -re --argjson x "{$FOCUSED_STATE_X}" --argjson y "{$FOCUSED_STATE_Y}" --argjson w "{$FOCUSED_STATE_W}" --argjson h "{$FOCUSED_STATE_H}" '.[] | select(.frame.x == $x and .frame.y == $y and .frame.w == $w and .frame.h == $h) | .id'
echo $?
yabai -m query --windows --space | jq -re --arg x "{$FOCUSED_STATE_X}" --arg y "{$FOCUSED_STATE_Y}" --arg w "{$FOCUSED_STATE_W}" --arg h "{$FOCUSED_STATE_H}" '.[] | select(.frame.x == $x and .frame.y == $y and .frame.w == $w and .frame.h == $h) | .id'
yabai -m query --windows --space | jq -re --arg x "{$FOCUSED_STATE_X}" --arg y "{$FOCUSED_STATE_Y}" --arg w "{$FOCUSED_STATE_W}" --arg h "{$FOCUSED_STATE_H}" '.[] | select(.frame.x == $x) | .id'
mv copilot.lua lua/plugins
man yabai
yabai -m window --ratio rel:left:20
yabai -m window --ratio left:20
yabai -m window --ratio left:20:20
yabai -m window --ratio left:40:20
yabai -m window --ratio bottom:3:1
yabai -m query --windows --window
yabai -m window 6001 --resize right:-200
yabai -m window 6001 --resize right:-200:0
yabai -m window 6001 --resize right:-20:0
yabai -m window 6001 --resize right:-20:-20
yabai -m window 6001 --resize left:-20:0
yabai -m window 6001 --resize left:20:0
block_size=$(yabai -m query --displays --display | jq -r '.frame.w / 4')
echo $block_size
rightmost=$(yabai -m query --windows --space | jq -r '[.[] | select(."has-focus" != true)] | max_by(.frame.x)')
echo $rightmost
yabai -m config layout bsp
state=$(yabai -m query --windows --space)
rightmost=$(yabai -m query --windows --space | jq -r '[.[] | select(."has-focus" != true)] | max_by(.frame.x).id')
yabai -m query --windows --window $rightmost | jq -r --argjson block_size "$block_size" '.frame.w - $block_size'
yabai -m query --windows --window $leftmost | jq -r --argjson block_size "$block_size" '.frame.w - $block_size'
delta=$(yabai -m query --windows --window $rightmost | jq -r --argjson block_size "$block_size" '.frame.w - $block_size')
yabai -m window $leftmost --resize right:$delta:0
yabai -m window $leftmost --resize right:-$delta:0
leftmost=$(yabai -m query --windows --space | jq -r '[.[] | select(."has-focus" != true)] | min_by(.frame.x).id')
yabai -m window $rightmost --resize left:$delta:0
yabai -m window $rightmost --resize left:-$delta:0
yabai -m window $rightmost --resize right:$delta:0
display_size=$(yabai -m query --displays --display | jq -r '.frame')\
focus_size=$(echo $display_size | jq -r '.w / 1.75')
focus_size=$(echo $display_size | jq -r 'ceil(.w / 1.75)')
focus_size=$(echo $display_size | jq -r '.w / 1.75 | floor')
FOCUS_RATIO=1.75
focus_size=$(echo $display_size | jq -r --arg ratio "$FOCUS_RATIO" '.w / $ratio | floor')
focus_size=$(echo $display_size | jq -r --argjson ratio "$FOCUS_RATIO" '.w / $ratio | floor')
echo ~
echo $f
echo $focus_size
delta=$((($focus_size / 2) - ($window_size)))
display_size=$(yabai -m query --displays --display | jq -r '.frame')
focus_width=$(echo $display_size | jq -r --argjson ratio "$FOCUS_RATIO" '.w / $ratio | floor')
unfocused_block_width=$((($display_size - $focus_width) / 2)))
unfocused_block_width=$((($display_size - $focus_width) / 2))
unfocused_block_width=$((($focus_width) / 2))
echo $focused_width
echo $focus_width
yabai -m query --displays --display
unfocused_block_width=$(echo $display_size | jq -r --argjson ratio "$focus_width" '(.w - $ratio) / 2 | floor')
echo $unfocused_block_width
window_size=$(yabai -m query --windows --window | jq -r '.frame.w')
delta=$((window_size - $unfocused_block_width))
echo $delta
yabai -m window 509 --resize right:453:0
yabai -m query --windows 509
yabai -m query --windows --window 509
yabai -m window --move rel:100:0
yabai -m window --move rel:-100:0
yabai -m window --resize rel:-100:0
yabai -m window --resize left:-100:0
yabai -m query --windows --space
tail -f focus.log
yabai -m query --windows --space | jq -r '.[] | select(."has-focus" != true and .id not in [509, 3891])'
yabai -m query --windows --space | jq -rc '.[] | select(."has-focus" != true && .id - [509, 3891])'
yabai -m query --windows --space | jq -rc '.[] | select(."has-focus" != true and .id - [509, 3891])'
yabai -m query --windows --space | jq -rc '.id - [509,3891]'
yabai -m query --windows --space | jq -c '.id - [509,3891]'
yabai -m query --windows --space | jq -c '.[].id - [509,3891]'
yabai -m query --windows --space 
yabai -m query --windows --space  | pbcopy
yabai -m query --windows --space | jq -r '.[] | select(."has-focus" != true and .id  | not in (509, 3891))'
yabai -m query --windows --space | jq -r '.[] | select(."has-focus" != true and .id | not in (509, 3891))'\

yabai -m query --windows --space | jq -r '.[] | select(."has-focus" != true and .id != 509 and .id != 3891)'\

yabai -m query --windows --space | jq -r '.[] | select(."has-focus" != true)'
wins=yabai -m query --windows --space | jq -r '.[] | select(."has-focus" != true)'
wins=$(yabai -m query --windows --space | jq -r '.[] | select(."has-focus" != true)')
wins
wins_len=$(echo "$wins" | jq '. | length')\
\
# Calculate the length of each slice\
slice_len=$(($wins_len / 2))\
slice_len2=$(($wins_len - $slice_len))\
\
# Slice the wins object into two lists of equal length (or one larger)\
wins1=$(echo "$wins" | jq ".[:$slice_len]")\
wins2=$(echo "$wins" | jq ".[-$slice_len2:]")
wins_len=$(echo "$wins" | jq '. | length')\

remaining_windows=$(yabai -m query --windows --space | jq -r --argjson leftmost "$leftmost" --argjson rightmost "$rightmost" '.[] | select(."has-focus" != true).id')
echo "$remaining_windows" | jq '. | length'
echo "$remaining_windows" | jq '.[] | length'
echo "$remaining_windows" | wc -l
echo "$remaining_windows" | wc -rl
echo "$remaining_windows" | wc 
echo "$remaining_windows" 
echo $wins
echo $wins_len
remaining_windows=$(yabai -m query --windows --space | jq -r --argjson leftmost "$leftmost" --argjson rightmost "$rightmost" '[.[] | select(."has-focus" != true and .id != $leftmost and .id != $rightmost).id]')
remaining_windows=$(yabai -m query --windows --space | jq -r --argjson leftmost "$leftmost" --argjson rightmost "$rightmost" '[.[] | select(."has-focus" != true).id]')
echo $remaining_windows
wins_len=$(echo "$remaining_windows" | jq '. | length')
slice_len=$(($wins_len / 2))
echo $slice_len
slice_len2=$(($wins_len - $slice_len))
echo $slice_len2
wins1=$(echo "$remaining_windows" | jq ".[:$slice_len]")
wins2=$(echo "$remaining_windows" | jq ".[-$slice_len2:]")
echo $wins2
wins1=$(echo "$remaining_windows" | jq -r ".[:$slice_len]")
echo $wins1
echo "$remaining_windows" | jq ".[:$slice_len]"
echo "$remaining_windows" | jq -r ".[:$slice_len].[]"
echo "$remaining_windows" | jq -r ".[:$slice_len]"
echo "$remaining_windows" | jq -rc ".[:$slice_len]"
echo "$remaining_windows" | jq -rc ".[:$slice_len] | .[]"
skhd --reload
$
FOCUS_THRESHOLD=0.6\

FOCUS_RATIO=$((1+$FOCUS_THRESHOLD))\

echo $FOCUS_RATIO
myarray=( "test1" "test2" "test3" "test4")\
for (( i=1;  i < ${#myarray[@]};  i++ ))\
do\
    # only print the indices to simplify the example\
    echo $i \
done
myarray=( "test1" "test2" "test3" "test4")\
for (( i=1;  i < ${#myarray[@]};  i++ ))\
do\
    echo $i \
done
myarray=( "test1" "test2" "test3" "test4")\
for (( i=2;  i < ${#myarray[@]};  i++ ))\
do\
    echo $i \
done
myarray=( "test1" "test2" "test3" "test4")\
for (( i=2;  i <= ${#myarray[@]};  i++ ))\
do\
    echo $i \
done
brew install balance
brew install pget
gem install colorls
sudo gem install colorls
colorsls
colorls
colorls yabai
colorls -la
gem uninstall colorls
sudo gem uninstall colorls
cd ~/.local/share/nvim/site/
ls pack
rm -rf karabiner
cd ~/.config/nvim
mkdir lua
mkdir dormunis
touch options.lua
vim options.lua
rm -r lua
git clone https://github.com/LazyVim/starter ~/.config/nvim\
\

rm -rf ~/.config/nvim/.git
rm copilot.lua
rm -rf yabai
open -a "Google Chrome" google.com
open -a "Google Chrome" https://google.com
open -a "Todoist"
rm -rf bak.nvim
cd github-copilot
cat hosts.json
git clone https://github.com/github/copilot.vim \\
   ~/.config/nvim/pack/github/start/copilot.vim
rm -rf ~/.config/nvim/pack/github/start/copilot.vim
rm -rf pack
rm focus.log
cd ~/dev
brew install ripgrep
brew install fd
brew install lazygit\

cd ~/.config/
mv lua/plugins/treesitter.nvim lua/plugins/treesitter.lua
cd .config/nvim
brew install --cask --help
brew install google-chrome
brew install --cask visual-studio-code
brew install visual-studio-code
pget https://www.xtrafondos.com/wallpapers/superman-de-espaldas-7443.jpg -O ~/Pictures/superman-wallpaper.jpg
pget https://www.xtrafondos.com/wallpapers/superman-de-espaldas-7443.jpg -o ~/Pictures/superman-wallpaper.jpg
rm RectangleConfig
cat /System/Library/LaunchDaemons/com.apple.pfctl.plist
cat /System/Library/LaunchDaemons/
cd /System/Library/LaunchDaemons/
cd LaunchAgents
aws --profile apono configure set role_arn arn:aws:iam::277411487094:role/yizhar_apono-apono-oidc-role && aws --profile apono configure set web_identity_token_file ~/.aws/token && echo eyJhbGciOiAiUlMyNTYiLCAidHlwIjogIkpXVCJ9.eyJ1c2VyX25hbWUiOiAieWl6aGFyX2Fwb25vLWFwb25vLW9pZGMtcm9sZSIsICJpc3MiOiAiaHR0cHM6Ly9hcG9uby1vaWRjLnMzLmFtYXpvbmF3cy5jb20vYjU1Njk4MzAtYWU4NC0xMWVjLWFlOGUtMGViNmRiMjNkYTQ1IiwgImlhdCI6IDE2Nzk4OTc4NzEsICJleHAiOiAxNjc5OTU1NDY3LCAiYXVkIjogIkFXUyIsICJzdWIiOiAieWl6aGFyX2Fwb25vLWFwb25vLW9pZGMtcm9sZSJ9.BHsbDHxfwiEVfjwS_eCAD6h4gn4prjkYb5GMd2E3zTGcAqCeLpvd-PCD2d-62fqEoMMhiUYOprCiSUJlJeMaW2STZ-lp8GQA2oOFK62Ey49TpJedUtp_m7EQkXlbmQ3ZprA_e-gpX_BTxhh4mRLju-Tt4kSte0Q9xh1FBrsRUOpXWUen4WXOVsJye2UaomSy-46bENVn4un52j2nhEoPa9s6ab4SybURK5RaAKsw6zUh9q1blnUfSkHEFJvsJzY-MkzFk1PkwwEhwttNSLGvUN9bh0OkkzKeDW_9MFGBq0lChLaeOpLldKjkVmodaQ_Sorjdk-51_R9Nan11oSBFDA > ~/.aws/token
helm uninstall -n airflow-historical airflow-airflow-historical
helm uninstall -n airflow-airflow-airflow-main airflow-airflow-main
helm uninstall -n airflow-airflow-main airflow-main
git diff --cached
klf airflow-airflow-historical-db-migrations-677d88d9bc-24fm2
klf airflow-airflow-historical-db-migrations-677d88d9bc-24fm2 db-migrations
klf airflow-airflow-historical-db-migrations-677d88d9bc-24fm2 check-db
klf airflow-airflow-historical-db-migrations-677d88d9bc-24fm2 dags-git-clone
keti airflow-airflow-historical-db-migrations-677d88d9bc-24fm2 dags-git-clone
keti airflow-airflow-historical-db-migrations-677d88d9bc-24fm2 -c dags-git-clone
keti airflow-airflow-historical-db-migrations-677d88d9bc-24fm2 -c dags-git-clone bash
kgp -n hisotorical
kgp -n historical
ked airflow-airflow-historical-db-migrations-677d88d9bc-24fm2
klf airflow-airflow-historical-db-migrations-5dccbbfbc7-99kdc -c dags-git-clone bash
klf airflow-airflow-historical-db-migrations-5dccbbfbc7-99kdc -c dags-git-clone
helm list airflow/airflow
klf airflow-airflow-historical-db-migrations-f68c5d6c8-j6qsx -c dags-git-clone
kgd -o yaml airflow-airflow-historical-db-migrations
kgd -o yaml airflow-airflow-historical-db-migrations -n airflow-historical
klf airflow-airflow-historical-db-migrations-677d88d9bc-zxwjh dags-git-clone
keti airflow-airflow-historical-db-migrations-677d88d9bc-zxwjh -c dags-git-clone ls /dags
kdelp airflow-airflow-historical-db-migrations-677d88d9bc-zxwjh
kubectgl airflow-airflow-historical-db-migrations-677d88d9bc-lxpjs -c dags-git-clone ls /dags
kubectl airflow-airflow-historical-db-migrations-677d88d9bc-lxpjs -c dags-git-clone ls /dags
keti airflow-airflow-historical-db-migrations-677d88d9bc-lxpjs -c dags-git-clone ls /dags
klf airflow-airflow-historical-flower-7b57fd8648-tl72m
brew services restart yabai
helm ls --all -A
kgp -n airflow | grep db-migrations
kgd -o yaml airflow-db-migrations
kgd -o yaml airflow-db-migrations -n airflow
klf airflow-airflow-historical-sync-connections-84f4757944-b246f
klf airflow-airflow-historical-sync-connections-84f4757944-b246f dags-git-clone
klf airflow-airflow-historical-db-migrations-677d88d9bc-lxpjs dags-git-clone
klf airflow-airflow-historical-worker-4
klf airflow-airflow-historical-worker-4 dags-git-clone
kpg
klf airflow-airflow-historical-db-migrations-85bf67c67b-v2qcx dags-git-clone
kgd -o yaml airflow-db-migrations > lol.yaml
kgd -o yaml airflow-airflow-db-migrations > lol.yaml
kgd -o yaml airflow-airflow-db-migrations 
kgd -o yaml airflow-airflow-historical-db-migrations 
kgd -o yaml airflow-airflow-historical-db-migrations > lol.yaml
kgsec -o json airflow-ssh-git-secret
kgsec -o json airflow-ssh-git-secret | jq -r '.data.gitSshKey' | base64 -D
kgsec -o json airflow-ssh-git-secret | jq -r '.data.gitSshKey' | base64 -D > sshkey
chmod 420 sshkey
mv sshkey idrsa
docker run -e GIT_SYNC_ROOT="/dags" \ \
-e GIT_SYNC_DEST="repo" \ \
-e GIT_SYNC_REPO="git@github.com:finout-io/finout-data-pipelines.git" \\
-e GIT_SYNC_BRANCH="main" \\
-e GIT_SYNC_REV="HEAD" \\
-e GIT_SYNC_DEPTH="1" \\
-e GIT_SYNC_WAIT="60" \\
-e GIT_SYNC_TIMEOUT="120" \\
-e GIT_SYNC_ADD_USER="true" \\
-e GIT_SYNC_MAX_SYNC_FAILURES="0" \\
-e GIT_SYNC_SSH="true" \\
-e GIT_SSH_KEY_FILE="/etc/git-secret/id_rsa" \\
-e GIT_KNOWN_HOSTS="false"\
-v idrsa:/etc/git-secret/id_rsa \\
k8s.gcr.io/git-sync/git-sync:v3.5.0
mkdir dags
docker run -e GIT_SYNC_ROOT="/dags" -e GIT_SYNC_DEST="repo" -e GIT_SYNC_REPO="git@github.com:finout-io/finout-data-pipelines.git" -e GIT_SYNC_BRANCH="main" -e GIT_SYNC_REV="HEAD" -e GIT_SYNC_DEPTH="1" -e GIT_SYNC_WAIT="60" -e GIT_SYNC_TIMEOUT="120" -e GIT_SYNC_ADD_USER="true" -e GIT_SYNC_MAX_SYNC_FAILURES="0" -e GIT_SYNC_SSH="true" -e GIT_SSH_KEY_FILE="/etc/git-secret/id_rsa" -e GIT_KNOWN_HOSTS="false" -v idrsa:/etc/git-secret/id_rsa k8s.gcr.io/git-sync/git-sync:v3.5.0
docker run -e GIT_SYNC_ROOT="/dags" -e GIT_SYNC_DEST="repo" -e GIT_SYNC_REPO="git@github.com:finout-io/finout-data-pipelines.git" -e GIT_SYNC_BRANCH="main" -e GIT_SYNC_REV="HEAD" -e GIT_SYNC_DEPTH="1" -e GIT_SYNC_WAIT="60" -e GIT_SYNC_TIMEOUT="120" -e GIT_SYNC_ADD_USER="true" -e GIT_SYNC_MAX_SYNC_FAILURES="0" -e GIT_SYNC_SSH="true" -e GIT_SSH_KEY_FILE="/etc/git-secret/id_rsa" -e GIT_KNOWN_HOSTS="false" -v idrsa:/etc/git-secret/id_rsa -v dags:/dags:rw k8s.gcr.io/git-sync/git-sync:v3.5.0 bash
docker run -e GIT_SYNC_ROOT="/dags" -e GIT_SYNC_DEST="repo" -e GIT_SYNC_REPO="git@github.com:finout-io/finout-data-pipelines.git" -e GIT_SYNC_BRANCH="main" -e GIT_SYNC_REV="HEAD" -e GIT_SYNC_DEPTH="1" -e GIT_SYNC_WAIT="60" -e GIT_SYNC_TIMEOUT="120" -e GIT_SYNC_ADD_USER="true" -e GIT_SYNC_MAX_SYNC_FAILURES="0" -e GIT_SYNC_SSH="true" -e GIT_SSH_KEY_FILE="/etc/git-secret/id_rsa" -e GIT_KNOWN_HOSTS="false" -v idrsa:/etc/git-secret/id_rsa -v dags:/dags:rw k8s.gcr.io/git-sync/git-sync:v3.5.0 -c bash
docker run -e GIT_SYNC_ROOT="/dags" -e GIT_SYNC_DEST="repo" -e GIT_SYNC_REPO="git@github.com:finout-io/finout-data-pipelines.git" -e GIT_SYNC_BRANCH="main" -e GIT_SYNC_REV="HEAD" -e GIT_SYNC_DEPTH="1" -e GIT_SYNC_WAIT="60" -e GIT_SYNC_TIMEOUT="120" -e GIT_SYNC_ADD_USER="true" -e GIT_SYNC_MAX_SYNC_FAILURES="0" -e GIT_SYNC_SSH="true" -e GIT_SSH_KEY_FILE="/etc/git-secret/id_rsa" -e GIT_KNOWN_HOSTS="false" -v idrsa:/etc/git-secret/id_rsa -v dags:/dags:rw k8s.gcr.io/git-sync/git-sync:v3.5.0
docker volumes
docker volume create
docker volume
docker volume ls
docker run -e GIT_SYNC_ROOT="/dags" -e GIT_SYNC_DEST="repo" -e GIT_SYNC_REPO="git@github.com:finout-io/finout-data-pipelines.git" -e GIT_SYNC_BRANCH="main" -e GIT_SYNC_REV="HEAD" -e GIT_SYNC_DEPTH="1" -e GIT_SYNC_WAIT="60" -e GIT_SYNC_TIMEOUT="120" -e GIT_SYNC_ADD_USER="true" -e GIT_SYNC_MAX_SYNC_FAILURES="0" -e GIT_SYNC_SSH="true" -e GIT_SSH_KEY_FILE="/etc/git-secret/id_rsa" -e GIT_KNOWN_HOSTS="false" -v idrsa:/etc/git-secret/id_rsa -v dags:/opt/airflow/dags k8s.gcr.io/git-sync/git-sync:v3.5.0
docker run -e GIT_SYNC_ROOT="/dags" -e GIT_SYNC_DEST="repo" -e GIT_SYNC_REPO="git@github.com:finout-io/finout-data-pipelines.git" -e GIT_SYNC_BRANCH="main" -e GIT_SYNC_REV="HEAD" -e GIT_SYNC_DEPTH="1" -e GIT_SYNC_WAIT="60" -e GIT_SYNC_TIMEOUT="120" -e GIT_SYNC_ADD_USER="true" -e GIT_SYNC_MAX_SYNC_FAILURES="0" -e GIT_SYNC_SSH="true" -e GIT_SSH_KEY_FILE="/etc/git-secret/id_rsa" -e GIT_KNOWN_HOSTS="false" -v idrsa:/etc/git-secret/id_rsa -v dags:/dags k8s.gcr.io/git-sync/git-sync:v3.5.0
docker run -e GIT_SYNC_ROOT="/dags" -e GIT_SYNC_DEST="repo" -e GIT_SYNC_REPO="https://github.com/finout-io/finout-data-pipelines.git" -e GIT_SYNC_BRANCH="main" -e GIT_SYNC_REV="HEAD" -e GIT_SYNC_DEPTH="1" -e GIT_SYNC_WAIT="60" -e GIT_SYNC_TIMEOUT="120" -e GIT_SYNC_ADD_USER="true" -e GIT_SYNC_MAX_SYNC_FAILURES="0" -e GIT_SYNC_SSH="true" -e GIT_SSH_KEY_FILE="/etc/git-secret/id_rsa" -e GIT_KNOWN_HOSTS="false" -v idrsa:/etc/git-secret/id_rsa -v dags:/dags k8s.gcr.io/git-sync/git-sync:v3.5.0
docker ps
docker ps -a
docker ps -a | awk '{print $11}'
docker ps -a | awk '{print $1}'
docker run -e GIT_SYNC_ROOT="/dags" -e GIT_SYNC_DEST="repo" -e GIT_SYNC_REPO="https://github.com/finout-io/finout-data-pipelines.git" -e GIT_SYNC_BRANCH="main" -e GIT_SYNC_REV="HEAD" -e GIT_SYNC_DEPTH="1" -e GIT_SYNC_WAIT="60" -e GIT_SYNC_TIMEOUT="120" -e GIT_SYNC_ADD_USER="true" -e GIT_SYNC_MAX_SYNC_FAILURES="0" -e GIT_SYNC_SSH="true" -e GIT_SSH_KEY_FILE="/etc/git-secret/id_rsa" -e GIT_KNOWN_HOSTS="false" -v idrsa:/etc/git-secret/id_rsa -v dags:/dags --rm k8s.gcr.io/git-sync/git-sync:v3.5.0
docker ps -a | awk '{print $1}' | xargs docker rm
kgsec -n airflow -o json airflow-ssh-git-secret
kgsec -n airflow -o json airflow-ssh-git-secret | jq -r '.data.gitSshKey'
kgsec -n airflow -o json airflow-ssh-git-secret | jq -r '.data.gitSshKey' | base64 -D
helm ls -n airflow
helm values airflow -n airflow
helm get values airflow -n airflow
kgsec -n airflow airflow-sync-users -o json
kgsec -n airflow airflow-sync-users -o json | jq -r '.data."sync-users.py"'
kgsec -n airflow airflow-sync-users -o json | jq -r '.data."sync_users.py"'
kgsec -n airflow airflow-sync-users -o json | jq -r '.data."sync_users.py"' | base64 -D
kgsec -n airflow
kgd -n airflow
kgd airflow-sync-users -o yaml
kgcm | grep -v spark
kgp | airflow
keti airflow-web-58fcb8ccf6-4v2bp bash
echo $DATABASE_PSQL_CMD
kgsec airflow-airflow-historical-pgbouncer -o json
kgsec airflow-airflow-historical-pgbouncer -o json | jq -r '.data."pgbouncer.ini"
kgsec airflow-airflow-historical-pgbouncer -o json | jq -r '.data."pgbouncer.ini"'
kgsec airflow-airflow-historical-pgbouncer -o json | jq -r '.data."pgbouncer.ini"' | base64 -D
klf airflow-airflow-historical-worker-0 check-db
klf airflow-airflow-historical-flower-7b57fd8648-29x56 check-db
kl airflow-airflow-historical-flower-7b57fd8648-29x56 check-db
kdp airflow-airflow-historical-flower-7b57fd8648-29x56
klf airflow-airflow-historical-db-migrations-677d88d9bc-mph6v check-db
kdelp airflow-airflow-historical-db-migrations-677d88d9bc-mph6v
klf airflow-airflow-historical-db-migrations-677d88d9bc-ghhxk check-db
cat > lol.yaml
kgp -n airflow
kubens airflow
kgp | grep airflow-airflow-historical-db-migrations
kgp | grep airflow-db-migrations
klf airflow-db-migrations-5fc7bf85f6-rzwvz -c dags-git-
klf airflow-db-migrations-5fc7bf85f6-rzwvz -c dags-git-sync
klf airflow-airflow-historical-db-migrations-bdf499b4-d68x5 dags-git-sync
kdp airflow-airflow-historical-db-migrations-bdf499b4-d68x5 check-db
klf airflow-airflow-historical-db-migrations-bdf499b4-d68x5 check-db
kdp airflow-airflow-historical-db-migrations-bdf499b4-d68x5
klf airflow-airflow-historical-db-migrations-bdf499b4-d68x5
klf airflow-airflow-historical-db-migrations-bdf499b4-d68x5 dags-git-clone
ked airflow-airflow-historical-db-migrations
helm ls -n airflow-historical 
helm get values airflow
helm get values airflow/airflow
klf airflow-airflow-historical-db-migrations-677d88d9bc-c556w dags-git-clone
kgsec -n airflow-main airflow-ssh-git-secret -o json | jq -r '.data.gitSshKey' | base64 -D
kdp airflow-airflow-historical-db-migrations-677d88d9bc-c556w
rm lol.yaml
rm idrsa
rm lol
terraform destory -var-file=profiles/preprod.tfvars
klf airflow-airflow-historical-db-migrations-677d88d9bc-c556w
terraform destroy -var-file=profiles/preprod.tfvars
kgp -n airflow | grep migrations
klf airflow-db-migrations-5fc7bf85f6-gnjg5 dags-git-clone
klf airflow-db-migrations-5fc7bf85f6-gnjg5 dags-git-sync
klf airflow-db-migrations-5fc7bf85f6-gnjg5 check-db
klf airflow-airflow-historical-db-migrations-677d88d9bc-c556w check-db
klf airflow-airflow-historical-pgbouncer-7cff67b94c-xzl79
keti airflow-airflow-historical-pgbouncer-86c5dd8776-c7m5d
psql airflow-airflow-preprod-test-airflow-historical.cyk2led4j3ya.us-east-1.rds.amazonaws.com airflow_cluster
kds airflow-airflow-historical-pgbouncer -o yaml
kgs airflow-airflow-historical-pgbouncer -o yaml
ked airflow-airflow-historical-pgbouncer
aws ec2 describe-vpcs \\
  --vpc-ids vpc-07f9628106e924b83 \\
  --query "Vpcs[].CidrBlock" \\
  --output text
aws ec2 describe-vpcs \\
  --vpc-ids vpc-07f9628106e924b83 \\
  --query "Vpcs[].CidrBlock" \\
  --output text --region us-east-1
aws aws rds describe-db-parameters
aws aws rds list-db-parameters
aws aws rds --help
aws rds list -db-parameters
aws rds list-db-parameters
aws rds descirbe-db-parameters
aws rds describe-db-parameters
klf airflow-airflow-historical-pgbouncer-86c5dd8776-c7m5d
keti airflow-airflow-historical-pgbouncer-86c5dd8776-c7m5d bash
kubens
echo | openssl s_client -connect oidc.eks.us-east-1.amazonaws.com:443 2>&- | openssl x509 -fingerprint -noout | sed 's/://g' | awk -F= '{print tolower($2)}'
aws eks --region us-east-1 update-kubeconfig --name staging-clickhouse-test --role arn:aws:iam::277411487094:role/k8sAdmin --profile apono
export 
git checkout master
git commit -m "more fixes"
cd prod
cd airflow-cluster
cd eks-app-lb
cd EKS
cd ../
cd preprod/developers-airflow
kdi airflow-historical -n airflow-historical
kdi airflow-historical -n airflow
kdi airflow -n airflow-historical
kgp | grep airflow
keti airflow-pgbouncer-557869df97-m4mzg sh
keti airflow-airflow-historical-pgbouncer-86c5dd8776-c7m5d sh
kubecx
git commit -m "fixed node security groups"
cd clickhouse
pbpaste | wc -c
echo | openssl s_client -servername oidc.eks.us-east-1.amazonaws.com -showcerts -connect oidc.eks.${1}.amazonaws.com:443 2>&- | tac | sed -n '/-----END CERTIFICATE-----/,/-----BEGIN CERTIFICATE-----/p; /-----BEGIN CERTIFICATE-----/q' | tac | openssl x509 -fingerprint -sha1 -noout | sed 's/://g' | awk -F= '{print tolower($2)}'
echo | openssl s_client -servername oidc.eks.us-east-1.amazonaws.com -showcerts -connect oidc.eks.us-east-1.amazonaws.com:443 2>&- | tac | sed -n '/-----END CERTIFICATE-----/,/-----BEGIN CERTIFICATE-----/p; /-----BEGIN CERTIFICATE-----/q' | tac | openssl x509 -fingerprint -sha1 -noout | sed 's/://g' | awk -F= '{print tolower($2)}'
echo | openssl s_client -connect oidc.eks.$1.amazonaws.com:443 2>&- | openssl x509 -fingerprint -noout | sed 's/://g' | awk -F= '{print tolower($2)}'
echo | openssl s_client -connect oidc.eks.us-east-1.amazonaws.com:443 2>&- | openssl x509 -fingerprint -noout | sed 's/://g' | awk -F= '{print tolower($2)}'\

kgi -n clickhouse
git commit -m "fixed ingress"
git commit -m "added context to helm provider"
git push 
ping 8.8.8.8
terraform workspace
terraform workspace preprod
terraform workspace new preprod
terraform workspace default
terraform workspace select default
terraform workspace delete preprod
git commit -m "Fixed helm"
terraform init -backend-config=profiles/prod.s3.tfbackend -upgrade
kgcm -n kube-system
kgcm -n kube-system -o yaml aws-auth
kgr
kubectl get role
kubectl get role -A --all
kubectl get role -A
kgd prometheus-stack-server-68547d9cfc-6k56w
kgp prometheus-stack-server-68547d9cfc-6k56w
kdp prometheus-stack-server-68547d9cfc-6k56w
klf prometheus-stack-server-68547d9cfc-6k56w
klf prometheus-stack-server-68547d9cfc-6k56w prometheus-server
klf airflow-airflow-historical-db-migrations-677d88d9bc-p5fgz
klf airflow-airflow-historical-db-migrations-677d88d9bc-p5fgz -n airflow-historical
klf airflow-airflow-historical-db-migrations-677d88d9bc-p5fgz -n airflow-historical db-checker
kgsec -n monitoring
helm uninstall airflow-airflow-historical -n airflow-historical
helm uninstall airflow-airflow-main -n airflow-main
kgp -n airflow-main
kgi -n airflow-main
kgp -n monitoring
kdp prometheus-stack-prometheus-pushgateway-5bfcd894bc-h2dgv
kgsec -n airflow-main
kgsec -n airflow-main airflow-admin-credentials -o json
kgsec -n airflow-main airflow-admin-credentials -o json | jq -r '.data.username'
kgsec -n airflow-main airflow-admin-credentials -o json | jq -r '.data.username' | base64 -D
kgsec -n airflow-main airflow-admin-credentials -o json | jq -r '.data.password' | base64 -D
kgsec -n airflow-main airflow-admin-credentials -o json | jq -r '.data.password' | base64 -D | pbcopy
klf prometheus-stack-server-68547d9cfc-tpstr
klf prometheus-stack-server-68547d9cfc-tpstr prometheus-server
klf prometheus-stack-server-68547d9cfc-cmr52
ked prometheus-stack-prometheus-pushgateway
klf prometheus-stack-server-68547d9cfc-cmr52 prometheus-server
ked prometheus-stack-server
ks
kgsec airflow-cluster-postgres-basicauth -o json
kgsec airflow-cluster-postgres-basicauth -o json | jq -r '.data.password' | base64 -D | pbcopy
kgsec airflow-cluster-postgres-basicauth -o json | jq -r '.data.password' | base64 -D
kgsec airflow-cluster-postgres-basicauth -o json | jq -r '.data.username' | base64 -D
RhpaVOns2ktEXGQKEJ7155GNKRvTW9qc
kpf svc/airflow 8080:8080
klf prometheus-stack-server-68547d9cfc-sr67p
klf prometheus-stack-server-68547d9cfc-sr67p prometheus-server
klf prometheus-stack-server-68547d9cfc-d4sg9
klf prometheus-stack-server-68547d9cfc-d4sg9 prometheus-server
k get serviceaccount -n monitoring
k get serviceaccount -n monitoring amp-iamproxy-ingest-service-account -o yaml
helm get values prometheus-stack/prometheus
helm get values prometheus -n monitoring
helm get values prometheus-stack -n monitoring
git statu
git commit -m "fixed helm"
git push origin feature/clickhouse-ingress
git checkout refactor/airflow
chmod +x airflow/thumbprint.sh
terraform outputs -var-file=profiles/prod.tfvars
terraform output -var-file=profiles/prod.tfvars
terraform console
terraform console -var-file=profiles/prod.tfvars
kubectx cluster/airflow-prod-02
helm uninstall prometheus-stack -n monitoring
git commit -m "fixed prometheus"
git push origin refactor/airflow
helm ls -n monitoring --all
klf prometheus-stack-server-68547d9cfc-dpd7f prometheus-server
k describe role airflow -n airflow
k describe role eks:addon-manager -n kube-system
k describe role -n kube-system system:controller:cloud-provider
k describe role -A
k get role -A -o yaml
k get role -A -o yaml > all_roles.yaml
k get rolebinding -A -o yaml > all_role_bindings.yaml
k get rolebinding -A -o yaml > all_role_bindings-new.yaml
k get role -A -o yaml > all_roles-new.yaml
k get serviceaccount -o yaml -A
k get clusterroles
k get clusterrole cluster-admin -o yaml
k get clusterrole > clusterroles-new.yaml
k get clusterrole -o yaml > clusterroles-old.yaml
k get clusterrole -o yaml > clusterroles-new.yaml
rm all_role*
rm clusterroles-*
rm tfstate-new
git commit -m "added dependencies on ingresses"
eksctl create iamidentitymapping \\
 --cluster airflow-preprod-test \\
 --arn arn:aws:iam::277411487094:role/airflow-preprod-test-cluster-2023031312345218380000000b \\
 --username admin \\
 --group system:masters \\
 --region us-east-1
k get IAMIdentityMapping
k get resources
k get crds
eksctl get iamidentitymapping --cluster airflow-preprod-test --region=us-east-1\

eksctl get iamidentitymapping --cluster finout-prod-airflow-spark --region=us-east-1
eksctl get iamidentitymapping --cluster airflow-prod-02 --region=us-east-1
kubectl get clusterroles
kubectl get clusterroles admin -o yaml
kubectl get clusterrolesbinding
kubectl get rolebinding -A
k get clusterolebinding
k get clusterolesbinding
k get rolebinding
k get rolebinding -n kube-system
k get rolebinding -n kube-system -o eks:authenticator
k get rolebinding -n kube-system -o yaml eks:authenticator
k get rolebinding -n kube-system -o yaml system:controller:cloud-provider
k get rolebinding -n kube-system -o yaml eks:cloud-controller-manager:apiserver-authentication-reader
k get rolebinding -n kube-system -o yaml Role/eks-vpc-resource-controller-role
k get rolebinding -n kube-system -o yaml eks-vpc-resource-controller-role
kubectx arn:aws:eks:us-east-1:277411487094:cluster/finout-prod-airflow-spark
k get rolebinding -n kube-system -o yaml eks-vpc-resource-controller-rolebinding
kubectx arn:aws:eks:us-east-1:277411487094:cluster/airflow-prod-02
kubectx arn:aws:eks:us-east-1:277411487094:cluster/airflow-preprod-test
terraform show -no-color > tfstate
keti airflow-airflow-historical-worker-2 bash
git commit -m "fixed various stuff"
git checkout feature/clickhouse-ingress
terraform init -backend-config=profiles/prod.s3.tfbackend -upgrade 
git commit -m "removed rolearn for apm var"
las
vim ~/.kube/config
terraform validate
git commit -m "terraform formatted"
vim versions.tf
rm versions.tf
vim airflow-deployment.tf
aws --profile apono configure set role_arn arn:aws:iam::277411487094:role/yizhar_apono-apono-oidc-role && aws --profile apono configure set web_identity_token_file ~/.aws/token && echo eyJhbGciOiAiUlMyNTYiLCAidHlwIjogIkpXVCJ9.eyJ1c2VyX25hbWUiOiAieWl6aGFyX2Fwb25vLWFwb25vLW9pZGMtcm9sZSIsICJpc3MiOiAiaHR0cHM6Ly9hcG9uby1vaWRjLnMzLmFtYXpvbmF3cy5jb20vYjU1Njk4MzAtYWU4NC0xMWVjLWFlOGUtMGViNmRiMjNkYTQ1IiwgImlhdCI6IDE2ODAwNzI5NDksICJleHAiOiAxNjgwMTMwNTQyLCAiYXVkIjogIkFXUyIsICJzdWIiOiAieWl6aGFyX2Fwb25vLWFwb25vLW9pZGMtcm9sZSJ9.ThbpNlTOtjyquWw96A30BJySkKR_NdBHHgjOTimQHKcrc21IQ-19wRdWFQJzNiaLDlq96wIXHP710VF0wrNW-2IM-BVCYdz6yIySE06tJiCwIxeIaUyqFuPgs4ojSPMMuWyZkX4XpH6dElVZas4mU6GeW4QWYlMxjy7uLWEbU8JYx67COvazcpapnobrjHwFE35lcw5KNgrzTcP3q9qXxKZe7iL35r16uzeil3fWKgpdh5vrBa_BHHHHq-os43LMk9hE4xzPqWK8pzah1qBSDdjD7pOBcTFvTriDqN8uHcgh-BRNoUbu1tRXN541BnaeSguMgaAg8087Ps6ubLSiPQ > ~/.aws/token
vim variables.tf
vim ~/dev/private/workstation-setup/setup.sh
git checkout clickhouse/kubernetes.tf
git commit -m "changed account_id to be implicit"
aws eks --region us-east-1 update-kubeconfig --name airflow-prod-02 --role arn:aws:iam::277411487094:role/k8sAdmin --profile apono
aws eks --region us-east-1 update-kubeconfig --name airflow-preprod-test --role arn:aws:iam::277411487094:role/k8sAdmin --profile apono
terraform init -backend-config=profiles/preprod.s3.tfbackend -upgrade -reconfigure
terraform -version
helm ls -n kube-system
k9s
gem install k9s
brew install fzf\

$(brew --prefix)/opt/fzf/install\

$(brew --prefix)/opt/fzf/install -y
$(brew --prefix)/opt/fzf/install
cd .
git clone git@github.com:finout-io/finout-application.git
code .
k get clusteroles
k get clusterole
kubectl get roles -A
terraform import aws_iam_openid_connect_provider default DEDB32687CBE155BE9BBC714FDC530C0
terraform import aws_iam_openid_connect_provider.default DEDB32687CBE155BE9BBC714FDC530C0
terraform import aws_iam_openid_connect_provider.deafult DEDB32687CBE155BE9BBC714FDC530C0 -var-file=profiles/preprod.tfvars
terraform import -var-file=profiles/preprod.tfvars aws_iam_openid_connect_provider.deafult DEDB32687CBE155BE9BBC714FDC530C0
terraform import -var-file=profiles/preprod.tfvars aws_iam_openid_connect_provider.default DEDB32687CBE155BE9BBC714FDC530C0
terraform import -var-file=profiles/preprod.tfvars aws_iam_openid_connect_provider.default https://oidc.eks.us-east-1.amazonaws.com/id/DEDB32687CBE155BE9BBC714FDC530C0
terraform import -var-file=profiles/preprod.tfvars aws_iam_openid_connect_provider.default arn:aws:iam::277411487094:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/DEDB32687CBE155BE9BBC714FDC530C0
terraform state -var-file=profiles/preprod.tfvars
terraform state show -var-file=profiles/preprod.tfvars
terraform state -var-file=profiles/preprod.tfvars show aws_rds.airflow-airflow-preprod-test-airflow-main
terraform state show aws_rds.airflow-airflow-preprod-test-airflow-main
terraform state show aws_db_instance.airflow-airflow-preprod-test-airflow-main
terraform state show aws_db_instance.postgres["airflow-main"]
terraform state show aws_db_instance.postgres.airflow-main
terraform state show aws_db_instance.postgres[\"airflow-main\"
terraform state show aws_db_instance.postgres[\"airflow-main\"]
terraform state show aws_db_instance.postgres
index(data.aws_prometheus_workspace.amp.arn, "apn"\
)
index(data.aws_prometheus_workspace.amp.arn, "apn")
data.aws_prometheus_workspace.amp.arn
git commit -m "adding application, initial stage"
git push origin refactor/application
pmset noidle
helm get values data-dog-agent -n monitoring > datadog.yaml
echo "datadog-3.3.3" > datadog
helm get values finout-release -n default > configserver.yaml
helm get values externaldns-release -n application > externaldns.yaml
echo "external-dns-5.0.0" > externaldns
aws eks --region us-east-1 update-kubeconfig --name staging-application --role arn:aws:iam::277411487094:role/k8sAdmin --profile apono
terraform state rm module.documentdb_cluster.aws_docdb_cluster_parameter_group.default
terraform state rm module.documentdb_cluster.aws_docdb_subnet_group.default
terraform state rm module.documentdb_cluster.aws_security_group.default
terraform state rm module.documentdb_cluster.aws_security_group_rule.egress
terraform state rm module.documentdb_cluster.aws_security_group_rule.ingress_security_groups
kdi finout-ingress
kgi -o yaml finout-ingress
kgi -o yaml finout-ingress > configserver-ingress.yaml
kgs -o yaml finout-config-2-service > configserver-service.yaml
kgd -o yaml finout-config-2-deployment > configserver-deployment.yaml
kgns
klf cluster-autoscaler-aws-cluster-autoscaler-c996447b-pwdv8 -n cluster-autoscaler
kubectl get roles -A\

kubectl get clusterroles\

kubectl get clusterroles > new-clusterroles
kubectl get clusterroles > old-clusterroles
kubectl get clusterroles --no-headers | awk '{print $1}'
kubectl get clusterroles --no-headers | awk '{print $1}' > new-clusterroles
kubectl get clusterroles --no-headers | awk '{print $1}' > old-clusterroles
cat old-clusterroles
diff old-clusterroles new-clusterroles
rm old-clusterroles
rm new-clusterroles
k describe clusterrolebinding
k describe clusterrolebinding > old-bindings
mv old-bindings old-clusterrolebindings
k describe rolebinding -A > old-rolebindings
k describe clusterrolebinding > new-clusterrolebindings
k describe rolebinding -A > new-rolebindings
terraform console -var-file=profiles/preprod.tfvars
kgcm -o yaml aws-auth -n kube-system
k get roles -A
k get rolebinding -A
k get clusterrole
k get clusterrole -o yaml admin
pbpaste > clusterrole.yamk
mv clusterrole.yamk clusterrole.yaml
cat clusterrole.yaml
k get clusterrolebinding
k get clusterrolebinding -o yaml cluster-admin
kaf clusterrole.yaml
rm clusterrole.yaml
cd finout-
git commit -m "prettify"
terraform apply -var-file=profiles/preprod.tfvars
git checkout
terraform fmt
git checkout -b feature/airflow-postgres-backups
git commit -m "Added backups"
git push origin feature/airflow-postgres-backup
git push origin feature/airflow-postgres-backups
git checkout -b refactor/application
terraform workspace list
terraform init -backend-config=profiles/staging.s3.tfbackend -upgrade -reconfigure
terraform workspace create staging
terraform init -upgrade -reconfigure
terraform workspace new staging
terraform apply -var-file "profiles/$(terraform workspace show).tfvars" -target random_password.docdb
terraform state list -var-file "profiles/$(terraform workspace show).tfvars"
terraform import module.documentdb_cluster.aws_docdb_cluster_parameter_group.default documentdb-staging-application-staging-staging-application -var-file "profiles/$(terraform workspace show).tfvars"
terraform import module.documentdb_cluster.aws_docdb_cluster_parameter_group.default documentdb-staging-application-staging-staging-application
pbpaste > docdberrors
terraform -var-file "profiles/$(terraform workspace show).tfvars" import module.documentdb_cluster.aws_security_group.default documentdb-staging-application-staging-staging-application
terraform import -var-file "profiles/$(terraform workspace show).tfvars" module.documentdb_cluster.aws_security_group.default documentdb-staging-application-staging-staging-application
terraform import -var-file "profiles/$(terraform workspace show).tfvars" module.documentdb_cluster.aws_docdb_subnet_group.default documentdb-staging-application-staging-staging-application
terraform import -var-file "profiles/$(terraform workspace show).tfvars" module.documentdb_cluster.aws_docdb_cluster_parameter_group.default documentdb-staging-application-staging-staging-application
terraform state list
terraform import -var-file "profiles/$(terraform workspace show).tfvars" module.documentdb_cluster.aws_security_group.default[0] documentdb-staging-application-staging-staging-application
terraform import -var-file "profiles/$(terraform workspace show).tfvars" module.documentdb_cluster.aws_security_group.default.0 documentdb-staging-application-staging-staging-application
terraform import -var-file "profiles/$(terraform workspace show).tfvars" 'module.documentdb_cluster.aws_security_group.default[0]' documentdb-staging-application-staging-staging-application
terraform import -var-file "profiles/$(terraform workspace show).tfvars" 'module.documentdb_cluster.aws_security_group.default[0]' sg-05f8c0489fa57c396
terraform import -var-file "profiles/$(terraform workspace show).tfvars" 'module.documentdb_cluster.aws_docdb_subnet_group.default[0]' documentdb-staging-application-staging-staging-application
terraform import -var-file "profiles/$(terraform workspace show).tfvars" 'module.documentdb_cluster.aws_docdb_cluster_parameter_group.default[0]' documentdb-staging-application-staging-staging-application
terraform import -var-file "profiles/$(terraform workspace show).tfvars" 'module.documentdb_cluster.aws_security_group_rule.egress[0]' sg-05f8c0489fa57c396_egress_tcp_27017_27017_0.0.0.0/0
cat doc
terraform import -var-file "profiles/$(terraform workspace show).tfvars" 'module.documentdb_cluster.aws_security_group_rule.egress[0]' sg-05f8c0489fa57c396_egress_all_all_all_0.0.0.0/0
terraform import -var-file "profiles/$(terraform workspace show).tfvars" 'module.documentdb_cluster.aws_security_group_rule.egress[0]' sg-05f8c0489fa57c396_egress_-1_-1_-1_0.0.0.0/0
cat docdberrors
terraform import -var-file "profiles/$(terraform workspace show).tfvars" 'module.documentdb_cluster.aws_security_group_rule.ingress[0]' sg-05f8c0489fa57c396_ingress_tcp_27017_27017_sg-05bd7e82197c996d7
terraform import -var-file "profiles/$(terraform workspace show).tfvars" 'module.documentdb_cluster.aws_security_group_rule.ingress_security_groups[0]' sg-05f8c0489fa57c396_ingress_tcp_27017_27017_sg-05bd7e82197c996d7
terraform apply -var-file "profiles/$(terraform workspace show).tfvars"
kgsec -o json config-server-github-private-key-20220217 | jq 
kgsec -o json config-server-github-private-key-20220217 | jq -r '.data.config-server-github-private-key'
kgsec -o json config-server-github-private-key-20220217 | jq -r '.data."config-server-github-private-key"'
kgsec -o json config-server-github-private-key-20220217 | jq -r '.data."config-server-github-private-key"' | base64 -D
kgsec -o json config-server-github-private-key-20220217 | jq -r '.data."config-server-github-private-key"' | base64 -D > github-private-key
cat github-private-key
kgsec -o json config-server-host-key-20220217 
kgsec -o json config-server-host-key-20220217 | jq -r '.data."config-server-host-key"' 
kgsec -o json config-server-host-key-20220217 | jq -r '.data."config-server-host-key"' | base64 -D
kgsec -o json config-server-user-20220217 | jq -r '.data."config-server-user"' | base64 -D > configserver-hostkey
kgsec -o json config-server-user-20220217 | jq -r '.data."config-server-user"' | base64 -D > configserver-username
kgsec -o json config-server-password-20220217 | jq -r '.data."config-server-password"' | base64 -D > configserver-password
kgsec -o json config-server-host-key-20220217 | jq -r '.data."config-server-host-key"' | base64 -D > configserver-hostkey
ls -lh
ls -l
mv configserver* ../../finout-application
kgsec -o json active-profile 
kgsec -o json active-profile  | jq -r '.data."active-profile"'
kgsec -o json active-profile  | jq -r '.data."active-profile"' | base64 -D
aws eks --region us-east-1 update-kubeconfig --name finout-staging-app --role arn:aws:iam::277411487094:role/k8sAdmin --profile apono
kgp -A
aws eks --region us-east-1 update-kubeconfig --name finout-staging01	 --role arn:aws:iam::277411487094:role/k8sAdmin --profile apono
ked finout-config-2-deployment
klf configserver-b75cb5778-8nmjb
kgsec 
kgsec -o json configserver-github-credentials | jq -r '.data'
kgsec -o json configserver-github-credentials | jq -r '.data."github.yml"'
ked
klf configserver-b75cb5778-7x6hr
kdeld configserver
cargo run -- install terraform --versions
cargo run -- install kubectl --versions
cargo run -- install kubectl 1.26.3
ls ~/.envctl/kubectl/bin/*
rm ~/.envctl/kubectl/bin/*
sudo rm ~/.envctl/kubectl/bin/*
ls ~/.envctl/kubectl/bin
brew bundle --help
brew uninstall karabiner-elements
echo https://google.com
open -a "Google Chrome" --args --make-default-browser
brew install defaultbrowser
defaultbrowser chrome
brew uninstall defaultbrowser
cat .zshrc
rm .zsh
cd ~/.zsh/
cp .zshrc .bak.zshrc
rm .bak.zshrc
rm .zshrc
touch macos-config.zsh
rm macos-config.zsh
pbpaste > macos-setup.zsh
sudo scutil --set ComputerName "0x6D746873"
sudo scutil --set ComputerName "Batbook Pro"
defaults write com.apple.universalacess reduceTransparency -bool true
defaults write com.apple.universalacess reduceTransparency -bool false
defaults write NSGlobalDomain AppleHighlightColor -string "0.764700 0.976500 0.568600"
sudo defaults write /Library/Preferences/com.apple.loginwindow AdminHostInfo HostName
sudo defaults write /Library/Preferences/com.apple.loginwindow.plist AdminHostInfo HostName
cat setup.sh
defaults read com.apple.dock orientation
defaults read com.apple.dock | grep mag
defaults read com.apple.dock | grep tilesize -B 20
defaults read com.apple.dock tilesize
defaults read com.apple.dock | grep size
defaults read com.apple.dock | grep largesize -A 30
defaults read com.apple.desktop 
defaults domains
defaults domains -l
defaults find wallpaper
defaults find desktop
defaults read com.apple.desktop
defaults write NSGlobalDomain NSAutomaticSpellingCorrectionEnabled -bool false
defaults read NSGlobalDomain NSAutomaticSpellingCorrectionEnabled
defaults read NSGlobalDomain NSAutomaticPeriodSubstitutionEnabled
defaults write NSGlobalDomain NSAutomaticPeriodSubstitutionEnabled -bool false
defaults write NSGlobalDomain NSAutomaticQuoteSubstitutionEnabled -bool false
defaults write NSGlobalDomain NSAutomaticDashSubstitutionEnabled -bool false
defaults read NSGlobalDomain | grep -i automatic
defaults read NSGlobalDomain AppleLanguages
defaults read NSGlobalDomain AppleLocale
defaults read NSGlobalDomain | grep -i currency
defaults read NSGlobalDomain | grep -i usd
defaults read NSGlobalDomain | grep -i nis
defaults read NSGlobalDomain AppleMeasurementUnits
defaults read NSGlobalDomain | grep -i measure
defaults read NSGlobalDomain | grep -i metric
defaults read NSGlobalDomain
defaults read NSGlobalDomain | grep -i locale
defaults write NSGlobalDomain InitialKeyRepeat -int 10
defaults write NSGlobalDomain KeyRepeat -int 1
defaults read NSGlobalDomain ApplePressAndHoldEnabled
defaults read NSGlobalDomain | grep -i press
defaults read NSGlobalDomain | grep -i hold
defaults read NSGlobalDomain | grep -i repeat
defaults read NSGlobalDomain | grep -i sticky
defaults read com.apple.driver.AppleBluetoothMultitouch.trackpad Clicking
defaults read com.apple.mouse.tapBehavior
defaults read com.apple.mouse.tapBehaviour
defaults read com.apple.mouse
sudo nvram SystemAudioVolume=" "
sudo nvram SystemAudioVolume
defaults read NSGlobalDomain AppleHighlightColor
defaults read NSGlobalDomain NSTableViewDefaultSizeMode
defaults read NSGlobalDomain NSToolbarTitleViewRolloverDelay
defaults read NSGlobalDomain NSDocumentSaveNewDocumentsToCloud
defaults read com.apple.print.PrintingPrefs
defaults read com.apple.LaunchServices
lscleanup
defaults read NSGlobalDomain NSDisableAutomaticTermination
sudo systemsetup
defaults read /Library/Preferences/com.apple.windowserver.displays.plist DisplayResolutionEnabled
defaults read /Library/Preferences/com.apple.windowserver DisplayResolutionEnabled
defaults read NSGlobalDomain AppleFontSmoothing
defaults read com.apple.screencapture
defaults read com.apple.finder QuitMenuItem
defaults read com.apple.finder | grep -i animation
defaults read com.apple.finder | grep -i newwindow
defaults read com.apple.finder | grep -i path
defaults read com.apple.finder | grep -i showall
defaults read com.apple.finder | grep -i hidden
defaults read com.apple.finder | grep -i files
defaults read com.apple.finder | grep -i show
defaults read com.apple.finder
defaults read com.apple.finder ShowStatusBar
defaults read com.apple.finder ShowPathbar
defaults read com.apple.finder _FXShowPosixPathInTitle
defaults read com.apple.finder FXDefaultSearchScope
defaults read com.apple.desktopservices DSDontWriteUSBStores
defaults read com.apple.desktopservices
defaults read com.apple.dock | grep hilite
defaults read com.apple.dock | grep mouse
defaults read com.apple.dock | grep indicator
defaults read com.apple.dock | grep persistent
defaults read com.apple.dock persistent-apps
defaults read com.apple.dock static-only
defaults read com.apple.dock | grep -i static
defaults read com.apple.dock
defaults read com.apple.dock mru-spaces
defaults read com.apple.dock autohide-delay
defaults read com.apple.dock | grep -i hide
defaults read com.apple.dock show-recents
defaults read com.apple.dock | grep -i recent
sudo rm -rf /System/Applications/Mail.app
defaults read com.apple.TimeMachine
defaults read com.apple.ActivityMonitor
defaults --help
defaults domains 
defaults read com.googlecode.iterm2 | grep -i natu
defaults read com.googlecode.iterm2 | grep -i key
defaults read com.googlecode.iterm2 | grep -i profile
defaults read com.googlecode.iterm2 | jq
defaults read com.googlecode.iterm2 
defaults read com.googlecode.iterm2 > iterm2
defaults write --help
vim iterm2.plist
cat gruvbox.itermcolors
rm gruvbox.itermcolors
git remove -
git remove -v
git remote -v
mv workstation-setup dotfiles
vim .git/config
vim .git/description
git remote add origin git@github.com:dormunis/dotfiles.git
git commit -m "initial commit"
git push -u origin main
vim README.md
git dif
git commit -m "Added readme"
git commit -m "echo cleanup"
defaults read NSGlobalDomain KeyRepeat
defaults read NSGlobalDomain InitialKeyRepeat
cd ~
git submodule add git git@github.com:tmux-plugins/tpm.git
git submodule add git github.com:tmux-plugins/tpm.git
git submodule add git https://github.com/tmux-plugins/tpm.git
git submodule add git@github.com:tmux-plugins/tpm.git
git submodule update git@github.com:tmux-plugins/tpm.git ./tmux/plugins/tpm
git submodule update git@github.com:tmux-plugins/tpm.git tmux/plugins/tpm
git submodule --help
git submodule ls
git submodule list
git submodule -v
git submodule update tpm tmux/plugins/tpm
git submodule update --path tpm tmux/plugins/tpm
git submodule deinit tpm
git rm tpm
rm -rf .git/modules/tpm
git submodule add git@github.com:tmux-plugins/tpm.git tmux/plugins/tpm
cd ...
cd ../../..
git submodule update --init --recursive
rm tmux
vim Brewfile
mkdir tmux
mv .tmux.conf tmux
vim .tmux.conf
rm -rf tpm
vim .gitmodules
ln -s --help
cd tpm
ll -a
cd .gem
cat .profile
ls .pyenv
ls .terraform.d
brew bundle dump
cat Brewfile
mv Brewfile ~/dev/private/workstation-setup
cd workstation-setup
cd dev/private/workstation-setup
ls -la
rm -r .zsh
rm iterm2
chmod +x macos-setup.zsh
defaults read  /Library/Preferences/SystemConfiguration/com.apple.smb.server NetBIOSName
sudo systemsetup -gettimezones
sudo systemsetup -gettimezone
sudo systemsetup --hep
sudo systemsetup --help
sudo systemsetup -help
sudo systemsetup -help | grep list
sudo systemsetup -listtimezones
sudo systemsetup -listtimezones | grep -i jerus
sudo systemsetup -settimezone "Asia/Jerusalem"
./macos-setup.zsh
defaults read com.googlecode.iterm2
defaults read com.googlecode.iterm2 > iterm2.plist
[A
git commit -m "keyboard repeat fine-tuning"
brew install tmux
ls ~/.config/tmux
cd /Users/dormunis/.config/tmux
rm ~/.config/tmux
cd ~/.config/tmux
ln -s tmux/ ~/.config/tmux
lll
ln -s tmux ~/.config/tmux
ln -s "$PWD/tmux" ~/.config/tmux
tmux source ~/.tmux.conf
ll ~/.config/tmux
ll ~/.config/tmux/
yabai -m window --toggle full-screen
yabai -m window --toggle zoom-fullscreen
echo $TEST
echo $SHELL
TEST=1 $SHELL
defaults write NSGlobalDomain KeyRepeat 1
cd dev/private/envctl
cargo run -- install helm --versions
cargo run -- install 3.11.2
cargo run -- install helm 3.11.2
vim ~/.config/tmux
vim ~/.config/skhd
$SHELL
defaults write NSGlobalDomain KeyRepeat 2
defaults write NSGlobalDomain InitialKeyRepeat 10
defaults write NSGlobalDomain InitialKeyRepeat 15
cargo run -- --help
mkdir serverlessgpt
mv serverlessgpt sls-gpt
cd sls-gpt
pbpaste > .openai-secret
cat .openai-secret
git init
mv sls-gpt chat-ifttt
cd chat-ifttt
brew install --cask android-studio
echo test
wwq
cd ~/.config
mv nvim bak.nvim
mv ~/.local/share/nvim ~/.local/share/nvim.bak\

cd tmux/.tmux.conf
tmux source ~/.config/tmux/.tmux.conf
mkdir nvim
ln -s "$PWD/nvim" ~/.config/nvim
\\ls
nvim .
vim $ZSH/aliases/customized.plugin.zsh
cd ~/dev/
cd envctl
vim defaultbrowser.py
cd ~/dev/private/dotfiles
cd ~/dev/private/envctl
\:q
rm -rm ~/.local/share/nvim/site/pack/packer
rm -rf ~/.local/share/nvim/site/pack/packer
cat lsp.lua
ls lsp-zero.lua
cat lsp-zero.lua
vim ../plugins.lua
rm lsp-zero.lua
vim ,.
cd after/plugin
rm harpoon.lua
cd after
mv packer.lua plugins.lua
ls plugin
rm -rf plugin
cat lazy-lock.json
cd plugins.lua
rm plugins.lua.bak
mkdir plugins
touch plugins/init.lua
mv after/plugin/* lua/dormunis/plugins
tree
ls after/plugin
rm -rf after
vim lualine.lua
cat init.lua
mv lua/dormunis/init.lua .
ls lua
ls lua/dormunis
mv lua/dormunis/* lua/
rm -r dormunis
cd lua/plugins
lx
vim ini
vim treesitter.lua
vim set.lua
vim lsp-zero.lua
mkdir plugin_config
mv plugins/fugitive.lua plugin_config
mv plugins/lsp-zero.lua plugin_config
cat undotree.lua
mv undotree.lua ../plugin_config
cp telescope.lua ../plugin_config
cat treesitter.lua
cat nvim-tree.lua
mv nvim-tree.lua ../plugin_config
mv colors.lua ../plugin_config
cat lualine.lua
mv lualine.lua ../plugin_config
ld
mv lazy.lua dormunis
mv plugin_config dormunis
mv remap.lua dormunis
mv set.lua dormunis
mv plugins dormunis
mv plugins/init.lua plugins.lua
vim lazy.lua
cd plugin
rm lazy.lua
mv plugins/lsp.lua plugin_config
cat telescope.lua
cat tele
vim telescope.lua
rm telescope.lua
vim plugins.lua
cat plugins/treesitter.lua
rm -rf plugins
ls plugin_config
cd ../..
rm lazy-lock.json
cd lua/dormunis
mv remap.lua keymap.lua
mv set.lua options.lua
cd lua/dormunis/plugin_config
cat colors.lua
git commit -m "added nvim config"
cat lua/dormunis/plugin_config/telescope.lua
vim lua/dormunis/plugin_defs/telescope.lua
mv plugin_config/lsp.lua .
l
cd lua
cd dormunis
cat plugin_config/colors.lua
cd plugin_defs
touch colorscheme.lua
mv ../plugin_config/nvim-tree.lua .
vim nvim-tree.lua
mv ../plugin_config/fugitive.lua .
mv ../plugin_config/lualine.lua .
mv ../plugin_config/undotree.lua .
mv init.lua copilot.lua
touch init.lua
cat copilot.lua
cd plugin_config
rm -r plugin_config
mv plugin_defs plugins
vim init.lua
cd lua/dormunis/plugins
rm init.lua
git commit -m "a more sensible plugins dir"
git staqtus
cat nvim/lazy-lock.json
git commit -m "really updated gitignore"
vi
nvim
cd nvim
vim 
vim .gitignore
git rm -r --cached .
git commit -m "added lazy-lock.json to gitignore"
ls .gitmodules
cat .gitmodules
cd tmux
ll
cd plugins
pyenv shell 3.11.2
git checkout -b feature/deployer-environment-agnostic"
git checkout -b feature/deployer-environment-agnostic
git commit -m "added configserver;added environments"
git push origin feature/deployer-environment-agnostic
aws --profile apono configure set role_arn arn:aws:iam::277411487094:role/yizhar_apono-apono-oidc-role && aws --profile apono configure set web_identity_token_file ~/.aws/token && echo eyJhbGciOiAiUlMyNTYiLCAidHlwIjogIkpXVCJ9.eyJ1c2VyX25hbWUiOiAieWl6aGFyX2Fwb25vLWFwb25vLW9pZGMtcm9sZSIsICJpc3MiOiAiaHR0cHM6Ly9hcG9uby1vaWRjLnMzLmFtYXpvbmF3cy5jb20vYjU1Njk4MzAtYWU4NC0xMWVjLWFlOGUtMGViNmRiMjNkYTQ1IiwgImlhdCI6IDE2ODA1MDY1MjgsICJleHAiOiAxNjgwNTY0MTI0LCAiYXVkIjogIkFXUyIsICJzdWIiOiAieWl6aGFyX2Fwb25vLWFwb25vLW9pZGMtcm9sZSJ9.EvtMhJaRkqIsQOeuV7EPViF9A6wlnT17GXcJ6UidlG3eYLr5jX9Ib_cUOJH2P-2lkLIZ-V9WB4PO4-MHfAzA3EhNUEdT6aJ6DzwXed0X-toBIYpqgK6iBNmhTVGWWFnEvZ2JikYrznBWYJMJnVMWcpYNMVgpMlewCMX4qcxEjJCrEe0CeNXT9sCrBIn5PFb1wyNSr66NgHaPVE-JQHDcWnBtN-YrHEVolHgE9Fgoxy5w26ndDtB54AdPKTI5lSLk4ihWZMvyRC0whh8GJouRD2WVziWGqfgJKlDEekkjw0mtmq7CoL8udC5FhTmTaOPcNX38Dos7JtgwkPKaYZ9oWg > ~/.aws/token
kubectl get --raw /apis/metrics.k8s.io/\

aws eks --region us-east-1 update-kubeconfig --name finout-prod02 --role arn:aws:iam::277411487094:role/k8sAdmin --profile apono
kgcm prometheus-prometheus-kube-prometheus-prometheus-rulefiles-0
kgcm prometheus-prometheus-kube-prometheus-prometheus-rulefiles-0 -o yaml
kpf svc/prometheus-grafana 8080:80
aws eks --region us-east-1 update-kubeconfig --name finout-prod-airflow-spark --role arn:aws:iam::277411487094:role/k8sAdmin --profile apono
kgcm prometheus-prometheus-kube-prometheus-prometheus-rulefiles-0 -o yaml -n monitoring
kgcm -n monitoring
kgcm -A
kgcm -A | grep prometheus
kgs -n monitoring
kpf svc/prometheus-stack-server 8080:80 -n monitoring
kubectx finout-prod02
kubectx 
kubens monitoring
kgd prometheus-stack-server -o yaml
kgcm
kgcm -o yaml prometheus-stack-server
helm ls -n monitoring
kpf prometheus-stack-kube-state-metrics 8080:8080
kpf svc/prometheus-stack-prometheus-node-exporter 9100:9100
kpf svc/prometheus-stack-kube-state-metrics 8080:8080
kpf svc/prometheus-stack-server 8080:80
kpf svc/prometheus-kube-prometheus-prometheus 9090:9090
kpf svc/prometheus-stack-server 9090:80 -n monitoring
pbpaste
kgi -A
cd dev/finout/finout-terraform
terraform output
kgsec -n airflow-historical airflow-admin-credentials -o json | jq -r '.data.password' | base64 -D | pbcopy
vim deploy.sh
cat deploy.sh
cd finout/finout-terraform
vim .
k get secret -n cluster-autoscaler-aws-cluster-autoscaler-token-zphpq
k get secret cluster-autoscaler-aws-cluster-autoscaler-token-zphpq
k get secret cluster-autoscaler-aws-cluster-autoscaler-token-zphpq -o yaml
helm ls -n cluster-autoscaler
              - - "groups":\
              -   - "system:masters"\
              -   "rolearn": "arn:aws:iam::277411487094:role/yizhar_apono-apono-oidc-role"\
              -   "username": "admin"
helm get values cluster-autoscaler -n cluster-autoscaler
kdelp cluster-autoscaler-aws-cluster-autoscaler-7c4675cb69-g7mnx
klf cluster-autoscaler-aws-cluster-autoscaler-7c4675cb69-c67jj
kdelp cluster-autoscaler-aws-cluster-autoscaler-7c4675cb69-c67jj
klf cluster-autoscaler-aws-cluster-autoscaler-7c4675cb69-c2ng2
klf cluster-autoscaler-aws-cluster-autoscaler-7c4675cb69-c2ng2 | grep failed
kubens airflow-historical
kgsec -n airflow-admin-credentials -o json | jq -r '.data.password' | base64 -D | pbcopy
kgsec -n airflow-admin-credentials -o json 
kgsec -n airflow-admin-credentials
kgsec airflow-admin-credentials
kgsec airflow-admin-credentials -o json | jq -r '.data.password' | base64 -D | pbcopy
kgd -n application
kdd -n application | grep -i image
kdp forter-costcenter-snow-virtual-1680107077-exec-48
kgp | grep forter
kgp | grep forter | awk "{print $1}" | xargs kubectl delete pods
kgp | grep forter | awk "{print $1}" 
kgp | grep forter | awk "{print $1}"
kgp | grep forter | awk '{print $1}'
kgp | grep forter | awk '{print $1}' | xargs kubectl delete pods
kgp | grep similarweb | awk '{print $1}' | xargs kubectl delete pods
kdelp -n kube-system aws-load-balancer-controller-547b487884-ns6w7 aws-load-balancer-controller-547b487884-qhrr2
klf aws-load-balancer-controller-547b487884-8j7nv -n kube-system
kdp aws-load-balancer-controller-547b487884-fq9jj
kdp aws-load-balancer-controller-547b487884-fq9jj -n kube-system
klf -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller
kubens cluster-autoscaler
klf cluster-autoscaler-aws-cluster-autoscaler-7c4675cb69-t6qt2
kdlep cluster-autoscaler-aws-cluster-autoscaler-7c4675cb69-t6qt2
kdelp cluster-autoscaler-aws-cluster-autoscaler-7c4675cb69-t6qt2
klf cluster-autoscaler-aws-cluster-autoscaler-7c4675cb69-g7mnx
klf cluster-autoscaler-aws-cluster-autoscaler-7c4675cb69-g7mnx | grep spark
klf cluster-autoscaler-aws-cluster-autoscaler-7c4675cb69-g7mnx | grep spark | grep -i access
klf cluster-autoscaler-aws-cluster-autoscaler-7c4675cb69-g7mnx | grep spark | grep -i access -A 3
kl cluster-autoscaler-aws-cluster-autoscaler-7c4675cb69-g7mnx | grep Failed
kl cluster-autoscaler-aws-cluster-autoscaler-7c4675cb69-g7mnx | grep -e Failed -e nodegroup
kl cluster-autoscaler-aws-cluster-autoscaler-7c4675cb69-g7mnx | grep AccessDeniedException -A 3
k get sa -o yaml cluster-autoscaler-aws-cluster-autoscaler
kgsec -A | grep service-account
kgsec -A | grep token
k get sa cluster-autoscaler-aws-cluster-autoscaler
k get sa cluster-autoscaler-aws-cluster-autoscaler -o yaml
cd airflow
gti diff
terraform init -backend-config=profiles/prod.s3.tfbackend -upgrade -reconfigure
;2A
terraform apply -var-file=profiles/prod.tfvars
ls ~/.zsh/plugins
mv ~/.zsh/plugins/git-completion.zsh ~/.zsh/plugins/git-completion.plugin.zsh
kubecs
kubecrx
k get sa
kdlep cluster-autoscaler-aws-cluster-autoscaler-7c4675cb69-c2ng2
kdelp cluster-autoscaler-aws-cluster-autoscaler-7c4675cb69-c2ng2
klf cluster-autoscaler-aws-cluster-autoscaler-7c4675cb69-46qrj
kdelp cluster-autoscaler-aws-cluster-autoscaler-7c4675cb69-46qrj
klf cluster-autoscaler-aws-cluster-autoscaler-7c4675cb69-r8x2j
k edit sa cluster-autoscaler-aws-cluster-autoscaler
kdelp cluster-autoscaler-aws-cluster-autoscaler-7c4675cb69-r8x2j
klf cluster-autoscaler-aws-cluster-autoscaler-7c4675cb69-4cvdx
kdelp cluster-autoscaler-aws-cluster-autoscaler-7c4675cb69-4cvdx
klf cluster-autoscaler-aws-cluster-autoscaler-7c4675cb69-rq6pp
kdelp cluster-autoscaler-aws-cluster-autoscaler-7c4675cb69-rq6pp
klf cluster-autoscaler-aws-cluster-autoscaler-7c4675cb69-pz7jd
kgp -n airflow-historical
klf cluster-autoscaler-aws-cluster-autoscaler-59d9db9cb5-4jmfg
klf cluster-autoscaler-aws-cluster-autoscaler-59d9db9cb5-4jmfg | grep spark
git commit -m "new irsa"
git push origin bugfix/airflow-autoscaler
×ƒº×ƒ³
git checkout main
git branch
git checkotu
git ..
git checkout refactor/application
vim $ZSH/aliases
pbpaste > ~/.zsh/plugins/git-completions.plugin.zsh
rm ~/.zcompdump\

autoload -U compinit && compinit\

cd $ZSH/plugins/
mkdir git-completions
mv git-completions.plugin.zsh git-completions/
cd git-completions
curl -o git-completion.bash https://raw.githubusercontent.com/git/git/master/contrib/completion/git-completion.bash\

chmod +x git-completion.bash
vim $ZSH/.zshrc
cat ~/.zsh/plugins/git-completion.plugin.zsh
mv ~/.zsh/plugins/git-completion.plugin.zsh ~/.zsh/plugins/git-completions.plugin.zsh
chmod +x ~/.zsh/plugins/git-completions.plugin.zsh
vim ~/.zshrc
helm get all -n application release=externaldns-release
helm get all -n application release="externaldns-release"
helm get all -n application -l release="externaldns-release"
helm get deployments -n application
helm get deployments -n application externaldns-release
helm get all -n application externaldns-release
helm get all -n application externaldns-release > externaldns_resources
cd application
kubens kube-system
helm ls -A
kecm aws-auth -n kube-system
terraform workspace show
mkdir test
cd test
pbpaste > github.yml
pbpaste > users.yml
rm -rf test
kgsec -o json configserver-user-credentials
kgsec -o json configserver-user-credentials | jq -r '.data."global-user-credentials.yml"'
kgsec -o json configserver-user-credentials | jq -r '.data."global-user-credentials.yml"' | base64 -D
git commit -m "continue work"
git push origin feature/application
git push -u origin feature/application
git push -u origin refactor/application
cd ../../finout-terraform/application
git commit -m "Adding secretmanager permissions"
git pull origin masin
git pull origin main
git pull origin main --rebase
terraform init
kgp -n cluster-autoscaler
klf cluster-autoscaler-aws-cluster-autoscaler-7c9cbc4756-5gpb8 -n cluster-autoscaler
git commit -m "fixed cluster-autoscaler"
klf kpi-center-service-deployment-7f866cff4b-s55sh
klf internal-api-gateway-deployment-5d7ffc954c-thbqf
kl internal-api-gateway-deployment-5d7ffc954c-thbqf
klf account-service-deployment-cd45f4586-vrrkn
kdp reports-service-deployment-7b5d885479-k7mw7
klf api-gateway-deployment-86dbb864c7-w4pq5
klf archiver-service-deployment-657c5c86-zv8g4
kgd
kgp -n kube-system
klf configserver-b75cb5778-ft7rc
keti configserver-b75cb5778-ft7rc
keti configserver-b75cb5778-ft7rc bash
kdp configserver-56b6cd65f7-zzc7v
sleep 3
keti configserver-6b9668d6f5-z5vvz bash
klf configserver-74df879bf-hzcnm
klf configserver-675bdb6494-v9nnr
ked configserver
klf configserver-859ff5f7d7-jtnbd
export AWS_PROFILE=apono
kdelp configserver-859ff5f7d7-jtnbd
klf configserver-859ff5f7d7-dld2r
kdelp configserver-859ff5f7d7-dld2r
kdp configserver-859ff5f7d7-lxwf2
klf configserver-859ff5f7d7-lxwf2
kgsec -o json configserver-github-credentials | jq -r '.data."github.yml"' | base64 -D
terraform apply -var-file=profiles/staging.tfvars
kdelp configserver-859ff5f7d7-lxwf2
klf configserver-859ff5f7d7-s2jz7
kgs
kpf svc/configserver 8080:8080
git commit -m "Fixed configserver"
pwd
cd ../../
cd finout-application
git log
kdp api-gateway-deployment-84cdb59566-c8k8w -n application | grep -i image
aws eks --region us-east-1 update-kubeconfig --name staging-application --profile apono
kubectx -d
kaf test.yaml
kgsec -n application
kdelsec configserver-github-credentials
spot
k get ns
kubens default
kgsec
kgd -n default 
kgd -n default configserver -o yaml
kgd -n default configserver -o yaml | pbcopy
./deploy.sh staging application main-d2d37c2 --debug
helm template --help
helm template finout-release -n application . \\
  --set image.tag="test" \\
  --set namespace="staging" \\
  -f $BASEDIR/environments/values.staging.yaml \\
  --debug
helm template finout-release -n application . \\
  --set image.tag="test" \\
  --set namespace="staging" \\
  -f ./environments/values.staging.yaml \\
  --debug
helm template finout-release -n application . \\
  --set image.tag="test" \\
  --set namespace="staging" \\
  -f ./environments/values.staging.yaml \\
  --debug | grep configserver
helm template finout-release -n application . \\
  --set image.tag="test" \\
  --set namespace="staging" \\
  -f ./environments/values.staging.yaml \\
  --debug | grep configserver -a 50
helm template finout-release -n application . \\
  --set image.tag="test" \\
  --set namespace="staging" \\
  -f ./environments/values.staging.yaml \\
  --debug | grep configserver -A 50
./deploy.sh staging application main-d2d37c2
./deploy.sh staging application main-d2d37c2 yes
kubectx
kgp -n application
kubens application
kgi
pbpaste | jq
klf cost-guard-service-deployment-9957c457-gwqqx
klf
klf internal-api-gateway-deployment-5d7ffc954c-gmv8j
kdelp internal-api-gateway-deployment-5d7ffc954c-gmv8j
kdelp screenshot-service-deployment-846d4f9d48-5x9sg
kdelp ui-reporter-deployment-68468f967d-wqttg
kdelp cost-guard-service-deployment-9957c457-gwqqx
klf screenshot-service-deployment-846d4f9d48-sh4b2
./deploy.sh staging application main-d2d37c2 no
git commit -m "deployer fixes"
git push -u origin feature/deployer-environment-agnostic
cd finout-terraform
git pull
git pull origin refactor/application --rebase
kgp -n default
cd finout/finout-application
cd deployer
./deploy.sh staging application feature-deployer-environment-agnostic-70aef7d no
kg
klf account-service-deployment-c984588c8-954v5
helm ls -A --all
helm uninstall finout-release -n application
kgno
kgp | awk '{print $1}'
kgp --no-headers | awk '{print $1}' | xargs kubectl delete pods --force --grace-period=0
kp
kgp
git commit -m "change configserver url to be relative"
git push
cd private/dotfiles
tmux
ls vim
cd vim/undodir
cd ..
rm -rf vim
cd dev
cd private
cd dotfiles
ls -la ~
clear
cd dev/private/dotfiles
vim macos-setup.zsh
cp -r ~/.config/* .
git diff
git add macos-setup.zsh setup.sh
git add macos.zsh
rm -r bak.nvim
ls iterm2
rm -rf iterm2
ls Autodesk
rm -rf Autodesk
ls github-copilot/hosts.json
ls -l github-copilot
rm -rf github-copilot
git commit -m "fixed some stuff in setup.sh"
git add nvim/lua/dormunis/keymap.lua nvim/lua/dormunis/plugins/lsp.lua
git commit -m "fixed some keybindings"
ls skhd
vim
git status
git add .
vim setup.sh
rm -rf ~/.config/yabai
rm -rf ~/.config/skhd
ln -s "$PWD/yabai" ~/.config/yabai
ln -s "$PWD/skhd" ~/.config/skhd
cd _bootstrap
ls
mv defaultbrowser.py set-chrome-default-browser.py
